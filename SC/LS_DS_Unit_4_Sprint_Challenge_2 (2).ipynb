{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2*\n",
    "\n",
    "# Sprint Challenge - Neural Network Foundations\n",
    "\n",
    "Table of Problems\n",
    "\n",
    "1. [Defining Neural Networks](#Q1)\n",
    "2. [Simple Perceptron](#Q2)\n",
    "    - Perceptron\n",
    "    - Multilayer Perceptron\n",
    "    - Analyze and Compare\n",
    "4. [Keras MMP](#Q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Q1\"></a>\n",
    "## 1. Defining Neural Networks \n",
    "\n",
    "Write *your own* definitions for the following terms:\n",
    "\n",
    "- **Neuron:** A neuron is a function from a vector space to a scalar. It sums its inputs(features) and applies an activation function, such as the sigmoid or relu function, and normalizes it.\n",
    "- **Input Layer:** The input layer == the num of features you will be using. Its where you send your observations to then be passed on to the hidden layer.\n",
    "- **Hidden Layer:** In the hidden layer, artificial neurons take in weighted inputs and produce an output through an activation function.\n",
    "- **Output Layer:** The output layer is the prediction vector. \n",
    "- **Activation Function:** Activation functions can normalize values to probabilities or clip them to probability--examples are sigmoid and relu, among others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain how back propagation works as if you were explaining it to a five year-old. Use your own words, but feel free to reference external materials for this question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Answer Here - Change the Cell to Markdown\n",
    "\n",
    "- **Backpropagation:** Since, hypothetically, im explaing it to a five year old: Backpropagation is a handy trick that helps my computer brain to continue to learn from it mistakes--many times in the hope that it will become better and better. Basically, just before it gives me an answer it goes 'WAIT, I got something better. give me a sec', until it-- for reasons I dont know yet, finds its final answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember our Simple Perceptron Class from Monday. In a simple prediction describe the process of making a prediction. How do you go from inputs to predicted output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Answer Here - Change the Cell to Markdown\n",
    "- **inputs to ouput:** You begin with giving your input layer--the initial step, observations. The size of the input layer is equal to the the number of observations(features) each instance has. From the input layer the values of each input node is passed to each node of the hidden layer. In the hidden layer the nodes take in the weighted inputs and then normalized in the activation function. Finally, the output layer behaves similar to the hidden layer but commonly the activation function is different depending on whether the target is a regression problem or classification--also, if its a binary classification or multiclass classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Q2\"></a>\n",
    "## 2. Simple Perceptron\n",
    "\n",
    "In this question, you will build two neural networks using Tensorflow Keras. After you build these two models, compare the results of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Our Dataset\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(-3, 3, 50),\n",
    "                     np.linspace(-3, 3, 50))\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "\"Use this X & y in the following 2 models\"\n",
    "X = rng.randn(300, 2)\n",
    "y = np.array(np.logical_xor(X[:, 0] > 0, X[:, 1] > 0), \n",
    "             dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50, 50), (50, 50))"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx.shape, yy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((300, 2), (300,))"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Perceptron\n",
    "Construct a simple perceptron using Keras. You model should have 1 dense layer with a single neuron and a sigmoid activation function. Your model should be called `model1` and make sure to save the results of your fit statement to a variable called `h1`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_590\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3699 (Dense)           (None, 1)                 3         \n",
      "_________________________________________________________________\n",
      "dense_3700 (Dense)           (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 5\n",
      "Trainable params: 5\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# simple perceptron model\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(1, input_dim=2, activation='relu')) # 1 dense layer with single neuron\n",
    "model1.add(Dense(1, activation='sigmoid')) # sigmoid activation function\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile\n",
    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "h1 = model1.fit(X, y, epochs=100, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 0s 393us/sample - loss: 0.5900 - accuracy: 0.6800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5899966557820638, 0.68]"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eval model1\n",
    "model1.evaluate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13ce5c940>]"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting the accuracy. \n",
    "plt.plot(h1.history['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron\n",
    "Now construct a multi-layer perceptron using. Here are some architecture suggestions: \n",
    "- 2 Hidden Layers\n",
    "- 5-32 Neurons in the Hidden Layers\n",
    "- Your pick of activation function and optimizer\n",
    "- Incorporate the Callback function below into your model\n",
    "\n",
    "Your model should be called `model2` and make sure to save the results of your fit statement to a variable called `h2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class myCallback(tf.keras.callbacks.Callback): \n",
    "    def on_epoch_end(self, epoch, logs={}): \n",
    "        if(logs.get('accuracy') > .99999):   \n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_612\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3790 (Dense)           (None, 1)                 3         \n",
      "_________________________________________________________________\n",
      "dense_3791 (Dense)           (None, 12)                24        \n",
      "_________________________________________________________________\n",
      "dense_3792 (Dense)           (None, 32)                416       \n",
      "_________________________________________________________________\n",
      "dense_3793 (Dense)           (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 476\n",
      "Trainable params: 476\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Dense(1, input_dim=2, activation='relu'))\n",
    "model2.add(Dense(12, activation='relu')),\n",
    "#model2.add(Dense(32, activation='relu')),\n",
    "model2.add(Dense(32, activation='tanh'))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping \n",
    "\n",
    "stop = EarlyStopping(monitor='accuracy', min_delta=.02, patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 300 samples\n",
      "Epoch 1/100\n",
      "300/300 [==============================] - 0s 2ms/sample - loss: 0.6553 - accuracy: 0.5533\n",
      "Epoch 2/100\n",
      "300/300 [==============================] - 0s 119us/sample - loss: 0.6421 - accuracy: 0.5900\n",
      "Epoch 3/100\n",
      "300/300 [==============================] - 0s 109us/sample - loss: 0.6315 - accuracy: 0.6233\n",
      "Epoch 4/100\n",
      "300/300 [==============================] - 0s 106us/sample - loss: 0.6239 - accuracy: 0.6433\n",
      "Epoch 5/100\n",
      "300/300 [==============================] - 0s 116us/sample - loss: 0.6169 - accuracy: 0.6500\n",
      "Epoch 6/100\n",
      "300/300 [==============================] - 0s 109us/sample - loss: 0.6107 - accuracy: 0.6600\n",
      "Epoch 7/100\n",
      "300/300 [==============================] - 0s 120us/sample - loss: 0.6057 - accuracy: 0.6533\n"
     ]
    }
   ],
   "source": [
    "h2 = model2.fit(X, y, epochs=100, callbacks=[stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 0s 437us/sample - loss: 0.6037 - accuracy: 0.6567\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6037402717272441, 0.6566667]"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze and Compare\n",
    "\n",
    "**Before you Start**: You will need to install an additional library for this next segment. Install the package `mlxtend` into the environment you are using for the sprint challenge.\n",
    "\n",
    "\n",
    "The cells below generate decision boundary plots of your models (`model1` & `model2`). Review the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlxtend\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/e2/1610a86284029abcad0ac9bc86cb19f9787fe6448ede467188b2a5121bb4/mlxtend-0.17.2-py2.py3-none-any.whl (1.3MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3MB 1.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.3 in /Users/yonipineda/opt/anaconda3/lib/python3.7/site-packages (from mlxtend) (0.21.3)\n",
      "Requirement already satisfied: scipy>=1.2.1 in /Users/yonipineda/opt/anaconda3/lib/python3.7/site-packages (from mlxtend) (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.16.2 in /Users/yonipineda/opt/anaconda3/lib/python3.7/site-packages (from mlxtend) (1.17.2)\n",
      "Requirement already satisfied: joblib>=0.13.2 in /Users/yonipineda/opt/anaconda3/lib/python3.7/site-packages (from mlxtend) (0.13.2)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in /Users/yonipineda/opt/anaconda3/lib/python3.7/site-packages (from mlxtend) (3.1.1)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /Users/yonipineda/opt/anaconda3/lib/python3.7/site-packages (from mlxtend) (0.25.1)\n",
      "Requirement already satisfied: setuptools in /Users/yonipineda/opt/anaconda3/lib/python3.7/site-packages (from mlxtend) (41.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/yonipineda/opt/anaconda3/lib/python3.7/site-packages (from matplotlib>=3.0.0->mlxtend) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/yonipineda/opt/anaconda3/lib/python3.7/site-packages (from matplotlib>=3.0.0->mlxtend) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/yonipineda/opt/anaconda3/lib/python3.7/site-packages (from matplotlib>=3.0.0->mlxtend) (2.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/yonipineda/opt/anaconda3/lib/python3.7/site-packages (from matplotlib>=3.0.0->mlxtend) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/yonipineda/opt/anaconda3/lib/python3.7/site-packages (from pandas>=0.24.2->mlxtend) (2019.3)\n",
      "Requirement already satisfied: six in /Users/yonipineda/opt/anaconda3/lib/python3.7/site-packages (from cycler>=0.10->matplotlib>=3.0.0->mlxtend) (1.12.0)\n",
      "Installing collected packages: mlxtend\n",
      "Successfully installed mlxtend-0.17.2\n"
     ]
    }
   ],
   "source": [
    "!pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Do Not change anything in this cell\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "# create a mesh to plot in\n",
    "x_min, x_max = X[:, 0].min() - .2, X[:, 0].max() + .2\n",
    "y_min, y_max = X[:, 1].min() - .2, X[:, 1].max() + .2\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "\n",
    "\n",
    "for clf, hist, name, grd in zip([model1,model2], [h1, h2],['Perceptron', 'Multi-Layer Perceptron'],[1,2]):\n",
    "\n",
    "    ax = plt.subplot(1,2, grd)\n",
    "    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n",
    "    title = f\"{name} with {hist.history['accuracy'][-1]:,.2f} Accuracy\"\n",
    "    plt.title(title)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does the Perceptron (`model1`) only achieve ~70% accuracy? What is the architectural property of the Multi-Layer Perceptron that allows it more accurately learn the relationship between X and y? \n",
    "\n",
    "Why might this property be useful in more complex data such as images?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thoughts on multiple iterations between the simple and multi-layer perceptron\n",
    "\n",
    "There is evidently randomness(rng) in the data-- i realized everytime i ran the dataset code it changed. Even when I dont, it still gets random results when fitting the data to the model--not too mention param tuning. I did at one point get above 70% but i made the mistake of running it again out of hope of getting a better score.   \n",
    "\n",
    "As for what the architectural property of the multi-layer perceptron is that allows for more accuracy in learning the relationship between X and y is the fact that multiple perceptron team up, stacked in several layers to solve the complex problem.\n",
    "\n",
    "This would be more useful due to the fact that it is more configurable and powerful with large datasets. Can better find connections that a simple perceptron just cant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Keras MMP <a id=\"Q3\"></a>\n",
    "\n",
    "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
    "Use the Heart Disease Dataset (binary classification)\n",
    "Use an appropriate loss function for a binary classification task\n",
    "Use an appropriate activation function on the final layer of your network.\n",
    "Train your model using verbose output for ease of grading.\n",
    "Use GridSearchCV or RandomSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
    "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "You must hyperparameter tune at least 3 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>260</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>1</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>295</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>124</td>\n",
       "      <td>197</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>136</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>303</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>212</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>157</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "211   61    1   0       120   260    0        1      140      1      3.6   \n",
       "132   42    1   1       120   295    0        1      162      0      0.0   \n",
       "296   63    0   0       124   197    0        1      136      1      0.0   \n",
       "59    57    0   0       128   303    0        0      159      0      0.0   \n",
       "26    59    1   2       150   212    1        1      157      0      1.6   \n",
       "\n",
       "     slope  ca  thal  target  \n",
       "211      1   1     3       0  \n",
       "132      2   0     2       1  \n",
       "296      1   0     2       0  \n",
       "59       2   1     2       1  \n",
       "26       2   0     2       1  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
    "df = df.sample(frac=1)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age         0\n",
       "sex         0\n",
       "cp          0\n",
       "trestbps    0\n",
       "chol        0\n",
       "fbs         0\n",
       "restecg     0\n",
       "thalach     0\n",
       "exang       0\n",
       "oldpeak     0\n",
       "slope       0\n",
       "ca          0\n",
       "thal        0\n",
       "target      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your Code Here\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age           int64\n",
       "sex           int64\n",
       "cp            int64\n",
       "trestbps      int64\n",
       "chol          int64\n",
       "fbs           int64\n",
       "restecg       int64\n",
       "thalach       int64\n",
       "exang         int64\n",
       "oldpeak     float64\n",
       "slope         int64\n",
       "ca            int64\n",
       "thal          int64\n",
       "target        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.731619</td>\n",
       "      <td>0.681005</td>\n",
       "      <td>-0.938515</td>\n",
       "      <td>-0.663867</td>\n",
       "      <td>0.265454</td>\n",
       "      <td>-0.417635</td>\n",
       "      <td>0.898962</td>\n",
       "      <td>-0.421862</td>\n",
       "      <td>1.435481</td>\n",
       "      <td>2.208842</td>\n",
       "      <td>-0.649113</td>\n",
       "      <td>0.265082</td>\n",
       "      <td>1.123029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.363869</td>\n",
       "      <td>0.681005</td>\n",
       "      <td>0.032031</td>\n",
       "      <td>-0.663867</td>\n",
       "      <td>0.941846</td>\n",
       "      <td>-0.417635</td>\n",
       "      <td>0.898962</td>\n",
       "      <td>0.540209</td>\n",
       "      <td>-0.696631</td>\n",
       "      <td>-0.896862</td>\n",
       "      <td>0.976352</td>\n",
       "      <td>-0.714429</td>\n",
       "      <td>-0.512922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.952197</td>\n",
       "      <td>-1.468418</td>\n",
       "      <td>-0.938515</td>\n",
       "      <td>-0.435415</td>\n",
       "      <td>-0.952051</td>\n",
       "      <td>-0.417635</td>\n",
       "      <td>0.898962</td>\n",
       "      <td>-0.596784</td>\n",
       "      <td>1.435481</td>\n",
       "      <td>-0.896862</td>\n",
       "      <td>-0.649113</td>\n",
       "      <td>-0.714429</td>\n",
       "      <td>-0.512922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.290464</td>\n",
       "      <td>-1.468418</td>\n",
       "      <td>-0.938515</td>\n",
       "      <td>-0.206964</td>\n",
       "      <td>1.096450</td>\n",
       "      <td>-0.417635</td>\n",
       "      <td>-1.005832</td>\n",
       "      <td>0.409017</td>\n",
       "      <td>-0.696631</td>\n",
       "      <td>-0.896862</td>\n",
       "      <td>0.976352</td>\n",
       "      <td>0.265082</td>\n",
       "      <td>-0.512922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.511041</td>\n",
       "      <td>0.681005</td>\n",
       "      <td>1.002577</td>\n",
       "      <td>1.049520</td>\n",
       "      <td>-0.662169</td>\n",
       "      <td>2.394438</td>\n",
       "      <td>0.898962</td>\n",
       "      <td>0.321556</td>\n",
       "      <td>-0.696631</td>\n",
       "      <td>0.483451</td>\n",
       "      <td>0.976352</td>\n",
       "      <td>-0.714429</td>\n",
       "      <td>-0.512922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age       sex        cp  trestbps      chol       fbs   restecg  \\\n",
       "0  0.731619  0.681005 -0.938515 -0.663867  0.265454 -0.417635  0.898962   \n",
       "1 -1.363869  0.681005  0.032031 -0.663867  0.941846 -0.417635  0.898962   \n",
       "2  0.952197 -1.468418 -0.938515 -0.435415 -0.952051 -0.417635  0.898962   \n",
       "3  0.290464 -1.468418 -0.938515 -0.206964  1.096450 -0.417635 -1.005832   \n",
       "4  0.511041  0.681005  1.002577  1.049520 -0.662169  2.394438  0.898962   \n",
       "\n",
       "    thalach     exang   oldpeak     slope        ca      thal  \n",
       "0 -0.421862  1.435481  2.208842 -0.649113  0.265082  1.123029  \n",
       "1  0.540209 -0.696631 -0.896862  0.976352 -0.714429 -0.512922  \n",
       "2 -0.596784  1.435481 -0.896862 -0.649113 -0.714429 -0.512922  \n",
       "3  0.409017 -0.696631 -0.896862  0.976352  0.265082 -0.512922  \n",
       "4  0.321556 -0.696631  0.483451  0.976352 -0.714429 -0.512922  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wrangle data \n",
    "def wrangle(data):\n",
    "    # make some assertions\n",
    "    assert data.isna().sum().sum() == 0\n",
    "    assert all([t.name in ['int64', 'float64'] for t in data.dtypes])\n",
    "    print(data.shape)\n",
    "    scaler = StandardScaler().fit_transform(data.drop('target', axis=1))\n",
    "    \n",
    "    return (pd.DataFrame(data=scaler, columns=data.drop('target', axis=1).columns),\n",
    "            data.target)\n",
    "\n",
    "X, y = wrangle(df)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Cannot clone object <tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x136ffa710>, as the constructor either does not set or modifies parameter layers_topo",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-0d007153e57c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mgrid_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# I have no clue what does erros is telling me. Will try a different method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/U4-S2-NeuralNetworks/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0;31m# of the params are estimators as well.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m             self.best_estimator_ = clone(clone(base_estimator).set_params(\n\u001b[0;32m--> 736\u001b[0;31m                 **self.best_params_))\n\u001b[0m\u001b[1;32m    737\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/U4-S2-NeuralNetworks/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     80\u001b[0m             raise RuntimeError('Cannot clone object %s, as the constructor '\n\u001b[1;32m     81\u001b[0m                                \u001b[0;34m'either does not set or modifies parameter %s'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                                (estimator, name))\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cannot clone object <tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x136ffa710>, as the constructor either does not set or modifies parameter layers_topo"
     ]
    }
   ],
   "source": [
    "# Import Hyperparameters\n",
    "inputs = X.shape[1]\n",
    "epochs = 22\n",
    "batch_size = 16\n",
    "\n",
    "def create_model(optimizer, layers_topo, loss): \n",
    "    \n",
    "    # Create Model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(inputs, activation='sigmoid', input_shape=(inputs,)))\n",
    "    for k in layers_topo: \n",
    "        model.add(Dense(k, activation='relu'))\n",
    "        model.add(Dense(k, activation='relu'))\n",
    "        model.add(Dense(k, activation='sigmoid'))\n",
    "    model.add(Dense(1))\n",
    "    # Compile Model\n",
    "    model.compile(optimizer=optimizer, metrics=['accuracy'], loss=loss)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# hyper param for tuning\n",
    "batch_size = [64, 128]\n",
    "epochs = [256]\n",
    "losses = ['binary_crossentropy', 'mse']\n",
    "optimizers = ['adam', 'SGD']\n",
    "layering = [[64,32,64], [32,64,32]]\n",
    "\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs, loss=losses, optimizer=optimizers, layers_topo=layering)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, iid=False, verbose=0)\n",
    "\n",
    "grid_result = grid.fit(X, y) # I have no clue what that error is telling me. Will try a different method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method number 2\n",
    "\n",
    "I could not resolve that error from the previous code and time was running out. This method takes much from our lectures! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_325\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1713 (Dense)           (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 13)                0         \n",
      "_________________________________________________________________\n",
      "dense_1714 (Dense)           (None, 1)                 14        \n",
      "=================================================================\n",
      "Total params: 196\n",
      "Trainable params: 196\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# model\n",
    "model = Sequential()\n",
    "\n",
    "#hidden\n",
    "model.add(Dense(13, input_dim=13, activation='relu'))\n",
    "model.add(Dropout(.2))\n",
    "\n",
    "#output\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                    optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 205 samples, validate on 37 samples\n",
      "Epoch 1/50\n",
      "205/205 [==============================] - 1s 3ms/sample - loss: 0.8001 - accuracy: 0.5415 - val_loss: 0.6896 - val_accuracy: 0.5946\n",
      "Epoch 2/50\n",
      "205/205 [==============================] - 0s 296us/sample - loss: 0.7209 - accuracy: 0.6146 - val_loss: 0.6649 - val_accuracy: 0.6216\n",
      "Epoch 3/50\n",
      "205/205 [==============================] - 0s 244us/sample - loss: 0.7243 - accuracy: 0.5951 - val_loss: 0.6416 - val_accuracy: 0.5946\n",
      "Epoch 4/50\n",
      "205/205 [==============================] - 0s 262us/sample - loss: 0.6728 - accuracy: 0.6732 - val_loss: 0.6220 - val_accuracy: 0.6486\n",
      "Epoch 5/50\n",
      "205/205 [==============================] - 0s 256us/sample - loss: 0.6403 - accuracy: 0.6732 - val_loss: 0.6049 - val_accuracy: 0.6757\n",
      "Epoch 6/50\n",
      "205/205 [==============================] - 0s 248us/sample - loss: 0.6292 - accuracy: 0.6829 - val_loss: 0.5888 - val_accuracy: 0.6757\n",
      "Epoch 7/50\n",
      "205/205 [==============================] - 0s 225us/sample - loss: 0.5924 - accuracy: 0.7171 - val_loss: 0.5741 - val_accuracy: 0.7027\n",
      "Epoch 8/50\n",
      "205/205 [==============================] - 0s 227us/sample - loss: 0.5907 - accuracy: 0.7220 - val_loss: 0.5604 - val_accuracy: 0.7027\n",
      "Epoch 9/50\n",
      "205/205 [==============================] - 0s 245us/sample - loss: 0.5654 - accuracy: 0.7415 - val_loss: 0.5484 - val_accuracy: 0.7027\n",
      "Epoch 10/50\n",
      "205/205 [==============================] - 0s 267us/sample - loss: 0.5545 - accuracy: 0.7366 - val_loss: 0.5370 - val_accuracy: 0.7297\n",
      "Epoch 11/50\n",
      "205/205 [==============================] - 0s 276us/sample - loss: 0.5208 - accuracy: 0.7512 - val_loss: 0.5253 - val_accuracy: 0.7568\n",
      "Epoch 12/50\n",
      "205/205 [==============================] - 0s 230us/sample - loss: 0.5302 - accuracy: 0.7707 - val_loss: 0.5169 - val_accuracy: 0.7838\n",
      "Epoch 13/50\n",
      "205/205 [==============================] - 0s 218us/sample - loss: 0.5229 - accuracy: 0.7610 - val_loss: 0.5092 - val_accuracy: 0.7838\n",
      "Epoch 14/50\n",
      "205/205 [==============================] - 0s 259us/sample - loss: 0.4865 - accuracy: 0.7854 - val_loss: 0.5020 - val_accuracy: 0.7838\n",
      "Epoch 15/50\n",
      "205/205 [==============================] - 0s 253us/sample - loss: 0.4830 - accuracy: 0.8146 - val_loss: 0.4956 - val_accuracy: 0.7838\n",
      "Epoch 16/50\n",
      "205/205 [==============================] - 0s 286us/sample - loss: 0.4973 - accuracy: 0.7707 - val_loss: 0.4893 - val_accuracy: 0.7838\n",
      "Epoch 17/50\n",
      "205/205 [==============================] - 0s 314us/sample - loss: 0.4813 - accuracy: 0.7902 - val_loss: 0.4839 - val_accuracy: 0.7838\n",
      "Epoch 18/50\n",
      "205/205 [==============================] - 0s 285us/sample - loss: 0.5081 - accuracy: 0.7805 - val_loss: 0.4787 - val_accuracy: 0.8108\n",
      "Epoch 19/50\n",
      "205/205 [==============================] - 0s 303us/sample - loss: 0.4529 - accuracy: 0.8098 - val_loss: 0.4741 - val_accuracy: 0.8108\n",
      "Epoch 20/50\n",
      "205/205 [==============================] - 0s 289us/sample - loss: 0.4678 - accuracy: 0.8049 - val_loss: 0.4698 - val_accuracy: 0.8108\n",
      "Epoch 21/50\n",
      "205/205 [==============================] - 0s 270us/sample - loss: 0.4407 - accuracy: 0.8098 - val_loss: 0.4662 - val_accuracy: 0.8108\n",
      "Epoch 22/50\n",
      "205/205 [==============================] - 0s 282us/sample - loss: 0.4341 - accuracy: 0.8195 - val_loss: 0.4630 - val_accuracy: 0.8108\n",
      "Epoch 23/50\n",
      "205/205 [==============================] - 0s 246us/sample - loss: 0.4580 - accuracy: 0.8146 - val_loss: 0.4601 - val_accuracy: 0.8108\n",
      "Epoch 24/50\n",
      "205/205 [==============================] - 0s 273us/sample - loss: 0.4285 - accuracy: 0.8195 - val_loss: 0.4572 - val_accuracy: 0.8108\n",
      "Epoch 25/50\n",
      "205/205 [==============================] - 0s 249us/sample - loss: 0.4160 - accuracy: 0.8293 - val_loss: 0.4538 - val_accuracy: 0.8108\n",
      "Epoch 26/50\n",
      "205/205 [==============================] - 0s 262us/sample - loss: 0.4121 - accuracy: 0.8195 - val_loss: 0.4514 - val_accuracy: 0.8108\n",
      "Epoch 27/50\n",
      "205/205 [==============================] - 0s 243us/sample - loss: 0.4124 - accuracy: 0.8390 - val_loss: 0.4490 - val_accuracy: 0.8108\n",
      "Epoch 28/50\n",
      "205/205 [==============================] - 0s 255us/sample - loss: 0.4115 - accuracy: 0.8341 - val_loss: 0.4472 - val_accuracy: 0.7838\n",
      "Epoch 29/50\n",
      "205/205 [==============================] - 0s 301us/sample - loss: 0.4030 - accuracy: 0.8098 - val_loss: 0.4460 - val_accuracy: 0.7838\n",
      "Epoch 30/50\n",
      "205/205 [==============================] - 0s 334us/sample - loss: 0.3754 - accuracy: 0.8244 - val_loss: 0.4444 - val_accuracy: 0.7838\n",
      "Epoch 31/50\n",
      "205/205 [==============================] - 0s 454us/sample - loss: 0.3977 - accuracy: 0.8049 - val_loss: 0.4429 - val_accuracy: 0.7838\n",
      "Epoch 32/50\n",
      "205/205 [==============================] - 0s 277us/sample - loss: 0.4159 - accuracy: 0.8244 - val_loss: 0.4417 - val_accuracy: 0.7838\n",
      "Epoch 33/50\n",
      "205/205 [==============================] - 0s 253us/sample - loss: 0.3921 - accuracy: 0.8488 - val_loss: 0.4399 - val_accuracy: 0.7838\n",
      "Epoch 34/50\n",
      "205/205 [==============================] - 0s 252us/sample - loss: 0.3876 - accuracy: 0.8244 - val_loss: 0.4389 - val_accuracy: 0.7838\n",
      "Epoch 35/50\n",
      "205/205 [==============================] - 0s 235us/sample - loss: 0.3934 - accuracy: 0.8341 - val_loss: 0.4378 - val_accuracy: 0.7838\n",
      "Epoch 36/50\n",
      "205/205 [==============================] - 0s 250us/sample - loss: 0.3694 - accuracy: 0.8341 - val_loss: 0.4365 - val_accuracy: 0.7838\n",
      "Epoch 37/50\n",
      "205/205 [==============================] - 0s 249us/sample - loss: 0.3947 - accuracy: 0.8439 - val_loss: 0.4350 - val_accuracy: 0.7838\n",
      "Epoch 38/50\n",
      "205/205 [==============================] - 0s 236us/sample - loss: 0.3278 - accuracy: 0.8537 - val_loss: 0.4342 - val_accuracy: 0.7838\n",
      "Epoch 39/50\n",
      "205/205 [==============================] - 0s 229us/sample - loss: 0.3705 - accuracy: 0.8341 - val_loss: 0.4336 - val_accuracy: 0.7838\n",
      "Epoch 40/50\n",
      "205/205 [==============================] - 0s 226us/sample - loss: 0.3734 - accuracy: 0.8537 - val_loss: 0.4323 - val_accuracy: 0.7838\n",
      "Epoch 41/50\n",
      "205/205 [==============================] - 0s 234us/sample - loss: 0.3741 - accuracy: 0.8439 - val_loss: 0.4320 - val_accuracy: 0.7838\n",
      "Epoch 42/50\n",
      "205/205 [==============================] - 0s 233us/sample - loss: 0.3577 - accuracy: 0.8488 - val_loss: 0.4308 - val_accuracy: 0.7838\n",
      "Epoch 43/50\n",
      "205/205 [==============================] - 0s 241us/sample - loss: 0.3630 - accuracy: 0.8537 - val_loss: 0.4301 - val_accuracy: 0.7838\n",
      "Epoch 44/50\n",
      "205/205 [==============================] - 0s 228us/sample - loss: 0.3637 - accuracy: 0.8488 - val_loss: 0.4295 - val_accuracy: 0.7838\n",
      "Epoch 45/50\n",
      "205/205 [==============================] - 0s 227us/sample - loss: 0.3504 - accuracy: 0.8683 - val_loss: 0.4284 - val_accuracy: 0.7838\n",
      "Epoch 46/50\n",
      "205/205 [==============================] - 0s 247us/sample - loss: 0.3504 - accuracy: 0.8537 - val_loss: 0.4278 - val_accuracy: 0.7838\n",
      "Epoch 47/50\n",
      "205/205 [==============================] - 0s 247us/sample - loss: 0.3587 - accuracy: 0.8683 - val_loss: 0.4270 - val_accuracy: 0.7838\n",
      "Epoch 48/50\n",
      "205/205 [==============================] - 0s 257us/sample - loss: 0.3428 - accuracy: 0.8439 - val_loss: 0.4270 - val_accuracy: 0.7838\n",
      "Epoch 49/50\n",
      "205/205 [==============================] - 0s 222us/sample - loss: 0.3470 - accuracy: 0.8634 - val_loss: 0.4267 - val_accuracy: 0.7838\n",
      "Epoch 50/50\n",
      "205/205 [==============================] - 0s 228us/sample - loss: 0.3552 - accuracy: 0.8439 - val_loss: 0.4271 - val_accuracy: 0.7838\n",
      "61/61 [==============================] - 0s 180us/sample - loss: 0.4055 - accuracy: 0.8197\n",
      "Baseline accuracy: 81.96721076965332\n"
     ]
    }
   ],
   "source": [
    "results = model.fit(X_train, y_train, batch_size=16, epochs=50, validation_split=.15, verbose=1)\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(f'Baseline {model.metrics_names[1]}: {scores[1]*100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 161 samples\n",
      "Epoch 1/100\n",
      "161/161 [==============================] - 1s 3ms/sample - loss: 0.8546 - accuracy: 0.4472\n",
      "Epoch 2/100\n",
      "161/161 [==============================] - 0s 266us/sample - loss: 0.8397 - accuracy: 0.4845\n",
      "Epoch 3/100\n",
      "161/161 [==============================] - 0s 263us/sample - loss: 0.7225 - accuracy: 0.5342\n",
      "Epoch 4/100\n",
      "161/161 [==============================] - 0s 263us/sample - loss: 0.7671 - accuracy: 0.5342\n",
      "Epoch 5/100\n",
      "161/161 [==============================] - 0s 267us/sample - loss: 0.6894 - accuracy: 0.5963\n",
      "Epoch 6/100\n",
      "161/161 [==============================] - 0s 258us/sample - loss: 0.6725 - accuracy: 0.6025\n",
      "Epoch 7/100\n",
      "161/161 [==============================] - 0s 292us/sample - loss: 0.6564 - accuracy: 0.6211\n",
      "Epoch 8/100\n",
      "161/161 [==============================] - 0s 334us/sample - loss: 0.6490 - accuracy: 0.6522\n",
      "Epoch 9/100\n",
      "161/161 [==============================] - 0s 283us/sample - loss: 0.5850 - accuracy: 0.6894\n",
      "Epoch 10/100\n",
      "161/161 [==============================] - 0s 285us/sample - loss: 0.6059 - accuracy: 0.6460\n",
      "Epoch 11/100\n",
      "161/161 [==============================] - 0s 270us/sample - loss: 0.5727 - accuracy: 0.7267\n",
      "Epoch 12/100\n",
      "161/161 [==============================] - 0s 290us/sample - loss: 0.5774 - accuracy: 0.6646\n",
      "Epoch 13/100\n",
      "161/161 [==============================] - 0s 264us/sample - loss: 0.5309 - accuracy: 0.7391\n",
      "Epoch 14/100\n",
      "161/161 [==============================] - 0s 303us/sample - loss: 0.5273 - accuracy: 0.7267\n",
      "Epoch 15/100\n",
      "161/161 [==============================] - 0s 402us/sample - loss: 0.4945 - accuracy: 0.7826\n",
      "Epoch 16/100\n",
      "161/161 [==============================] - 0s 389us/sample - loss: 0.4745 - accuracy: 0.7826\n",
      "Epoch 17/100\n",
      "161/161 [==============================] - 0s 323us/sample - loss: 0.4693 - accuracy: 0.7453\n",
      "Epoch 18/100\n",
      "161/161 [==============================] - 0s 320us/sample - loss: 0.4760 - accuracy: 0.7516\n",
      "Epoch 19/100\n",
      "161/161 [==============================] - 0s 286us/sample - loss: 0.4328 - accuracy: 0.8012\n",
      "Epoch 20/100\n",
      "161/161 [==============================] - 0s 295us/sample - loss: 0.4654 - accuracy: 0.7888\n",
      "Epoch 21/100\n",
      "161/161 [==============================] - 0s 288us/sample - loss: 0.4505 - accuracy: 0.7702\n",
      "Epoch 22/100\n",
      "161/161 [==============================] - 0s 291us/sample - loss: 0.4203 - accuracy: 0.7826\n",
      "Epoch 23/100\n",
      "161/161 [==============================] - 0s 287us/sample - loss: 0.4373 - accuracy: 0.8012\n",
      "Epoch 24/100\n",
      "161/161 [==============================] - 0s 301us/sample - loss: 0.4075 - accuracy: 0.7950\n",
      "Epoch 25/100\n",
      "161/161 [==============================] - 0s 269us/sample - loss: 0.3830 - accuracy: 0.7888\n",
      "Epoch 26/100\n",
      "161/161 [==============================] - 0s 324us/sample - loss: 0.3830 - accuracy: 0.8199\n",
      "Epoch 27/100\n",
      "161/161 [==============================] - 0s 278us/sample - loss: 0.3849 - accuracy: 0.8199\n",
      "Epoch 28/100\n",
      "161/161 [==============================] - 0s 261us/sample - loss: 0.4069 - accuracy: 0.7516\n",
      "Epoch 29/100\n",
      "161/161 [==============================] - 0s 418us/sample - loss: 0.3695 - accuracy: 0.8385\n",
      "Epoch 30/100\n",
      "161/161 [==============================] - 0s 316us/sample - loss: 0.3747 - accuracy: 0.8137\n",
      "Epoch 31/100\n",
      "161/161 [==============================] - 0s 342us/sample - loss: 0.3734 - accuracy: 0.8075\n",
      "Epoch 32/100\n",
      "161/161 [==============================] - 0s 287us/sample - loss: 0.3857 - accuracy: 0.8075\n",
      "Epoch 33/100\n",
      "161/161 [==============================] - 0s 329us/sample - loss: 0.3750 - accuracy: 0.8075\n",
      "Epoch 34/100\n",
      "161/161 [==============================] - 0s 314us/sample - loss: 0.3605 - accuracy: 0.8075\n",
      "Epoch 35/100\n",
      "161/161 [==============================] - 0s 343us/sample - loss: 0.3369 - accuracy: 0.8385\n",
      "Epoch 36/100\n",
      "161/161 [==============================] - 0s 278us/sample - loss: 0.3396 - accuracy: 0.8323\n",
      "Epoch 37/100\n",
      "161/161 [==============================] - 0s 314us/sample - loss: 0.3628 - accuracy: 0.8323\n",
      "Epoch 38/100\n",
      "161/161 [==============================] - 0s 331us/sample - loss: 0.3430 - accuracy: 0.8323\n",
      "Epoch 39/100\n",
      "161/161 [==============================] - 0s 351us/sample - loss: 0.3423 - accuracy: 0.8385\n",
      "Epoch 40/100\n",
      "161/161 [==============================] - 0s 290us/sample - loss: 0.3266 - accuracy: 0.8571\n",
      "Epoch 41/100\n",
      "161/161 [==============================] - 0s 269us/sample - loss: 0.3306 - accuracy: 0.8509\n",
      "Epoch 42/100\n",
      "161/161 [==============================] - 0s 333us/sample - loss: 0.3600 - accuracy: 0.8137\n",
      "Epoch 43/100\n",
      "161/161 [==============================] - 0s 311us/sample - loss: 0.3304 - accuracy: 0.8571\n",
      "Epoch 44/100\n",
      "161/161 [==============================] - 0s 267us/sample - loss: 0.3413 - accuracy: 0.8447\n",
      "Epoch 45/100\n",
      "161/161 [==============================] - 0s 242us/sample - loss: 0.3317 - accuracy: 0.8323\n",
      "Epoch 46/100\n",
      "161/161 [==============================] - 0s 260us/sample - loss: 0.3268 - accuracy: 0.8571\n",
      "Epoch 47/100\n",
      "161/161 [==============================] - 0s 254us/sample - loss: 0.3396 - accuracy: 0.8447\n",
      "Epoch 48/100\n",
      "161/161 [==============================] - 0s 278us/sample - loss: 0.3345 - accuracy: 0.8447\n",
      "Epoch 49/100\n",
      "161/161 [==============================] - 0s 275us/sample - loss: 0.3305 - accuracy: 0.8509\n",
      "Epoch 50/100\n",
      "161/161 [==============================] - 0s 303us/sample - loss: 0.3199 - accuracy: 0.8509\n",
      "Epoch 51/100\n",
      "161/161 [==============================] - 0s 282us/sample - loss: 0.3199 - accuracy: 0.8385\n",
      "Epoch 52/100\n",
      "161/161 [==============================] - 0s 447us/sample - loss: 0.3215 - accuracy: 0.8696\n",
      "Epoch 53/100\n",
      "161/161 [==============================] - 0s 322us/sample - loss: 0.3113 - accuracy: 0.8571\n",
      "Epoch 54/100\n",
      "161/161 [==============================] - 0s 295us/sample - loss: 0.3048 - accuracy: 0.8509\n",
      "Epoch 55/100\n",
      "161/161 [==============================] - 0s 302us/sample - loss: 0.3144 - accuracy: 0.8696\n",
      "Epoch 56/100\n",
      "161/161 [==============================] - 0s 275us/sample - loss: 0.3055 - accuracy: 0.8509\n",
      "Epoch 57/100\n",
      "161/161 [==============================] - 0s 338us/sample - loss: 0.3143 - accuracy: 0.8634\n",
      "Epoch 58/100\n",
      "161/161 [==============================] - 0s 263us/sample - loss: 0.3131 - accuracy: 0.8696\n",
      "Epoch 59/100\n",
      "161/161 [==============================] - 0s 326us/sample - loss: 0.3005 - accuracy: 0.8696\n",
      "Epoch 60/100\n",
      "161/161 [==============================] - 0s 342us/sample - loss: 0.3168 - accuracy: 0.8634\n",
      "Epoch 61/100\n",
      "161/161 [==============================] - 0s 273us/sample - loss: 0.3135 - accuracy: 0.8696\n",
      "Epoch 62/100\n",
      "161/161 [==============================] - 0s 248us/sample - loss: 0.3073 - accuracy: 0.8820\n",
      "Epoch 63/100\n",
      "161/161 [==============================] - 0s 301us/sample - loss: 0.2935 - accuracy: 0.8696\n",
      "Epoch 64/100\n",
      "161/161 [==============================] - 0s 289us/sample - loss: 0.3015 - accuracy: 0.8758\n",
      "Epoch 65/100\n",
      "161/161 [==============================] - 0s 219us/sample - loss: 0.2782 - accuracy: 0.8820\n",
      "Epoch 66/100\n",
      "161/161 [==============================] - 0s 287us/sample - loss: 0.2989 - accuracy: 0.8571\n",
      "Epoch 67/100\n",
      "161/161 [==============================] - 0s 302us/sample - loss: 0.3187 - accuracy: 0.8634\n",
      "Epoch 68/100\n",
      "161/161 [==============================] - 0s 286us/sample - loss: 0.3176 - accuracy: 0.8634\n",
      "Epoch 69/100\n",
      "161/161 [==============================] - 0s 282us/sample - loss: 0.2781 - accuracy: 0.8758\n",
      "Epoch 70/100\n",
      "161/161 [==============================] - 0s 294us/sample - loss: 0.2655 - accuracy: 0.9255\n",
      "Epoch 71/100\n",
      "161/161 [==============================] - 0s 292us/sample - loss: 0.2965 - accuracy: 0.8820\n",
      "Epoch 72/100\n",
      "161/161 [==============================] - 0s 257us/sample - loss: 0.2723 - accuracy: 0.8882\n",
      "Epoch 73/100\n",
      "161/161 [==============================] - 0s 273us/sample - loss: 0.3008 - accuracy: 0.8882\n",
      "Epoch 74/100\n",
      "161/161 [==============================] - 0s 252us/sample - loss: 0.2848 - accuracy: 0.8696\n",
      "Epoch 75/100\n",
      "161/161 [==============================] - 0s 352us/sample - loss: 0.2847 - accuracy: 0.8944\n",
      "Epoch 76/100\n",
      "161/161 [==============================] - 0s 266us/sample - loss: 0.2806 - accuracy: 0.8696\n",
      "Epoch 77/100\n",
      "161/161 [==============================] - 0s 268us/sample - loss: 0.2791 - accuracy: 0.8758\n",
      "Epoch 78/100\n",
      "161/161 [==============================] - 0s 267us/sample - loss: 0.3039 - accuracy: 0.8758\n",
      "Epoch 79/100\n",
      "161/161 [==============================] - 0s 344us/sample - loss: 0.2776 - accuracy: 0.8758\n",
      "Epoch 80/100\n",
      "161/161 [==============================] - 0s 532us/sample - loss: 0.2666 - accuracy: 0.8944\n",
      "Epoch 81/100\n",
      "161/161 [==============================] - 0s 312us/sample - loss: 0.2777 - accuracy: 0.9130\n",
      "Epoch 82/100\n",
      "161/161 [==============================] - 0s 275us/sample - loss: 0.2762 - accuracy: 0.8944\n",
      "Epoch 83/100\n",
      "161/161 [==============================] - 0s 317us/sample - loss: 0.2879 - accuracy: 0.8571\n",
      "Epoch 84/100\n",
      "161/161 [==============================] - 0s 296us/sample - loss: 0.3035 - accuracy: 0.8571\n",
      "Epoch 85/100\n",
      "161/161 [==============================] - 0s 286us/sample - loss: 0.2881 - accuracy: 0.8944\n",
      "Epoch 86/100\n",
      "161/161 [==============================] - 0s 233us/sample - loss: 0.2806 - accuracy: 0.8944\n",
      "Epoch 87/100\n",
      "161/161 [==============================] - 0s 261us/sample - loss: 0.2621 - accuracy: 0.9193\n",
      "Epoch 88/100\n",
      "161/161 [==============================] - 0s 251us/sample - loss: 0.2743 - accuracy: 0.8696\n",
      "Epoch 89/100\n",
      "161/161 [==============================] - 0s 248us/sample - loss: 0.2709 - accuracy: 0.8882\n",
      "Epoch 90/100\n",
      "161/161 [==============================] - 0s 247us/sample - loss: 0.2521 - accuracy: 0.8882\n",
      "Epoch 91/100\n",
      "161/161 [==============================] - 0s 239us/sample - loss: 0.2734 - accuracy: 0.8944\n",
      "Epoch 92/100\n",
      "161/161 [==============================] - 0s 271us/sample - loss: 0.2572 - accuracy: 0.9006\n",
      "Epoch 93/100\n",
      "161/161 [==============================] - 0s 234us/sample - loss: 0.2730 - accuracy: 0.9068\n",
      "Epoch 94/100\n",
      "161/161 [==============================] - 0s 312us/sample - loss: 0.2845 - accuracy: 0.8634\n",
      "Epoch 95/100\n",
      "161/161 [==============================] - 0s 269us/sample - loss: 0.2485 - accuracy: 0.9006\n",
      "Epoch 96/100\n",
      "161/161 [==============================] - 0s 270us/sample - loss: 0.2847 - accuracy: 0.9068\n",
      "Epoch 97/100\n",
      "161/161 [==============================] - 0s 320us/sample - loss: 0.2630 - accuracy: 0.9068\n",
      "Epoch 98/100\n",
      "161/161 [==============================] - 0s 252us/sample - loss: 0.2919 - accuracy: 0.8882\n",
      "Epoch 99/100\n",
      "161/161 [==============================] - 0s 270us/sample - loss: 0.2631 - accuracy: 0.8944\n",
      "Epoch 100/100\n",
      "161/161 [==============================] - 0s 290us/sample - loss: 0.2557 - accuracy: 0.8882\n",
      "81/81 [==============================] - 0s 2ms/sample - loss: 0.3717 - accuracy: 0.8519\n",
      "Train on 161 samples\n",
      "Epoch 1/100\n",
      "161/161 [==============================] - 1s 4ms/sample - loss: 0.7158 - accuracy: 0.5714\n",
      "Epoch 2/100\n",
      "161/161 [==============================] - 0s 255us/sample - loss: 0.6955 - accuracy: 0.5776\n",
      "Epoch 3/100\n",
      "161/161 [==============================] - 0s 229us/sample - loss: 0.6565 - accuracy: 0.6522\n",
      "Epoch 4/100\n",
      "161/161 [==============================] - 0s 271us/sample - loss: 0.6662 - accuracy: 0.6460\n",
      "Epoch 5/100\n",
      "161/161 [==============================] - 0s 259us/sample - loss: 0.5953 - accuracy: 0.7143\n",
      "Epoch 6/100\n",
      "161/161 [==============================] - 0s 421us/sample - loss: 0.5737 - accuracy: 0.7205\n",
      "Epoch 7/100\n",
      "161/161 [==============================] - 0s 256us/sample - loss: 0.5235 - accuracy: 0.7826\n",
      "Epoch 8/100\n",
      "161/161 [==============================] - 0s 304us/sample - loss: 0.5337 - accuracy: 0.7329\n",
      "Epoch 9/100\n",
      "161/161 [==============================] - 0s 256us/sample - loss: 0.5394 - accuracy: 0.7950\n",
      "Epoch 10/100\n",
      "161/161 [==============================] - 0s 250us/sample - loss: 0.5188 - accuracy: 0.7826\n",
      "Epoch 11/100\n",
      "161/161 [==============================] - 0s 329us/sample - loss: 0.5097 - accuracy: 0.7516\n",
      "Epoch 12/100\n",
      "161/161 [==============================] - 0s 244us/sample - loss: 0.5142 - accuracy: 0.7391\n",
      "Epoch 13/100\n",
      "161/161 [==============================] - 0s 278us/sample - loss: 0.4685 - accuracy: 0.7764\n",
      "Epoch 14/100\n",
      "161/161 [==============================] - 0s 249us/sample - loss: 0.4570 - accuracy: 0.7826\n",
      "Epoch 15/100\n",
      "161/161 [==============================] - 0s 271us/sample - loss: 0.4543 - accuracy: 0.8075\n",
      "Epoch 16/100\n",
      "161/161 [==============================] - 0s 231us/sample - loss: 0.4361 - accuracy: 0.7950\n",
      "Epoch 17/100\n",
      "161/161 [==============================] - 0s 270us/sample - loss: 0.4245 - accuracy: 0.8075\n",
      "Epoch 18/100\n",
      "161/161 [==============================] - 0s 230us/sample - loss: 0.4181 - accuracy: 0.8075\n",
      "Epoch 19/100\n",
      "161/161 [==============================] - 0s 306us/sample - loss: 0.4022 - accuracy: 0.8199\n",
      "Epoch 20/100\n",
      "161/161 [==============================] - 0s 246us/sample - loss: 0.4051 - accuracy: 0.8261\n",
      "Epoch 21/100\n",
      "161/161 [==============================] - 0s 278us/sample - loss: 0.4020 - accuracy: 0.8075\n",
      "Epoch 22/100\n",
      "161/161 [==============================] - 0s 237us/sample - loss: 0.3846 - accuracy: 0.8323\n",
      "Epoch 23/100\n",
      "161/161 [==============================] - 0s 300us/sample - loss: 0.3816 - accuracy: 0.8447\n",
      "Epoch 24/100\n",
      "161/161 [==============================] - 0s 243us/sample - loss: 0.3405 - accuracy: 0.8571\n",
      "Epoch 25/100\n",
      "161/161 [==============================] - 0s 297us/sample - loss: 0.3596 - accuracy: 0.8696\n",
      "Epoch 26/100\n",
      "161/161 [==============================] - 0s 239us/sample - loss: 0.3899 - accuracy: 0.8075\n",
      "Epoch 27/100\n",
      "161/161 [==============================] - 0s 249us/sample - loss: 0.3585 - accuracy: 0.8323\n",
      "Epoch 28/100\n",
      "161/161 [==============================] - 0s 259us/sample - loss: 0.3448 - accuracy: 0.8634\n",
      "Epoch 29/100\n",
      "161/161 [==============================] - 0s 235us/sample - loss: 0.3557 - accuracy: 0.8696\n",
      "Epoch 30/100\n",
      "161/161 [==============================] - 0s 248us/sample - loss: 0.3316 - accuracy: 0.8509\n",
      "Epoch 31/100\n",
      "161/161 [==============================] - 0s 276us/sample - loss: 0.3383 - accuracy: 0.8820\n",
      "Epoch 32/100\n",
      "161/161 [==============================] - 0s 263us/sample - loss: 0.3479 - accuracy: 0.8447\n",
      "Epoch 33/100\n",
      "161/161 [==============================] - 0s 235us/sample - loss: 0.3502 - accuracy: 0.8571\n",
      "Epoch 34/100\n",
      "161/161 [==============================] - 0s 255us/sample - loss: 0.3359 - accuracy: 0.8509\n",
      "Epoch 35/100\n",
      "161/161 [==============================] - 0s 253us/sample - loss: 0.3503 - accuracy: 0.8509\n",
      "Epoch 36/100\n",
      "161/161 [==============================] - 0s 214us/sample - loss: 0.3377 - accuracy: 0.8882\n",
      "Epoch 37/100\n",
      "161/161 [==============================] - 0s 253us/sample - loss: 0.3056 - accuracy: 0.8882\n",
      "Epoch 38/100\n",
      "161/161 [==============================] - 0s 230us/sample - loss: 0.3288 - accuracy: 0.8944\n",
      "Epoch 39/100\n",
      "161/161 [==============================] - 0s 269us/sample - loss: 0.3307 - accuracy: 0.8820\n",
      "Epoch 40/100\n",
      "161/161 [==============================] - 0s 278us/sample - loss: 0.2975 - accuracy: 0.8944\n",
      "Epoch 41/100\n",
      "161/161 [==============================] - 0s 269us/sample - loss: 0.3188 - accuracy: 0.8634\n",
      "Epoch 42/100\n",
      "161/161 [==============================] - 0s 474us/sample - loss: 0.3130 - accuracy: 0.8820\n",
      "Epoch 43/100\n",
      "161/161 [==============================] - 0s 526us/sample - loss: 0.3242 - accuracy: 0.8634\n",
      "Epoch 44/100\n",
      "161/161 [==============================] - 0s 454us/sample - loss: 0.2922 - accuracy: 0.8758\n",
      "Epoch 45/100\n",
      "161/161 [==============================] - 0s 364us/sample - loss: 0.3026 - accuracy: 0.8820\n",
      "Epoch 46/100\n",
      "161/161 [==============================] - 0s 570us/sample - loss: 0.3034 - accuracy: 0.8758\n",
      "Epoch 47/100\n",
      "161/161 [==============================] - 0s 514us/sample - loss: 0.3408 - accuracy: 0.8509\n",
      "Epoch 48/100\n",
      "161/161 [==============================] - 0s 407us/sample - loss: 0.2928 - accuracy: 0.8696\n",
      "Epoch 49/100\n",
      "161/161 [==============================] - 0s 258us/sample - loss: 0.3102 - accuracy: 0.8820\n",
      "Epoch 50/100\n",
      "161/161 [==============================] - 0s 237us/sample - loss: 0.3171 - accuracy: 0.8758\n",
      "Epoch 51/100\n",
      "161/161 [==============================] - 0s 261us/sample - loss: 0.2729 - accuracy: 0.8758\n",
      "Epoch 52/100\n",
      "161/161 [==============================] - 0s 312us/sample - loss: 0.3077 - accuracy: 0.8882\n",
      "Epoch 53/100\n",
      "161/161 [==============================] - 0s 380us/sample - loss: 0.2779 - accuracy: 0.8882\n",
      "Epoch 54/100\n",
      "161/161 [==============================] - 0s 444us/sample - loss: 0.2941 - accuracy: 0.8696\n",
      "Epoch 55/100\n",
      "161/161 [==============================] - 0s 1ms/sample - loss: 0.2806 - accuracy: 0.8882\n",
      "Epoch 56/100\n",
      "161/161 [==============================] - 0s 390us/sample - loss: 0.2918 - accuracy: 0.8634\n",
      "Epoch 57/100\n",
      "161/161 [==============================] - 0s 426us/sample - loss: 0.2753 - accuracy: 0.8820\n",
      "Epoch 58/100\n",
      "161/161 [==============================] - 0s 438us/sample - loss: 0.2749 - accuracy: 0.8944\n",
      "Epoch 59/100\n",
      "161/161 [==============================] - 0s 314us/sample - loss: 0.2960 - accuracy: 0.8634\n",
      "Epoch 60/100\n",
      "161/161 [==============================] - 0s 400us/sample - loss: 0.2878 - accuracy: 0.8882\n",
      "Epoch 61/100\n",
      "161/161 [==============================] - 0s 485us/sample - loss: 0.2670 - accuracy: 0.8758\n",
      "Epoch 62/100\n",
      "161/161 [==============================] - 0s 556us/sample - loss: 0.2684 - accuracy: 0.8696\n",
      "Epoch 63/100\n",
      "161/161 [==============================] - 0s 382us/sample - loss: 0.2544 - accuracy: 0.9006\n",
      "Epoch 64/100\n",
      "161/161 [==============================] - 0s 229us/sample - loss: 0.2605 - accuracy: 0.8944\n",
      "Epoch 65/100\n",
      "161/161 [==============================] - 0s 209us/sample - loss: 0.2779 - accuracy: 0.9006\n",
      "Epoch 66/100\n",
      "161/161 [==============================] - 0s 216us/sample - loss: 0.2665 - accuracy: 0.9006\n",
      "Epoch 67/100\n",
      "161/161 [==============================] - 0s 207us/sample - loss: 0.2543 - accuracy: 0.8944\n",
      "Epoch 68/100\n",
      "161/161 [==============================] - 0s 211us/sample - loss: 0.2711 - accuracy: 0.9068\n",
      "Epoch 69/100\n",
      "161/161 [==============================] - 0s 414us/sample - loss: 0.2505 - accuracy: 0.8944\n",
      "Epoch 70/100\n",
      "161/161 [==============================] - 0s 599us/sample - loss: 0.2757 - accuracy: 0.9006\n",
      "Epoch 71/100\n",
      "161/161 [==============================] - 0s 370us/sample - loss: 0.2406 - accuracy: 0.9006\n",
      "Epoch 72/100\n",
      "161/161 [==============================] - 0s 343us/sample - loss: 0.2629 - accuracy: 0.8882\n",
      "Epoch 73/100\n",
      "161/161 [==============================] - 0s 382us/sample - loss: 0.2686 - accuracy: 0.8820\n",
      "Epoch 74/100\n",
      "161/161 [==============================] - 0s 383us/sample - loss: 0.2809 - accuracy: 0.8944\n",
      "Epoch 75/100\n",
      "161/161 [==============================] - 0s 228us/sample - loss: 0.2565 - accuracy: 0.8882\n",
      "Epoch 76/100\n",
      "161/161 [==============================] - 0s 224us/sample - loss: 0.2857 - accuracy: 0.8882\n",
      "Epoch 77/100\n",
      "161/161 [==============================] - 0s 234us/sample - loss: 0.2439 - accuracy: 0.9006\n",
      "Epoch 78/100\n",
      "161/161 [==============================] - 0s 222us/sample - loss: 0.2454 - accuracy: 0.9193\n",
      "Epoch 79/100\n",
      "161/161 [==============================] - 0s 220us/sample - loss: 0.2427 - accuracy: 0.9068\n",
      "Epoch 80/100\n",
      "161/161 [==============================] - 0s 232us/sample - loss: 0.2666 - accuracy: 0.9006\n",
      "Epoch 81/100\n",
      "161/161 [==============================] - 0s 213us/sample - loss: 0.2571 - accuracy: 0.9006\n",
      "Epoch 82/100\n",
      "161/161 [==============================] - 0s 201us/sample - loss: 0.2531 - accuracy: 0.8882\n",
      "Epoch 83/100\n",
      "161/161 [==============================] - 0s 199us/sample - loss: 0.2397 - accuracy: 0.9193\n",
      "Epoch 84/100\n",
      "161/161 [==============================] - 0s 228us/sample - loss: 0.2566 - accuracy: 0.9006\n",
      "Epoch 85/100\n",
      "161/161 [==============================] - 0s 243us/sample - loss: 0.2436 - accuracy: 0.9006\n",
      "Epoch 86/100\n",
      "161/161 [==============================] - 0s 272us/sample - loss: 0.2378 - accuracy: 0.9130\n",
      "Epoch 87/100\n",
      "161/161 [==============================] - 0s 230us/sample - loss: 0.2651 - accuracy: 0.9006\n",
      "Epoch 88/100\n",
      "161/161 [==============================] - 0s 294us/sample - loss: 0.2540 - accuracy: 0.8944\n",
      "Epoch 89/100\n",
      "161/161 [==============================] - 0s 311us/sample - loss: 0.2335 - accuracy: 0.9006\n",
      "Epoch 90/100\n",
      "161/161 [==============================] - 0s 289us/sample - loss: 0.2408 - accuracy: 0.9068\n",
      "Epoch 91/100\n",
      "161/161 [==============================] - 0s 315us/sample - loss: 0.2369 - accuracy: 0.9006\n",
      "Epoch 92/100\n",
      "161/161 [==============================] - 0s 480us/sample - loss: 0.2441 - accuracy: 0.9068\n",
      "Epoch 93/100\n",
      "161/161 [==============================] - 0s 239us/sample - loss: 0.2238 - accuracy: 0.9255\n",
      "Epoch 94/100\n",
      "161/161 [==============================] - 0s 271us/sample - loss: 0.2291 - accuracy: 0.9130\n",
      "Epoch 95/100\n",
      "161/161 [==============================] - 0s 218us/sample - loss: 0.2380 - accuracy: 0.9068\n",
      "Epoch 96/100\n",
      "161/161 [==============================] - 0s 253us/sample - loss: 0.2289 - accuracy: 0.9317\n",
      "Epoch 97/100\n",
      "161/161 [==============================] - 0s 212us/sample - loss: 0.2330 - accuracy: 0.9379\n",
      "Epoch 98/100\n",
      "161/161 [==============================] - 0s 221us/sample - loss: 0.2247 - accuracy: 0.9255\n",
      "Epoch 99/100\n",
      "161/161 [==============================] - 0s 226us/sample - loss: 0.2399 - accuracy: 0.9130\n",
      "Epoch 100/100\n",
      "161/161 [==============================] - 0s 208us/sample - loss: 0.2342 - accuracy: 0.9193\n",
      "81/81 [==============================] - 0s 1ms/sample - loss: 0.4960 - accuracy: 0.8272\n",
      "Train on 162 samples\n",
      "Epoch 1/100\n",
      "162/162 [==============================] - 1s 4ms/sample - loss: 0.6840 - accuracy: 0.6049\n",
      "Epoch 2/100\n",
      "162/162 [==============================] - 0s 244us/sample - loss: 0.6599 - accuracy: 0.6235\n",
      "Epoch 3/100\n",
      "162/162 [==============================] - 0s 224us/sample - loss: 0.6475 - accuracy: 0.6481\n",
      "Epoch 4/100\n",
      "162/162 [==============================] - 0s 209us/sample - loss: 0.5788 - accuracy: 0.6975\n",
      "Epoch 5/100\n",
      "162/162 [==============================] - 0s 213us/sample - loss: 0.5891 - accuracy: 0.7037\n",
      "Epoch 6/100\n",
      "162/162 [==============================] - 0s 204us/sample - loss: 0.5306 - accuracy: 0.7593\n",
      "Epoch 7/100\n",
      "162/162 [==============================] - 0s 205us/sample - loss: 0.5239 - accuracy: 0.7778\n",
      "Epoch 8/100\n",
      "162/162 [==============================] - 0s 205us/sample - loss: 0.5290 - accuracy: 0.7469\n",
      "Epoch 9/100\n",
      "162/162 [==============================] - 0s 209us/sample - loss: 0.5016 - accuracy: 0.7469\n",
      "Epoch 10/100\n",
      "162/162 [==============================] - 0s 200us/sample - loss: 0.5053 - accuracy: 0.7654\n",
      "Epoch 11/100\n",
      "162/162 [==============================] - 0s 249us/sample - loss: 0.4800 - accuracy: 0.7284\n",
      "Epoch 12/100\n",
      "162/162 [==============================] - 0s 224us/sample - loss: 0.4442 - accuracy: 0.8395\n",
      "Epoch 13/100\n",
      "162/162 [==============================] - 0s 249us/sample - loss: 0.4545 - accuracy: 0.8025\n",
      "Epoch 14/100\n",
      "162/162 [==============================] - 0s 289us/sample - loss: 0.4233 - accuracy: 0.8086\n",
      "Epoch 15/100\n",
      "162/162 [==============================] - 0s 345us/sample - loss: 0.4396 - accuracy: 0.8148\n",
      "Epoch 16/100\n",
      "162/162 [==============================] - 0s 322us/sample - loss: 0.4000 - accuracy: 0.8025\n",
      "Epoch 17/100\n",
      "162/162 [==============================] - 0s 350us/sample - loss: 0.4055 - accuracy: 0.8395\n",
      "Epoch 18/100\n",
      "162/162 [==============================] - 0s 272us/sample - loss: 0.3815 - accuracy: 0.8642\n",
      "Epoch 19/100\n",
      "162/162 [==============================] - 0s 243us/sample - loss: 0.3915 - accuracy: 0.8333\n",
      "Epoch 20/100\n",
      "162/162 [==============================] - 0s 257us/sample - loss: 0.3793 - accuracy: 0.8333\n",
      "Epoch 21/100\n",
      "162/162 [==============================] - 0s 227us/sample - loss: 0.3654 - accuracy: 0.8519\n",
      "Epoch 22/100\n",
      "162/162 [==============================] - 0s 245us/sample - loss: 0.3754 - accuracy: 0.8457\n",
      "Epoch 23/100\n",
      "162/162 [==============================] - 0s 201us/sample - loss: 0.3777 - accuracy: 0.8519\n",
      "Epoch 24/100\n",
      "162/162 [==============================] - 0s 202us/sample - loss: 0.3740 - accuracy: 0.8395\n",
      "Epoch 25/100\n",
      "162/162 [==============================] - 0s 223us/sample - loss: 0.3703 - accuracy: 0.8272\n",
      "Epoch 26/100\n",
      "162/162 [==============================] - 0s 205us/sample - loss: 0.3763 - accuracy: 0.8148\n",
      "Epoch 27/100\n",
      "162/162 [==============================] - 0s 196us/sample - loss: 0.3690 - accuracy: 0.8457\n",
      "Epoch 28/100\n",
      "162/162 [==============================] - 0s 193us/sample - loss: 0.3452 - accuracy: 0.8272\n",
      "Epoch 29/100\n",
      "162/162 [==============================] - 0s 201us/sample - loss: 0.3847 - accuracy: 0.8457\n",
      "Epoch 30/100\n",
      "162/162 [==============================] - 0s 193us/sample - loss: 0.3472 - accuracy: 0.8519\n",
      "Epoch 31/100\n",
      "162/162 [==============================] - 0s 204us/sample - loss: 0.3460 - accuracy: 0.8827\n",
      "Epoch 32/100\n",
      "162/162 [==============================] - 0s 219us/sample - loss: 0.3462 - accuracy: 0.8333\n",
      "Epoch 33/100\n",
      "162/162 [==============================] - 0s 445us/sample - loss: 0.3398 - accuracy: 0.8642\n",
      "Epoch 34/100\n",
      "162/162 [==============================] - 0s 407us/sample - loss: 0.3536 - accuracy: 0.8580\n",
      "Epoch 35/100\n",
      "162/162 [==============================] - 0s 327us/sample - loss: 0.3062 - accuracy: 0.8765\n",
      "Epoch 36/100\n",
      "162/162 [==============================] - 0s 396us/sample - loss: 0.3290 - accuracy: 0.8765\n",
      "Epoch 37/100\n",
      "162/162 [==============================] - 0s 399us/sample - loss: 0.3547 - accuracy: 0.8210\n",
      "Epoch 38/100\n",
      "162/162 [==============================] - 0s 308us/sample - loss: 0.3366 - accuracy: 0.8519\n",
      "Epoch 39/100\n",
      "162/162 [==============================] - 0s 211us/sample - loss: 0.3399 - accuracy: 0.8272\n",
      "Epoch 40/100\n",
      "162/162 [==============================] - 0s 239us/sample - loss: 0.3169 - accuracy: 0.8704\n",
      "Epoch 41/100\n",
      "162/162 [==============================] - 0s 253us/sample - loss: 0.3253 - accuracy: 0.8519\n",
      "Epoch 42/100\n",
      "162/162 [==============================] - 0s 220us/sample - loss: 0.2992 - accuracy: 0.8765\n",
      "Epoch 43/100\n",
      "162/162 [==============================] - 0s 222us/sample - loss: 0.3447 - accuracy: 0.8580\n",
      "Epoch 44/100\n",
      "162/162 [==============================] - 0s 203us/sample - loss: 0.3062 - accuracy: 0.8642\n",
      "Epoch 45/100\n",
      "162/162 [==============================] - 0s 243us/sample - loss: 0.3245 - accuracy: 0.8827\n",
      "Epoch 46/100\n",
      "162/162 [==============================] - 0s 204us/sample - loss: 0.3201 - accuracy: 0.8519\n",
      "Epoch 47/100\n",
      "162/162 [==============================] - 0s 300us/sample - loss: 0.3240 - accuracy: 0.8827\n",
      "Epoch 48/100\n",
      "162/162 [==============================] - 0s 772us/sample - loss: 0.3321 - accuracy: 0.8642\n",
      "Epoch 49/100\n",
      "162/162 [==============================] - 0s 400us/sample - loss: 0.3224 - accuracy: 0.8642\n",
      "Epoch 50/100\n",
      "162/162 [==============================] - 0s 641us/sample - loss: 0.3205 - accuracy: 0.8519\n",
      "Epoch 51/100\n",
      "162/162 [==============================] - 0s 550us/sample - loss: 0.3102 - accuracy: 0.8642\n",
      "Epoch 52/100\n",
      "162/162 [==============================] - 0s 714us/sample - loss: 0.3345 - accuracy: 0.8580\n",
      "Epoch 53/100\n",
      "162/162 [==============================] - 0s 428us/sample - loss: 0.2687 - accuracy: 0.8889\n",
      "Epoch 54/100\n",
      "162/162 [==============================] - 0s 292us/sample - loss: 0.3213 - accuracy: 0.8519\n",
      "Epoch 55/100\n",
      "162/162 [==============================] - 0s 276us/sample - loss: 0.3294 - accuracy: 0.8457\n",
      "Epoch 56/100\n",
      "162/162 [==============================] - 0s 206us/sample - loss: 0.3077 - accuracy: 0.8704\n",
      "Epoch 57/100\n",
      "162/162 [==============================] - 0s 206us/sample - loss: 0.2812 - accuracy: 0.8827\n",
      "Epoch 58/100\n",
      "162/162 [==============================] - 0s 204us/sample - loss: 0.3119 - accuracy: 0.8519\n",
      "Epoch 59/100\n",
      "162/162 [==============================] - 0s 212us/sample - loss: 0.2795 - accuracy: 0.8642\n",
      "Epoch 60/100\n",
      "162/162 [==============================] - 0s 213us/sample - loss: 0.3005 - accuracy: 0.8580\n",
      "Epoch 61/100\n",
      "162/162 [==============================] - 0s 216us/sample - loss: 0.2837 - accuracy: 0.8704\n",
      "Epoch 62/100\n",
      "162/162 [==============================] - 0s 248us/sample - loss: 0.2971 - accuracy: 0.8642\n",
      "Epoch 63/100\n",
      "162/162 [==============================] - 0s 539us/sample - loss: 0.2672 - accuracy: 0.8765\n",
      "Epoch 64/100\n",
      "162/162 [==============================] - 0s 638us/sample - loss: 0.2912 - accuracy: 0.8827\n",
      "Epoch 65/100\n",
      "162/162 [==============================] - 0s 557us/sample - loss: 0.2962 - accuracy: 0.8765\n",
      "Epoch 66/100\n",
      "162/162 [==============================] - 0s 435us/sample - loss: 0.3096 - accuracy: 0.8827\n",
      "Epoch 67/100\n",
      "162/162 [==============================] - 0s 240us/sample - loss: 0.2881 - accuracy: 0.8642\n",
      "Epoch 68/100\n",
      "162/162 [==============================] - 0s 222us/sample - loss: 0.3009 - accuracy: 0.8951\n",
      "Epoch 69/100\n",
      "162/162 [==============================] - 0s 219us/sample - loss: 0.2900 - accuracy: 0.8704\n",
      "Epoch 70/100\n",
      "162/162 [==============================] - 0s 213us/sample - loss: 0.3207 - accuracy: 0.8642\n",
      "Epoch 71/100\n",
      "162/162 [==============================] - 0s 202us/sample - loss: 0.2906 - accuracy: 0.8457\n",
      "Epoch 72/100\n",
      "162/162 [==============================] - 0s 211us/sample - loss: 0.2877 - accuracy: 0.8765\n",
      "Epoch 73/100\n",
      "162/162 [==============================] - 0s 204us/sample - loss: 0.3085 - accuracy: 0.8642\n",
      "Epoch 74/100\n",
      "162/162 [==============================] - 0s 410us/sample - loss: 0.2883 - accuracy: 0.8827\n",
      "Epoch 75/100\n",
      "162/162 [==============================] - 0s 739us/sample - loss: 0.2938 - accuracy: 0.8951\n",
      "Epoch 76/100\n",
      "162/162 [==============================] - 0s 460us/sample - loss: 0.2918 - accuracy: 0.8765\n",
      "Epoch 77/100\n",
      "162/162 [==============================] - 0s 306us/sample - loss: 0.2682 - accuracy: 0.8889\n",
      "Epoch 78/100\n",
      "162/162 [==============================] - 0s 246us/sample - loss: 0.2824 - accuracy: 0.8827\n",
      "Epoch 79/100\n",
      "162/162 [==============================] - 0s 200us/sample - loss: 0.2962 - accuracy: 0.8765\n",
      "Epoch 80/100\n",
      "162/162 [==============================] - 0s 290us/sample - loss: 0.2711 - accuracy: 0.8951\n",
      "Epoch 81/100\n",
      "162/162 [==============================] - 0s 259us/sample - loss: 0.2793 - accuracy: 0.8827\n",
      "Epoch 82/100\n",
      "162/162 [==============================] - 0s 505us/sample - loss: 0.2824 - accuracy: 0.8827\n",
      "Epoch 83/100\n",
      "162/162 [==============================] - 0s 840us/sample - loss: 0.2719 - accuracy: 0.9012\n",
      "Epoch 84/100\n",
      "162/162 [==============================] - 0s 477us/sample - loss: 0.2754 - accuracy: 0.8951\n",
      "Epoch 85/100\n",
      "162/162 [==============================] - 0s 205us/sample - loss: 0.2495 - accuracy: 0.8951\n",
      "Epoch 86/100\n",
      "162/162 [==============================] - 0s 244us/sample - loss: 0.2836 - accuracy: 0.8889\n",
      "Epoch 87/100\n",
      "162/162 [==============================] - 0s 213us/sample - loss: 0.2886 - accuracy: 0.8642\n",
      "Epoch 88/100\n",
      "162/162 [==============================] - 0s 342us/sample - loss: 0.2388 - accuracy: 0.9136\n",
      "Epoch 89/100\n",
      "162/162 [==============================] - 0s 649us/sample - loss: 0.2779 - accuracy: 0.9012\n",
      "Epoch 90/100\n",
      "162/162 [==============================] - 0s 220us/sample - loss: 0.2766 - accuracy: 0.8827\n",
      "Epoch 91/100\n",
      "162/162 [==============================] - 0s 368us/sample - loss: 0.2919 - accuracy: 0.8704\n",
      "Epoch 92/100\n",
      "162/162 [==============================] - 0s 575us/sample - loss: 0.2555 - accuracy: 0.8951\n",
      "Epoch 93/100\n",
      "162/162 [==============================] - 0s 333us/sample - loss: 0.2846 - accuracy: 0.8580\n",
      "Epoch 94/100\n",
      "162/162 [==============================] - 0s 292us/sample - loss: 0.2482 - accuracy: 0.9136\n",
      "Epoch 95/100\n",
      "162/162 [==============================] - 0s 253us/sample - loss: 0.2562 - accuracy: 0.8827\n",
      "Epoch 96/100\n",
      "162/162 [==============================] - 0s 249us/sample - loss: 0.2555 - accuracy: 0.8951\n",
      "Epoch 97/100\n",
      "162/162 [==============================] - 0s 641us/sample - loss: 0.2608 - accuracy: 0.8889\n",
      "Epoch 98/100\n",
      "162/162 [==============================] - 0s 685us/sample - loss: 0.2658 - accuracy: 0.8827\n",
      "Epoch 99/100\n",
      "162/162 [==============================] - 0s 709us/sample - loss: 0.2418 - accuracy: 0.9074\n",
      "Epoch 100/100\n",
      "162/162 [==============================] - 0s 396us/sample - loss: 0.2511 - accuracy: 0.9012\n",
      "80/80 [==============================] - 0s 1ms/sample - loss: 0.4210 - accuracy: 0.8250\n",
      "Train on 161 samples\n",
      "Epoch 1/100\n",
      "161/161 [==============================] - 1s 4ms/sample - loss: 0.7566 - accuracy: 0.5528\n",
      "Epoch 2/100\n",
      "161/161 [==============================] - 0s 183us/sample - loss: 0.7542 - accuracy: 0.5714\n",
      "Epoch 3/100\n",
      "161/161 [==============================] - 0s 267us/sample - loss: 0.7078 - accuracy: 0.5901\n",
      "Epoch 4/100\n",
      "161/161 [==============================] - 0s 252us/sample - loss: 0.7085 - accuracy: 0.5901\n",
      "Epoch 5/100\n",
      "161/161 [==============================] - 0s 172us/sample - loss: 0.6784 - accuracy: 0.6025\n",
      "Epoch 6/100\n",
      "161/161 [==============================] - 0s 172us/sample - loss: 0.6943 - accuracy: 0.6522\n",
      "Epoch 7/100\n",
      "161/161 [==============================] - 0s 185us/sample - loss: 0.6098 - accuracy: 0.6894\n",
      "Epoch 8/100\n",
      "161/161 [==============================] - 0s 204us/sample - loss: 0.6621 - accuracy: 0.6460\n",
      "Epoch 9/100\n",
      "161/161 [==============================] - 0s 246us/sample - loss: 0.6500 - accuracy: 0.6646\n",
      "Epoch 10/100\n",
      "161/161 [==============================] - 0s 120us/sample - loss: 0.6111 - accuracy: 0.6584\n",
      "Epoch 11/100\n",
      "161/161 [==============================] - ETA: 0s - loss: 0.6367 - accuracy: 0.70 - 0s 144us/sample - loss: 0.6106 - accuracy: 0.7019\n",
      "Epoch 12/100\n",
      "161/161 [==============================] - 0s 155us/sample - loss: 0.6357 - accuracy: 0.6832\n",
      "Epoch 13/100\n",
      "161/161 [==============================] - 0s 105us/sample - loss: 0.5810 - accuracy: 0.6832\n",
      "Epoch 14/100\n",
      "161/161 [==============================] - 0s 109us/sample - loss: 0.5820 - accuracy: 0.7267\n",
      "Epoch 15/100\n",
      "161/161 [==============================] - 0s 69us/sample - loss: 0.5546 - accuracy: 0.7329\n",
      "Epoch 16/100\n",
      "161/161 [==============================] - 0s 73us/sample - loss: 0.5669 - accuracy: 0.7329\n",
      "Epoch 17/100\n",
      "161/161 [==============================] - 0s 74us/sample - loss: 0.5555 - accuracy: 0.7578\n",
      "Epoch 18/100\n",
      "161/161 [==============================] - 0s 79us/sample - loss: 0.5420 - accuracy: 0.7453\n",
      "Epoch 19/100\n",
      "161/161 [==============================] - 0s 77us/sample - loss: 0.5371 - accuracy: 0.7453\n",
      "Epoch 20/100\n",
      "161/161 [==============================] - 0s 71us/sample - loss: 0.5416 - accuracy: 0.7702\n",
      "Epoch 21/100\n",
      "161/161 [==============================] - 0s 76us/sample - loss: 0.5137 - accuracy: 0.7826\n",
      "Epoch 22/100\n",
      "161/161 [==============================] - 0s 67us/sample - loss: 0.5175 - accuracy: 0.7640\n",
      "Epoch 23/100\n",
      "161/161 [==============================] - 0s 91us/sample - loss: 0.5212 - accuracy: 0.7578\n",
      "Epoch 24/100\n",
      "161/161 [==============================] - 0s 75us/sample - loss: 0.4800 - accuracy: 0.7826\n",
      "Epoch 25/100\n",
      "161/161 [==============================] - 0s 83us/sample - loss: 0.5283 - accuracy: 0.7826\n",
      "Epoch 26/100\n",
      "161/161 [==============================] - 0s 85us/sample - loss: 0.4901 - accuracy: 0.7826\n",
      "Epoch 27/100\n",
      "161/161 [==============================] - 0s 83us/sample - loss: 0.4797 - accuracy: 0.7826\n",
      "Epoch 28/100\n",
      "161/161 [==============================] - 0s 90us/sample - loss: 0.4937 - accuracy: 0.7826\n",
      "Epoch 29/100\n",
      "161/161 [==============================] - 0s 106us/sample - loss: 0.4531 - accuracy: 0.8447\n",
      "Epoch 30/100\n",
      "161/161 [==============================] - 0s 148us/sample - loss: 0.4715 - accuracy: 0.8012\n",
      "Epoch 31/100\n",
      "161/161 [==============================] - 0s 105us/sample - loss: 0.4722 - accuracy: 0.8137\n",
      "Epoch 32/100\n",
      "161/161 [==============================] - 0s 75us/sample - loss: 0.4567 - accuracy: 0.8075\n",
      "Epoch 33/100\n",
      "161/161 [==============================] - 0s 67us/sample - loss: 0.4589 - accuracy: 0.8199\n",
      "Epoch 34/100\n",
      "161/161 [==============================] - 0s 74us/sample - loss: 0.4835 - accuracy: 0.7826\n",
      "Epoch 35/100\n",
      "161/161 [==============================] - 0s 70us/sample - loss: 0.4326 - accuracy: 0.8137\n",
      "Epoch 36/100\n",
      "161/161 [==============================] - 0s 87us/sample - loss: 0.4453 - accuracy: 0.8012\n",
      "Epoch 37/100\n",
      "161/161 [==============================] - 0s 74us/sample - loss: 0.4491 - accuracy: 0.7950\n",
      "Epoch 38/100\n",
      "161/161 [==============================] - 0s 72us/sample - loss: 0.4537 - accuracy: 0.7950\n",
      "Epoch 39/100\n",
      "161/161 [==============================] - 0s 79us/sample - loss: 0.4388 - accuracy: 0.8075\n",
      "Epoch 40/100\n",
      "161/161 [==============================] - 0s 82us/sample - loss: 0.4466 - accuracy: 0.8199\n",
      "Epoch 41/100\n",
      "161/161 [==============================] - 0s 133us/sample - loss: 0.4409 - accuracy: 0.8075\n",
      "Epoch 42/100\n",
      "161/161 [==============================] - 0s 81us/sample - loss: 0.4568 - accuracy: 0.8137\n",
      "Epoch 43/100\n",
      "161/161 [==============================] - 0s 68us/sample - loss: 0.4468 - accuracy: 0.8261\n",
      "Epoch 44/100\n",
      "161/161 [==============================] - 0s 76us/sample - loss: 0.4241 - accuracy: 0.8261\n",
      "Epoch 45/100\n",
      "161/161 [==============================] - 0s 102us/sample - loss: 0.4404 - accuracy: 0.8199\n",
      "Epoch 46/100\n",
      "161/161 [==============================] - 0s 113us/sample - loss: 0.4251 - accuracy: 0.8199\n",
      "Epoch 47/100\n",
      "161/161 [==============================] - 0s 126us/sample - loss: 0.4016 - accuracy: 0.8199\n",
      "Epoch 48/100\n",
      "161/161 [==============================] - 0s 127us/sample - loss: 0.4198 - accuracy: 0.8323\n",
      "Epoch 49/100\n",
      "161/161 [==============================] - 0s 135us/sample - loss: 0.4028 - accuracy: 0.8323\n",
      "Epoch 50/100\n",
      "161/161 [==============================] - 0s 159us/sample - loss: 0.4202 - accuracy: 0.8075\n",
      "Epoch 51/100\n",
      "161/161 [==============================] - 0s 157us/sample - loss: 0.3862 - accuracy: 0.8261\n",
      "Epoch 52/100\n",
      "161/161 [==============================] - 0s 109us/sample - loss: 0.3971 - accuracy: 0.8509\n",
      "Epoch 53/100\n",
      "161/161 [==============================] - 0s 168us/sample - loss: 0.4062 - accuracy: 0.8447\n",
      "Epoch 54/100\n",
      "161/161 [==============================] - 0s 182us/sample - loss: 0.3936 - accuracy: 0.8137\n",
      "Epoch 55/100\n",
      "161/161 [==============================] - 0s 295us/sample - loss: 0.4053 - accuracy: 0.8075\n",
      "Epoch 56/100\n",
      "161/161 [==============================] - 0s 201us/sample - loss: 0.4166 - accuracy: 0.8075\n",
      "Epoch 57/100\n",
      "161/161 [==============================] - 0s 189us/sample - loss: 0.3956 - accuracy: 0.8137\n",
      "Epoch 58/100\n",
      "161/161 [==============================] - ETA: 0s - loss: 0.3360 - accuracy: 0.86 - 0s 154us/sample - loss: 0.3943 - accuracy: 0.8571\n",
      "Epoch 59/100\n",
      "161/161 [==============================] - 0s 121us/sample - loss: 0.3838 - accuracy: 0.8261\n",
      "Epoch 60/100\n",
      "161/161 [==============================] - 0s 138us/sample - loss: 0.4096 - accuracy: 0.8075\n",
      "Epoch 61/100\n",
      "161/161 [==============================] - 0s 193us/sample - loss: 0.3818 - accuracy: 0.8385\n",
      "Epoch 62/100\n",
      "161/161 [==============================] - 0s 111us/sample - loss: 0.4104 - accuracy: 0.8261\n",
      "Epoch 63/100\n",
      "161/161 [==============================] - 0s 99us/sample - loss: 0.3734 - accuracy: 0.8696\n",
      "Epoch 64/100\n",
      "161/161 [==============================] - 0s 75us/sample - loss: 0.3806 - accuracy: 0.8261\n",
      "Epoch 65/100\n",
      "161/161 [==============================] - 0s 116us/sample - loss: 0.3832 - accuracy: 0.8385\n",
      "Epoch 66/100\n",
      "161/161 [==============================] - 0s 135us/sample - loss: 0.3756 - accuracy: 0.8323\n",
      "Epoch 67/100\n",
      "161/161 [==============================] - 0s 76us/sample - loss: 0.3944 - accuracy: 0.8509\n",
      "Epoch 68/100\n",
      "161/161 [==============================] - 0s 86us/sample - loss: 0.3785 - accuracy: 0.8385\n",
      "Epoch 69/100\n",
      "161/161 [==============================] - 0s 88us/sample - loss: 0.3691 - accuracy: 0.8323\n",
      "Epoch 70/100\n",
      "161/161 [==============================] - 0s 109us/sample - loss: 0.3775 - accuracy: 0.8261\n",
      "Epoch 71/100\n",
      "161/161 [==============================] - 0s 71us/sample - loss: 0.3853 - accuracy: 0.8137\n",
      "Epoch 72/100\n",
      "161/161 [==============================] - 0s 108us/sample - loss: 0.3701 - accuracy: 0.8261\n",
      "Epoch 73/100\n",
      "161/161 [==============================] - 0s 157us/sample - loss: 0.3510 - accuracy: 0.8447\n",
      "Epoch 74/100\n",
      "161/161 [==============================] - 0s 100us/sample - loss: 0.3680 - accuracy: 0.8323\n",
      "Epoch 75/100\n",
      "161/161 [==============================] - 0s 128us/sample - loss: 0.3969 - accuracy: 0.8137\n",
      "Epoch 76/100\n",
      "161/161 [==============================] - 0s 109us/sample - loss: 0.3748 - accuracy: 0.8261\n",
      "Epoch 77/100\n",
      "161/161 [==============================] - 0s 117us/sample - loss: 0.3422 - accuracy: 0.8571\n",
      "Epoch 78/100\n",
      "161/161 [==============================] - 0s 93us/sample - loss: 0.3862 - accuracy: 0.8261\n",
      "Epoch 79/100\n",
      "161/161 [==============================] - 0s 124us/sample - loss: 0.3445 - accuracy: 0.8447\n",
      "Epoch 80/100\n",
      "161/161 [==============================] - 0s 148us/sample - loss: 0.3356 - accuracy: 0.8571\n",
      "Epoch 81/100\n",
      "161/161 [==============================] - 0s 106us/sample - loss: 0.3599 - accuracy: 0.8571\n",
      "Epoch 82/100\n",
      "161/161 [==============================] - 0s 112us/sample - loss: 0.3347 - accuracy: 0.8758\n",
      "Epoch 83/100\n",
      "161/161 [==============================] - 0s 143us/sample - loss: 0.3561 - accuracy: 0.8323\n",
      "Epoch 84/100\n",
      "161/161 [==============================] - 0s 136us/sample - loss: 0.3928 - accuracy: 0.7950\n",
      "Epoch 85/100\n",
      "161/161 [==============================] - 0s 128us/sample - loss: 0.3608 - accuracy: 0.8509\n",
      "Epoch 86/100\n",
      "161/161 [==============================] - 0s 88us/sample - loss: 0.3596 - accuracy: 0.8385\n",
      "Epoch 87/100\n",
      "161/161 [==============================] - 0s 82us/sample - loss: 0.3480 - accuracy: 0.8323\n",
      "Epoch 88/100\n",
      "161/161 [==============================] - 0s 93us/sample - loss: 0.3422 - accuracy: 0.8634\n",
      "Epoch 89/100\n",
      "161/161 [==============================] - 0s 143us/sample - loss: 0.3625 - accuracy: 0.8261\n",
      "Epoch 90/100\n",
      "161/161 [==============================] - 0s 189us/sample - loss: 0.3356 - accuracy: 0.8634\n",
      "Epoch 91/100\n",
      "161/161 [==============================] - 0s 168us/sample - loss: 0.3457 - accuracy: 0.8385\n",
      "Epoch 92/100\n",
      "161/161 [==============================] - 0s 118us/sample - loss: 0.3496 - accuracy: 0.8509\n",
      "Epoch 93/100\n",
      "161/161 [==============================] - 0s 198us/sample - loss: 0.3440 - accuracy: 0.8634\n",
      "Epoch 94/100\n",
      "161/161 [==============================] - 0s 139us/sample - loss: 0.3573 - accuracy: 0.8385\n",
      "Epoch 95/100\n",
      "161/161 [==============================] - 0s 168us/sample - loss: 0.3336 - accuracy: 0.8509\n",
      "Epoch 96/100\n",
      "161/161 [==============================] - 0s 296us/sample - loss: 0.3264 - accuracy: 0.8323\n",
      "Epoch 97/100\n",
      "161/161 [==============================] - 0s 154us/sample - loss: 0.3305 - accuracy: 0.8571\n",
      "Epoch 98/100\n",
      "161/161 [==============================] - 0s 148us/sample - loss: 0.3467 - accuracy: 0.8385\n",
      "Epoch 99/100\n",
      "161/161 [==============================] - 0s 187us/sample - loss: 0.3713 - accuracy: 0.8012\n",
      "Epoch 100/100\n",
      "161/161 [==============================] - 0s 101us/sample - loss: 0.3209 - accuracy: 0.8634\n",
      "81/81 [==============================] - 0s 2ms/sample - loss: 0.3885 - accuracy: 0.8642\n",
      "Train on 161 samples\n",
      "Epoch 1/100\n",
      "161/161 [==============================] - 1s 5ms/sample - loss: 0.7230 - accuracy: 0.5963\n",
      "Epoch 2/100\n",
      "161/161 [==============================] - 0s 157us/sample - loss: 0.7002 - accuracy: 0.6273\n",
      "Epoch 3/100\n",
      "161/161 [==============================] - 0s 249us/sample - loss: 0.6939 - accuracy: 0.6025\n",
      "Epoch 4/100\n",
      "161/161 [==============================] - 0s 228us/sample - loss: 0.6927 - accuracy: 0.6460\n",
      "Epoch 5/100\n",
      "161/161 [==============================] - 0s 217us/sample - loss: 0.6515 - accuracy: 0.6211\n",
      "Epoch 6/100\n",
      "161/161 [==============================] - 0s 290us/sample - loss: 0.6422 - accuracy: 0.6398\n",
      "Epoch 7/100\n",
      "161/161 [==============================] - 0s 131us/sample - loss: 0.6590 - accuracy: 0.6335\n",
      "Epoch 8/100\n",
      "161/161 [==============================] - 0s 177us/sample - loss: 0.6354 - accuracy: 0.6584\n",
      "Epoch 9/100\n",
      "161/161 [==============================] - 0s 194us/sample - loss: 0.6098 - accuracy: 0.7205\n",
      "Epoch 10/100\n",
      "161/161 [==============================] - 0s 167us/sample - loss: 0.6282 - accuracy: 0.6522\n",
      "Epoch 11/100\n",
      "161/161 [==============================] - 0s 182us/sample - loss: 0.5848 - accuracy: 0.7143\n",
      "Epoch 12/100\n",
      "161/161 [==============================] - 0s 195us/sample - loss: 0.5908 - accuracy: 0.6770\n",
      "Epoch 13/100\n",
      "161/161 [==============================] - 0s 98us/sample - loss: 0.5786 - accuracy: 0.6832\n",
      "Epoch 14/100\n",
      "161/161 [==============================] - 0s 105us/sample - loss: 0.5950 - accuracy: 0.6957\n",
      "Epoch 15/100\n",
      "161/161 [==============================] - 0s 106us/sample - loss: 0.6044 - accuracy: 0.6894\n",
      "Epoch 16/100\n",
      "161/161 [==============================] - 0s 124us/sample - loss: 0.5377 - accuracy: 0.7453\n",
      "Epoch 17/100\n",
      "161/161 [==============================] - 0s 103us/sample - loss: 0.5463 - accuracy: 0.7329\n",
      "Epoch 18/100\n",
      "161/161 [==============================] - 0s 136us/sample - loss: 0.5439 - accuracy: 0.7143\n",
      "Epoch 19/100\n",
      "161/161 [==============================] - 0s 187us/sample - loss: 0.5641 - accuracy: 0.7453\n",
      "Epoch 20/100\n",
      "161/161 [==============================] - 0s 121us/sample - loss: 0.5108 - accuracy: 0.7391\n",
      "Epoch 21/100\n",
      "161/161 [==============================] - 0s 135us/sample - loss: 0.5260 - accuracy: 0.7453\n",
      "Epoch 22/100\n",
      "161/161 [==============================] - 0s 100us/sample - loss: 0.5284 - accuracy: 0.7329\n",
      "Epoch 23/100\n",
      "161/161 [==============================] - 0s 121us/sample - loss: 0.5409 - accuracy: 0.7205\n",
      "Epoch 24/100\n",
      "161/161 [==============================] - 0s 67us/sample - loss: 0.5022 - accuracy: 0.7453\n",
      "Epoch 25/100\n",
      "161/161 [==============================] - 0s 99us/sample - loss: 0.4877 - accuracy: 0.7516\n",
      "Epoch 26/100\n",
      "161/161 [==============================] - 0s 143us/sample - loss: 0.4923 - accuracy: 0.7205\n",
      "Epoch 27/100\n",
      "161/161 [==============================] - 0s 84us/sample - loss: 0.4895 - accuracy: 0.7516\n",
      "Epoch 28/100\n",
      "161/161 [==============================] - 0s 72us/sample - loss: 0.4840 - accuracy: 0.7826\n",
      "Epoch 29/100\n",
      "161/161 [==============================] - 0s 68us/sample - loss: 0.4687 - accuracy: 0.7702\n",
      "Epoch 30/100\n",
      "161/161 [==============================] - 0s 79us/sample - loss: 0.4779 - accuracy: 0.8075\n",
      "Epoch 31/100\n",
      "161/161 [==============================] - 0s 116us/sample - loss: 0.4622 - accuracy: 0.7764\n",
      "Epoch 32/100\n",
      "161/161 [==============================] - 0s 290us/sample - loss: 0.4849 - accuracy: 0.7516\n",
      "Epoch 33/100\n",
      "161/161 [==============================] - 0s 108us/sample - loss: 0.4739 - accuracy: 0.7640\n",
      "Epoch 34/100\n",
      "161/161 [==============================] - 0s 111us/sample - loss: 0.4766 - accuracy: 0.7578\n",
      "Epoch 35/100\n",
      "161/161 [==============================] - 0s 84us/sample - loss: 0.4765 - accuracy: 0.7764\n",
      "Epoch 36/100\n",
      "161/161 [==============================] - 0s 183us/sample - loss: 0.4489 - accuracy: 0.7950\n",
      "Epoch 37/100\n",
      "161/161 [==============================] - 0s 133us/sample - loss: 0.4274 - accuracy: 0.8075\n",
      "Epoch 38/100\n",
      "161/161 [==============================] - 0s 119us/sample - loss: 0.4586 - accuracy: 0.7702\n",
      "Epoch 39/100\n",
      "161/161 [==============================] - 0s 120us/sample - loss: 0.4715 - accuracy: 0.7578\n",
      "Epoch 40/100\n",
      "161/161 [==============================] - 0s 106us/sample - loss: 0.4715 - accuracy: 0.7640\n",
      "Epoch 41/100\n",
      "161/161 [==============================] - 0s 105us/sample - loss: 0.4505 - accuracy: 0.7702\n",
      "Epoch 42/100\n",
      "161/161 [==============================] - 0s 135us/sample - loss: 0.4417 - accuracy: 0.7950\n",
      "Epoch 43/100\n",
      "161/161 [==============================] - 0s 105us/sample - loss: 0.4661 - accuracy: 0.7888\n",
      "Epoch 44/100\n",
      "161/161 [==============================] - 0s 87us/sample - loss: 0.4376 - accuracy: 0.7888\n",
      "Epoch 45/100\n",
      "161/161 [==============================] - 0s 109us/sample - loss: 0.4276 - accuracy: 0.7950\n",
      "Epoch 46/100\n",
      "161/161 [==============================] - 0s 94us/sample - loss: 0.4258 - accuracy: 0.8199\n",
      "Epoch 47/100\n",
      "161/161 [==============================] - 0s 100us/sample - loss: 0.4400 - accuracy: 0.8012\n",
      "Epoch 48/100\n",
      "161/161 [==============================] - 0s 155us/sample - loss: 0.4596 - accuracy: 0.7391\n",
      "Epoch 49/100\n",
      "161/161 [==============================] - 0s 97us/sample - loss: 0.4204 - accuracy: 0.7950\n",
      "Epoch 50/100\n",
      "161/161 [==============================] - 0s 110us/sample - loss: 0.4262 - accuracy: 0.8012\n",
      "Epoch 51/100\n",
      "161/161 [==============================] - 0s 209us/sample - loss: 0.4197 - accuracy: 0.8075\n",
      "Epoch 52/100\n",
      "161/161 [==============================] - 0s 235us/sample - loss: 0.4180 - accuracy: 0.8012\n",
      "Epoch 53/100\n",
      "161/161 [==============================] - 0s 203us/sample - loss: 0.4393 - accuracy: 0.7702\n",
      "Epoch 54/100\n",
      "161/161 [==============================] - ETA: 0s - loss: 0.3528 - accuracy: 0.86 - 0s 136us/sample - loss: 0.3915 - accuracy: 0.8385\n",
      "Epoch 55/100\n",
      "161/161 [==============================] - 0s 173us/sample - loss: 0.3992 - accuracy: 0.7950\n",
      "Epoch 56/100\n",
      "161/161 [==============================] - 0s 140us/sample - loss: 0.3921 - accuracy: 0.8447\n",
      "Epoch 57/100\n",
      "161/161 [==============================] - 0s 150us/sample - loss: 0.3965 - accuracy: 0.8012\n",
      "Epoch 58/100\n",
      "161/161 [==============================] - 0s 170us/sample - loss: 0.4172 - accuracy: 0.8075\n",
      "Epoch 59/100\n",
      "161/161 [==============================] - 0s 186us/sample - loss: 0.4085 - accuracy: 0.8137\n",
      "Epoch 60/100\n",
      "161/161 [==============================] - 0s 121us/sample - loss: 0.3784 - accuracy: 0.8385\n",
      "Epoch 61/100\n",
      "161/161 [==============================] - 0s 125us/sample - loss: 0.3989 - accuracy: 0.8137\n",
      "Epoch 62/100\n",
      "161/161 [==============================] - 0s 183us/sample - loss: 0.3985 - accuracy: 0.7950\n",
      "Epoch 63/100\n",
      "161/161 [==============================] - 0s 161us/sample - loss: 0.3812 - accuracy: 0.8447\n",
      "Epoch 64/100\n",
      "161/161 [==============================] - 0s 157us/sample - loss: 0.4139 - accuracy: 0.8075\n",
      "Epoch 65/100\n",
      "161/161 [==============================] - 0s 148us/sample - loss: 0.3833 - accuracy: 0.8509\n",
      "Epoch 66/100\n",
      "161/161 [==============================] - 0s 200us/sample - loss: 0.3697 - accuracy: 0.8696\n",
      "Epoch 67/100\n",
      "161/161 [==============================] - 0s 268us/sample - loss: 0.3823 - accuracy: 0.8447\n",
      "Epoch 68/100\n",
      "161/161 [==============================] - 0s 212us/sample - loss: 0.3694 - accuracy: 0.8137\n",
      "Epoch 69/100\n",
      "161/161 [==============================] - 0s 123us/sample - loss: 0.3877 - accuracy: 0.8447\n",
      "Epoch 70/100\n",
      "161/161 [==============================] - 0s 195us/sample - loss: 0.3720 - accuracy: 0.8385\n",
      "Epoch 71/100\n",
      "161/161 [==============================] - 0s 184us/sample - loss: 0.3680 - accuracy: 0.8199\n",
      "Epoch 72/100\n",
      "161/161 [==============================] - 0s 131us/sample - loss: 0.3397 - accuracy: 0.8447\n",
      "Epoch 73/100\n",
      "161/161 [==============================] - 0s 122us/sample - loss: 0.3588 - accuracy: 0.8509\n",
      "Epoch 74/100\n",
      "161/161 [==============================] - 0s 114us/sample - loss: 0.3666 - accuracy: 0.8261\n",
      "Epoch 75/100\n",
      "161/161 [==============================] - 0s 154us/sample - loss: 0.3816 - accuracy: 0.8385\n",
      "Epoch 76/100\n",
      "161/161 [==============================] - 0s 141us/sample - loss: 0.3354 - accuracy: 0.8696\n",
      "Epoch 77/100\n",
      "161/161 [==============================] - 0s 130us/sample - loss: 0.3492 - accuracy: 0.8571\n",
      "Epoch 78/100\n",
      "161/161 [==============================] - 0s 207us/sample - loss: 0.3335 - accuracy: 0.8634\n",
      "Epoch 79/100\n",
      "161/161 [==============================] - 0s 137us/sample - loss: 0.3885 - accuracy: 0.8137\n",
      "Epoch 80/100\n",
      "161/161 [==============================] - 0s 186us/sample - loss: 0.3849 - accuracy: 0.8634\n",
      "Epoch 81/100\n",
      "161/161 [==============================] - 0s 199us/sample - loss: 0.3235 - accuracy: 0.8696\n",
      "Epoch 82/100\n",
      "161/161 [==============================] - 0s 153us/sample - loss: 0.3741 - accuracy: 0.8323\n",
      "Epoch 83/100\n",
      "161/161 [==============================] - 0s 113us/sample - loss: 0.3594 - accuracy: 0.8634\n",
      "Epoch 84/100\n",
      "161/161 [==============================] - 0s 163us/sample - loss: 0.3509 - accuracy: 0.8261\n",
      "Epoch 85/100\n",
      "161/161 [==============================] - 0s 151us/sample - loss: 0.3378 - accuracy: 0.8696\n",
      "Epoch 86/100\n",
      "161/161 [==============================] - 0s 193us/sample - loss: 0.3406 - accuracy: 0.8820\n",
      "Epoch 87/100\n",
      "161/161 [==============================] - 0s 193us/sample - loss: 0.3540 - accuracy: 0.8447\n",
      "Epoch 88/100\n",
      "161/161 [==============================] - 0s 119us/sample - loss: 0.3744 - accuracy: 0.8634\n",
      "Epoch 89/100\n",
      "161/161 [==============================] - 0s 142us/sample - loss: 0.3516 - accuracy: 0.8385\n",
      "Epoch 90/100\n",
      "161/161 [==============================] - 0s 149us/sample - loss: 0.3584 - accuracy: 0.8509\n",
      "Epoch 91/100\n",
      "161/161 [==============================] - 0s 139us/sample - loss: 0.3479 - accuracy: 0.8634\n",
      "Epoch 92/100\n",
      "161/161 [==============================] - 0s 114us/sample - loss: 0.3194 - accuracy: 0.8634\n",
      "Epoch 93/100\n",
      "161/161 [==============================] - 0s 150us/sample - loss: 0.3284 - accuracy: 0.8696\n",
      "Epoch 94/100\n",
      "161/161 [==============================] - 0s 130us/sample - loss: 0.3600 - accuracy: 0.8571\n",
      "Epoch 95/100\n",
      "161/161 [==============================] - 0s 191us/sample - loss: 0.3369 - accuracy: 0.8571\n",
      "Epoch 96/100\n",
      "161/161 [==============================] - 0s 229us/sample - loss: 0.3418 - accuracy: 0.8385\n",
      "Epoch 97/100\n",
      "161/161 [==============================] - 0s 223us/sample - loss: 0.3475 - accuracy: 0.8447\n",
      "Epoch 98/100\n",
      "161/161 [==============================] - 0s 167us/sample - loss: 0.3338 - accuracy: 0.8509\n",
      "Epoch 99/100\n",
      "161/161 [==============================] - 0s 137us/sample - loss: 0.3459 - accuracy: 0.8820\n",
      "Epoch 100/100\n",
      "161/161 [==============================] - 0s 142us/sample - loss: 0.3245 - accuracy: 0.8509\n",
      "81/81 [==============================] - 0s 2ms/sample - loss: 0.4463 - accuracy: 0.7654\n",
      "Train on 162 samples\n",
      "Epoch 1/100\n",
      "162/162 [==============================] - 1s 5ms/sample - loss: 0.6116 - accuracy: 0.6049\n",
      "Epoch 2/100\n",
      "162/162 [==============================] - 0s 117us/sample - loss: 0.5711 - accuracy: 0.6852\n",
      "Epoch 3/100\n",
      "162/162 [==============================] - 0s 110us/sample - loss: 0.5807 - accuracy: 0.6605\n",
      "Epoch 4/100\n",
      "162/162 [==============================] - 0s 93us/sample - loss: 0.5448 - accuracy: 0.7099\n",
      "Epoch 5/100\n",
      "162/162 [==============================] - 0s 152us/sample - loss: 0.5640 - accuracy: 0.7037\n",
      "Epoch 6/100\n",
      "162/162 [==============================] - 0s 93us/sample - loss: 0.5352 - accuracy: 0.7531\n",
      "Epoch 7/100\n",
      "162/162 [==============================] - 0s 105us/sample - loss: 0.5698 - accuracy: 0.6975\n",
      "Epoch 8/100\n",
      "162/162 [==============================] - 0s 88us/sample - loss: 0.5296 - accuracy: 0.7407\n",
      "Epoch 9/100\n",
      "162/162 [==============================] - 0s 85us/sample - loss: 0.5321 - accuracy: 0.6914\n",
      "Epoch 10/100\n",
      "162/162 [==============================] - 0s 76us/sample - loss: 0.5197 - accuracy: 0.7531\n",
      "Epoch 11/100\n",
      "162/162 [==============================] - 0s 79us/sample - loss: 0.5228 - accuracy: 0.7654\n",
      "Epoch 12/100\n",
      "162/162 [==============================] - 0s 81us/sample - loss: 0.4876 - accuracy: 0.7778\n",
      "Epoch 13/100\n",
      "162/162 [==============================] - 0s 115us/sample - loss: 0.4814 - accuracy: 0.7901\n",
      "Epoch 14/100\n",
      "162/162 [==============================] - 0s 99us/sample - loss: 0.4742 - accuracy: 0.7716\n",
      "Epoch 15/100\n",
      "162/162 [==============================] - 0s 79us/sample - loss: 0.4828 - accuracy: 0.7840\n",
      "Epoch 16/100\n",
      "162/162 [==============================] - 0s 84us/sample - loss: 0.5010 - accuracy: 0.7531\n",
      "Epoch 17/100\n",
      "162/162 [==============================] - 0s 81us/sample - loss: 0.4739 - accuracy: 0.7469\n",
      "Epoch 18/100\n",
      "162/162 [==============================] - 0s 85us/sample - loss: 0.4736 - accuracy: 0.7778\n",
      "Epoch 19/100\n",
      "162/162 [==============================] - 0s 98us/sample - loss: 0.4638 - accuracy: 0.8025\n",
      "Epoch 20/100\n",
      "162/162 [==============================] - 0s 81us/sample - loss: 0.4839 - accuracy: 0.7840\n",
      "Epoch 21/100\n",
      "162/162 [==============================] - 0s 78us/sample - loss: 0.4847 - accuracy: 0.7222\n",
      "Epoch 22/100\n",
      "162/162 [==============================] - 0s 96us/sample - loss: 0.4379 - accuracy: 0.8086\n",
      "Epoch 23/100\n",
      "162/162 [==============================] - 0s 88us/sample - loss: 0.4504 - accuracy: 0.7963\n",
      "Epoch 24/100\n",
      "162/162 [==============================] - 0s 109us/sample - loss: 0.4499 - accuracy: 0.7901\n",
      "Epoch 25/100\n",
      "162/162 [==============================] - 0s 93us/sample - loss: 0.4786 - accuracy: 0.7901\n",
      "Epoch 26/100\n",
      "162/162 [==============================] - 0s 154us/sample - loss: 0.4053 - accuracy: 0.8395\n",
      "Epoch 27/100\n",
      "162/162 [==============================] - 0s 142us/sample - loss: 0.4519 - accuracy: 0.8025\n",
      "Epoch 28/100\n",
      "162/162 [==============================] - 0s 203us/sample - loss: 0.4266 - accuracy: 0.8148\n",
      "Epoch 29/100\n",
      "162/162 [==============================] - 0s 114us/sample - loss: 0.4442 - accuracy: 0.7963\n",
      "Epoch 30/100\n",
      "162/162 [==============================] - 0s 148us/sample - loss: 0.4293 - accuracy: 0.8272\n",
      "Epoch 31/100\n",
      "162/162 [==============================] - 0s 160us/sample - loss: 0.4304 - accuracy: 0.8333\n",
      "Epoch 32/100\n",
      "162/162 [==============================] - 0s 127us/sample - loss: 0.4290 - accuracy: 0.7963\n",
      "Epoch 33/100\n",
      "162/162 [==============================] - 0s 148us/sample - loss: 0.4291 - accuracy: 0.8148\n",
      "Epoch 34/100\n",
      "162/162 [==============================] - 0s 117us/sample - loss: 0.4210 - accuracy: 0.8210\n",
      "Epoch 35/100\n",
      "162/162 [==============================] - 0s 75us/sample - loss: 0.4071 - accuracy: 0.8148\n",
      "Epoch 36/100\n",
      "162/162 [==============================] - 0s 96us/sample - loss: 0.4077 - accuracy: 0.7963\n",
      "Epoch 37/100\n",
      "162/162 [==============================] - 0s 79us/sample - loss: 0.4006 - accuracy: 0.8395\n",
      "Epoch 38/100\n",
      "162/162 [==============================] - 0s 87us/sample - loss: 0.3747 - accuracy: 0.8148\n",
      "Epoch 39/100\n",
      "162/162 [==============================] - 0s 87us/sample - loss: 0.4370 - accuracy: 0.8148\n",
      "Epoch 40/100\n",
      "162/162 [==============================] - 0s 117us/sample - loss: 0.4047 - accuracy: 0.8333\n",
      "Epoch 41/100\n",
      "162/162 [==============================] - 0s 103us/sample - loss: 0.4363 - accuracy: 0.8272\n",
      "Epoch 42/100\n",
      "162/162 [==============================] - 0s 92us/sample - loss: 0.3735 - accuracy: 0.8395\n",
      "Epoch 43/100\n",
      "162/162 [==============================] - 0s 96us/sample - loss: 0.4110 - accuracy: 0.8210\n",
      "Epoch 44/100\n",
      "162/162 [==============================] - 0s 107us/sample - loss: 0.4127 - accuracy: 0.8148\n",
      "Epoch 45/100\n",
      "162/162 [==============================] - 0s 81us/sample - loss: 0.4097 - accuracy: 0.8333\n",
      "Epoch 46/100\n",
      "162/162 [==============================] - 0s 97us/sample - loss: 0.3760 - accuracy: 0.8333\n",
      "Epoch 47/100\n",
      "162/162 [==============================] - 0s 96us/sample - loss: 0.3773 - accuracy: 0.7963\n",
      "Epoch 48/100\n",
      "162/162 [==============================] - 0s 99us/sample - loss: 0.4037 - accuracy: 0.8210\n",
      "Epoch 49/100\n",
      "162/162 [==============================] - 0s 115us/sample - loss: 0.3794 - accuracy: 0.8210\n",
      "Epoch 50/100\n",
      "162/162 [==============================] - 0s 153us/sample - loss: 0.3913 - accuracy: 0.8457\n",
      "Epoch 51/100\n",
      "162/162 [==============================] - 0s 119us/sample - loss: 0.3971 - accuracy: 0.8148\n",
      "Epoch 52/100\n",
      "162/162 [==============================] - 0s 85us/sample - loss: 0.3795 - accuracy: 0.8395\n",
      "Epoch 53/100\n",
      "162/162 [==============================] - 0s 75us/sample - loss: 0.3600 - accuracy: 0.8395\n",
      "Epoch 54/100\n",
      "162/162 [==============================] - 0s 103us/sample - loss: 0.3816 - accuracy: 0.8519\n",
      "Epoch 55/100\n",
      "162/162 [==============================] - 0s 96us/sample - loss: 0.3745 - accuracy: 0.8272\n",
      "Epoch 56/100\n",
      "162/162 [==============================] - 0s 100us/sample - loss: 0.3901 - accuracy: 0.8333\n",
      "Epoch 57/100\n",
      "162/162 [==============================] - 0s 83us/sample - loss: 0.3519 - accuracy: 0.8333\n",
      "Epoch 58/100\n",
      "162/162 [==============================] - 0s 92us/sample - loss: 0.3614 - accuracy: 0.8457\n",
      "Epoch 59/100\n",
      "162/162 [==============================] - 0s 84us/sample - loss: 0.3694 - accuracy: 0.8519\n",
      "Epoch 60/100\n",
      "162/162 [==============================] - 0s 99us/sample - loss: 0.3709 - accuracy: 0.8580\n",
      "Epoch 61/100\n",
      "162/162 [==============================] - 0s 91us/sample - loss: 0.3359 - accuracy: 0.8333\n",
      "Epoch 62/100\n",
      "162/162 [==============================] - 0s 68us/sample - loss: 0.3517 - accuracy: 0.8457\n",
      "Epoch 63/100\n",
      "162/162 [==============================] - 0s 76us/sample - loss: 0.3288 - accuracy: 0.8580\n",
      "Epoch 64/100\n",
      "162/162 [==============================] - 0s 88us/sample - loss: 0.3516 - accuracy: 0.8642\n",
      "Epoch 65/100\n",
      "162/162 [==============================] - 0s 99us/sample - loss: 0.3591 - accuracy: 0.8148\n",
      "Epoch 66/100\n",
      "162/162 [==============================] - 0s 103us/sample - loss: 0.3594 - accuracy: 0.8765\n",
      "Epoch 67/100\n",
      "162/162 [==============================] - 0s 79us/sample - loss: 0.3539 - accuracy: 0.8642\n",
      "Epoch 68/100\n",
      "162/162 [==============================] - 0s 119us/sample - loss: 0.3447 - accuracy: 0.8704\n",
      "Epoch 69/100\n",
      "162/162 [==============================] - 0s 82us/sample - loss: 0.3822 - accuracy: 0.8519\n",
      "Epoch 70/100\n",
      "162/162 [==============================] - 0s 87us/sample - loss: 0.3441 - accuracy: 0.8333\n",
      "Epoch 71/100\n",
      "162/162 [==============================] - 0s 214us/sample - loss: 0.3480 - accuracy: 0.8580\n",
      "Epoch 72/100\n",
      "162/162 [==============================] - 0s 134us/sample - loss: 0.3462 - accuracy: 0.8642\n",
      "Epoch 73/100\n",
      "162/162 [==============================] - 0s 199us/sample - loss: 0.3518 - accuracy: 0.8395\n",
      "Epoch 74/100\n",
      "162/162 [==============================] - 0s 314us/sample - loss: 0.3512 - accuracy: 0.8580\n",
      "Epoch 75/100\n",
      "162/162 [==============================] - 0s 116us/sample - loss: 0.3479 - accuracy: 0.8395\n",
      "Epoch 76/100\n",
      "162/162 [==============================] - 0s 162us/sample - loss: 0.3321 - accuracy: 0.8642\n",
      "Epoch 77/100\n",
      "162/162 [==============================] - 0s 116us/sample - loss: 0.3488 - accuracy: 0.8395\n",
      "Epoch 78/100\n",
      "162/162 [==============================] - 0s 72us/sample - loss: 0.3681 - accuracy: 0.8457\n",
      "Epoch 79/100\n",
      "162/162 [==============================] - 0s 72us/sample - loss: 0.3561 - accuracy: 0.8272\n",
      "Epoch 80/100\n",
      "162/162 [==============================] - 0s 78us/sample - loss: 0.3419 - accuracy: 0.8395\n",
      "Epoch 81/100\n",
      "162/162 [==============================] - 0s 88us/sample - loss: 0.3355 - accuracy: 0.8580\n",
      "Epoch 82/100\n",
      "162/162 [==============================] - 0s 97us/sample - loss: 0.3380 - accuracy: 0.8642\n",
      "Epoch 83/100\n",
      "162/162 [==============================] - 0s 103us/sample - loss: 0.3353 - accuracy: 0.8580\n",
      "Epoch 84/100\n",
      "162/162 [==============================] - 0s 103us/sample - loss: 0.3771 - accuracy: 0.8395\n",
      "Epoch 85/100\n",
      "162/162 [==============================] - 0s 79us/sample - loss: 0.3543 - accuracy: 0.8395\n",
      "Epoch 86/100\n",
      "162/162 [==============================] - 0s 98us/sample - loss: 0.3336 - accuracy: 0.8580\n",
      "Epoch 87/100\n",
      "162/162 [==============================] - 0s 84us/sample - loss: 0.3547 - accuracy: 0.8519\n",
      "Epoch 88/100\n",
      "162/162 [==============================] - 0s 105us/sample - loss: 0.3452 - accuracy: 0.8333\n",
      "Epoch 89/100\n",
      "162/162 [==============================] - 0s 101us/sample - loss: 0.3334 - accuracy: 0.8457\n",
      "Epoch 90/100\n",
      "162/162 [==============================] - 0s 98us/sample - loss: 0.3359 - accuracy: 0.8580\n",
      "Epoch 91/100\n",
      "162/162 [==============================] - 0s 100us/sample - loss: 0.3435 - accuracy: 0.8519\n",
      "Epoch 92/100\n",
      "162/162 [==============================] - 0s 82us/sample - loss: 0.3525 - accuracy: 0.8519\n",
      "Epoch 93/100\n",
      "162/162 [==============================] - 0s 107us/sample - loss: 0.3498 - accuracy: 0.8395\n",
      "Epoch 94/100\n",
      "162/162 [==============================] - 0s 79us/sample - loss: 0.3498 - accuracy: 0.8519\n",
      "Epoch 95/100\n",
      "162/162 [==============================] - 0s 72us/sample - loss: 0.3444 - accuracy: 0.8704\n",
      "Epoch 96/100\n",
      "162/162 [==============================] - 0s 92us/sample - loss: 0.3343 - accuracy: 0.8580\n",
      "Epoch 97/100\n",
      "162/162 [==============================] - 0s 76us/sample - loss: 0.3484 - accuracy: 0.8827\n",
      "Epoch 98/100\n",
      "162/162 [==============================] - 0s 99us/sample - loss: 0.3553 - accuracy: 0.8395\n",
      "Epoch 99/100\n",
      "162/162 [==============================] - 0s 96us/sample - loss: 0.3668 - accuracy: 0.8580\n",
      "Epoch 100/100\n",
      "162/162 [==============================] - 0s 105us/sample - loss: 0.2955 - accuracy: 0.8765\n",
      "80/80 [==============================] - 0s 2ms/sample - loss: 0.3778 - accuracy: 0.8375\n",
      "Train on 161 samples\n",
      "Epoch 1/100\n",
      "161/161 [==============================] - 1s 6ms/sample - loss: 0.8179 - accuracy: 0.4720\n",
      "Epoch 2/100\n",
      "161/161 [==============================] - 0s 52us/sample - loss: 0.7832 - accuracy: 0.4907\n",
      "Epoch 3/100\n",
      "161/161 [==============================] - 0s 56us/sample - loss: 0.8066 - accuracy: 0.4596\n",
      "Epoch 4/100\n",
      "161/161 [==============================] - 0s 71us/sample - loss: 0.7696 - accuracy: 0.5652\n",
      "Epoch 5/100\n",
      "161/161 [==============================] - 0s 51us/sample - loss: 0.7537 - accuracy: 0.5342\n",
      "Epoch 6/100\n",
      "161/161 [==============================] - 0s 52us/sample - loss: 0.7653 - accuracy: 0.5093\n",
      "Epoch 7/100\n",
      "161/161 [==============================] - 0s 53us/sample - loss: 0.7658 - accuracy: 0.5217\n",
      "Epoch 8/100\n",
      "161/161 [==============================] - 0s 49us/sample - loss: 0.7412 - accuracy: 0.5466\n",
      "Epoch 9/100\n",
      "161/161 [==============================] - 0s 50us/sample - loss: 0.7322 - accuracy: 0.5280\n",
      "Epoch 10/100\n",
      "161/161 [==============================] - 0s 74us/sample - loss: 0.7150 - accuracy: 0.5404\n",
      "Epoch 11/100\n",
      "161/161 [==============================] - 0s 184us/sample - loss: 0.6892 - accuracy: 0.6211\n",
      "Epoch 12/100\n",
      "161/161 [==============================] - 0s 189us/sample - loss: 0.6780 - accuracy: 0.5963\n",
      "Epoch 13/100\n",
      "161/161 [==============================] - 0s 50us/sample - loss: 0.6902 - accuracy: 0.5652\n",
      "Epoch 14/100\n",
      "161/161 [==============================] - 0s 109us/sample - loss: 0.6896 - accuracy: 0.6025\n",
      "Epoch 15/100\n",
      "161/161 [==============================] - 0s 113us/sample - loss: 0.6815 - accuracy: 0.6025\n",
      "Epoch 16/100\n",
      "161/161 [==============================] - 0s 96us/sample - loss: 0.6579 - accuracy: 0.6149\n",
      "Epoch 17/100\n",
      "161/161 [==============================] - 0s 107us/sample - loss: 0.6457 - accuracy: 0.6832\n",
      "Epoch 18/100\n",
      "161/161 [==============================] - 0s 99us/sample - loss: 0.6252 - accuracy: 0.6957\n",
      "Epoch 19/100\n",
      "161/161 [==============================] - 0s 109us/sample - loss: 0.6554 - accuracy: 0.6460\n",
      "Epoch 20/100\n",
      "161/161 [==============================] - 0s 90us/sample - loss: 0.6456 - accuracy: 0.6398\n",
      "Epoch 21/100\n",
      "161/161 [==============================] - 0s 62us/sample - loss: 0.6193 - accuracy: 0.6708\n",
      "Epoch 22/100\n",
      "161/161 [==============================] - 0s 74us/sample - loss: 0.6316 - accuracy: 0.6832\n",
      "Epoch 23/100\n",
      "161/161 [==============================] - 0s 74us/sample - loss: 0.6327 - accuracy: 0.6522\n",
      "Epoch 24/100\n",
      "161/161 [==============================] - 0s 57us/sample - loss: 0.5926 - accuracy: 0.6770\n",
      "Epoch 25/100\n",
      "161/161 [==============================] - 0s 64us/sample - loss: 0.5946 - accuracy: 0.6646\n",
      "Epoch 26/100\n",
      "161/161 [==============================] - 0s 49us/sample - loss: 0.5646 - accuracy: 0.7329\n",
      "Epoch 27/100\n",
      "161/161 [==============================] - 0s 47us/sample - loss: 0.5700 - accuracy: 0.7640\n",
      "Epoch 28/100\n",
      "161/161 [==============================] - 0s 48us/sample - loss: 0.5923 - accuracy: 0.7205\n",
      "Epoch 29/100\n",
      "161/161 [==============================] - 0s 58us/sample - loss: 0.5760 - accuracy: 0.7267\n",
      "Epoch 30/100\n",
      "161/161 [==============================] - 0s 50us/sample - loss: 0.6022 - accuracy: 0.6832\n",
      "Epoch 31/100\n",
      "161/161 [==============================] - 0s 65us/sample - loss: 0.6221 - accuracy: 0.6273\n",
      "Epoch 32/100\n",
      "161/161 [==============================] - 0s 58us/sample - loss: 0.5691 - accuracy: 0.7329\n",
      "Epoch 33/100\n",
      "161/161 [==============================] - 0s 125us/sample - loss: 0.5476 - accuracy: 0.7453\n",
      "Epoch 34/100\n",
      "161/161 [==============================] - 0s 292us/sample - loss: 0.5437 - accuracy: 0.7702\n",
      "Epoch 35/100\n",
      "161/161 [==============================] - 0s 217us/sample - loss: 0.5386 - accuracy: 0.7826\n",
      "Epoch 36/100\n",
      "161/161 [==============================] - 0s 209us/sample - loss: 0.5261 - accuracy: 0.7702\n",
      "Epoch 37/100\n",
      "161/161 [==============================] - 0s 215us/sample - loss: 0.5264 - accuracy: 0.7578\n",
      "Epoch 38/100\n",
      "161/161 [==============================] - 0s 94us/sample - loss: 0.5255 - accuracy: 0.7640\n",
      "Epoch 39/100\n",
      "161/161 [==============================] - 0s 128us/sample - loss: 0.5175 - accuracy: 0.7640\n",
      "Epoch 40/100\n",
      "161/161 [==============================] - 0s 45us/sample - loss: 0.5064 - accuracy: 0.7888\n",
      "Epoch 41/100\n",
      "161/161 [==============================] - 0s 48us/sample - loss: 0.4772 - accuracy: 0.8199\n",
      "Epoch 42/100\n",
      "161/161 [==============================] - 0s 71us/sample - loss: 0.5144 - accuracy: 0.7950\n",
      "Epoch 43/100\n",
      "161/161 [==============================] - 0s 41us/sample - loss: 0.5281 - accuracy: 0.7329\n",
      "Epoch 44/100\n",
      "161/161 [==============================] - 0s 53us/sample - loss: 0.4958 - accuracy: 0.7640\n",
      "Epoch 45/100\n",
      "161/161 [==============================] - 0s 49us/sample - loss: 0.5031 - accuracy: 0.7764\n",
      "Epoch 46/100\n",
      "161/161 [==============================] - 0s 41us/sample - loss: 0.4836 - accuracy: 0.8012\n",
      "Epoch 47/100\n",
      "161/161 [==============================] - 0s 46us/sample - loss: 0.4609 - accuracy: 0.7950\n",
      "Epoch 48/100\n",
      "161/161 [==============================] - 0s 50us/sample - loss: 0.4810 - accuracy: 0.7516\n",
      "Epoch 49/100\n",
      "161/161 [==============================] - 0s 51us/sample - loss: 0.4716 - accuracy: 0.7888\n",
      "Epoch 50/100\n",
      "161/161 [==============================] - 0s 44us/sample - loss: 0.4768 - accuracy: 0.8075\n",
      "Epoch 51/100\n",
      "161/161 [==============================] - 0s 49us/sample - loss: 0.4716 - accuracy: 0.7950\n",
      "Epoch 52/100\n",
      "161/161 [==============================] - 0s 63us/sample - loss: 0.4697 - accuracy: 0.7888\n",
      "Epoch 53/100\n",
      "161/161 [==============================] - 0s 66us/sample - loss: 0.4828 - accuracy: 0.7764\n",
      "Epoch 54/100\n",
      "161/161 [==============================] - 0s 131us/sample - loss: 0.4205 - accuracy: 0.8509\n",
      "Epoch 55/100\n",
      "161/161 [==============================] - 0s 188us/sample - loss: 0.4376 - accuracy: 0.8075\n",
      "Epoch 56/100\n",
      "161/161 [==============================] - 0s 258us/sample - loss: 0.4572 - accuracy: 0.7950\n",
      "Epoch 57/100\n",
      "161/161 [==============================] - 0s 117us/sample - loss: 0.4368 - accuracy: 0.8261\n",
      "Epoch 58/100\n",
      "161/161 [==============================] - 0s 100us/sample - loss: 0.4721 - accuracy: 0.7516\n",
      "Epoch 59/100\n",
      "161/161 [==============================] - 0s 94us/sample - loss: 0.4427 - accuracy: 0.8075\n",
      "Epoch 60/100\n",
      "161/161 [==============================] - 0s 77us/sample - loss: 0.4289 - accuracy: 0.8509\n",
      "Epoch 61/100\n",
      "161/161 [==============================] - 0s 78us/sample - loss: 0.4504 - accuracy: 0.8012\n",
      "Epoch 62/100\n",
      "161/161 [==============================] - 0s 88us/sample - loss: 0.4521 - accuracy: 0.8137\n",
      "Epoch 63/100\n",
      "161/161 [==============================] - 0s 58us/sample - loss: 0.4169 - accuracy: 0.8261\n",
      "Epoch 64/100\n",
      "161/161 [==============================] - 0s 110us/sample - loss: 0.4205 - accuracy: 0.8447\n",
      "Epoch 65/100\n",
      "161/161 [==============================] - 0s 82us/sample - loss: 0.4057 - accuracy: 0.8323\n",
      "Epoch 66/100\n",
      "161/161 [==============================] - 0s 65us/sample - loss: 0.4102 - accuracy: 0.8137\n",
      "Epoch 67/100\n",
      "161/161 [==============================] - 0s 80us/sample - loss: 0.4306 - accuracy: 0.8137\n",
      "Epoch 68/100\n",
      "161/161 [==============================] - 0s 65us/sample - loss: 0.4180 - accuracy: 0.8634\n",
      "Epoch 69/100\n",
      "161/161 [==============================] - 0s 61us/sample - loss: 0.4154 - accuracy: 0.8447\n",
      "Epoch 70/100\n",
      "161/161 [==============================] - 0s 54us/sample - loss: 0.4467 - accuracy: 0.8075\n",
      "Epoch 71/100\n",
      "161/161 [==============================] - 0s 67us/sample - loss: 0.3947 - accuracy: 0.8634\n",
      "Epoch 72/100\n",
      "161/161 [==============================] - 0s 68us/sample - loss: 0.4004 - accuracy: 0.8261\n",
      "Epoch 73/100\n",
      "161/161 [==============================] - 0s 71us/sample - loss: 0.4187 - accuracy: 0.8075\n",
      "Epoch 74/100\n",
      "161/161 [==============================] - 0s 76us/sample - loss: 0.4208 - accuracy: 0.8261\n",
      "Epoch 75/100\n",
      "161/161 [==============================] - 0s 85us/sample - loss: 0.3963 - accuracy: 0.8199\n",
      "Epoch 76/100\n",
      "161/161 [==============================] - 0s 72us/sample - loss: 0.4172 - accuracy: 0.8075\n",
      "Epoch 77/100\n",
      "161/161 [==============================] - 0s 80us/sample - loss: 0.4090 - accuracy: 0.8509\n",
      "Epoch 78/100\n",
      "161/161 [==============================] - 0s 94us/sample - loss: 0.3978 - accuracy: 0.8509\n",
      "Epoch 79/100\n",
      "161/161 [==============================] - 0s 198us/sample - loss: 0.4120 - accuracy: 0.8447\n",
      "Epoch 80/100\n",
      "161/161 [==============================] - 0s 222us/sample - loss: 0.3925 - accuracy: 0.8385\n",
      "Epoch 81/100\n",
      "161/161 [==============================] - 0s 198us/sample - loss: 0.3793 - accuracy: 0.8447\n",
      "Epoch 82/100\n",
      "161/161 [==============================] - 0s 204us/sample - loss: 0.3950 - accuracy: 0.8075\n",
      "Epoch 83/100\n",
      "161/161 [==============================] - 0s 132us/sample - loss: 0.3949 - accuracy: 0.8385\n",
      "Epoch 84/100\n",
      "161/161 [==============================] - 0s 98us/sample - loss: 0.3822 - accuracy: 0.8696\n",
      "Epoch 85/100\n",
      "161/161 [==============================] - 0s 81us/sample - loss: 0.3596 - accuracy: 0.8758\n",
      "Epoch 86/100\n",
      "161/161 [==============================] - 0s 149us/sample - loss: 0.3755 - accuracy: 0.8447\n",
      "Epoch 87/100\n",
      "161/161 [==============================] - 0s 181us/sample - loss: 0.3971 - accuracy: 0.8571\n",
      "Epoch 88/100\n",
      "161/161 [==============================] - 0s 133us/sample - loss: 0.3804 - accuracy: 0.8634\n",
      "Epoch 89/100\n",
      "161/161 [==============================] - 0s 85us/sample - loss: 0.3694 - accuracy: 0.8634\n",
      "Epoch 90/100\n",
      "161/161 [==============================] - 0s 104us/sample - loss: 0.4145 - accuracy: 0.8137\n",
      "Epoch 91/100\n",
      "161/161 [==============================] - 0s 145us/sample - loss: 0.3778 - accuracy: 0.8509\n",
      "Epoch 92/100\n",
      "161/161 [==============================] - 0s 91us/sample - loss: 0.3478 - accuracy: 0.9006\n",
      "Epoch 93/100\n",
      "161/161 [==============================] - ETA: 0s - loss: 0.3526 - accuracy: 0.90 - 0s 139us/sample - loss: 0.3658 - accuracy: 0.8758\n",
      "Epoch 94/100\n",
      "161/161 [==============================] - 0s 78us/sample - loss: 0.3777 - accuracy: 0.8571\n",
      "Epoch 95/100\n",
      "161/161 [==============================] - 0s 72us/sample - loss: 0.3777 - accuracy: 0.8634\n",
      "Epoch 96/100\n",
      "161/161 [==============================] - 0s 105us/sample - loss: 0.3799 - accuracy: 0.8385\n",
      "Epoch 97/100\n",
      "161/161 [==============================] - 0s 87us/sample - loss: 0.3689 - accuracy: 0.8571\n",
      "Epoch 98/100\n",
      "161/161 [==============================] - 0s 60us/sample - loss: 0.3702 - accuracy: 0.8509\n",
      "Epoch 99/100\n",
      "161/161 [==============================] - 0s 77us/sample - loss: 0.3577 - accuracy: 0.8509\n",
      "Epoch 100/100\n",
      "161/161 [==============================] - 0s 69us/sample - loss: 0.3713 - accuracy: 0.8385\n",
      "81/81 [==============================] - 0s 1ms/sample - loss: 0.3914 - accuracy: 0.8519\n",
      "Train on 161 samples\n",
      "Epoch 1/100\n",
      "161/161 [==============================] - 1s 5ms/sample - loss: 0.8778 - accuracy: 0.4783\n",
      "Epoch 2/100\n",
      "161/161 [==============================] - 0s 55us/sample - loss: 0.8894 - accuracy: 0.4907\n",
      "Epoch 3/100\n",
      "161/161 [==============================] - 0s 89us/sample - loss: 0.8803 - accuracy: 0.5093\n",
      "Epoch 4/100\n",
      "161/161 [==============================] - 0s 108us/sample - loss: 0.8496 - accuracy: 0.5031\n",
      "Epoch 5/100\n",
      "161/161 [==============================] - 0s 126us/sample - loss: 0.8699 - accuracy: 0.5342\n",
      "Epoch 6/100\n",
      "161/161 [==============================] - 0s 283us/sample - loss: 0.8469 - accuracy: 0.4658\n",
      "Epoch 7/100\n",
      "161/161 [==============================] - 0s 111us/sample - loss: 0.8459 - accuracy: 0.4534\n",
      "Epoch 8/100\n",
      "161/161 [==============================] - 0s 105us/sample - loss: 0.8422 - accuracy: 0.4720\n",
      "Epoch 9/100\n",
      "161/161 [==============================] - 0s 112us/sample - loss: 0.8241 - accuracy: 0.4658\n",
      "Epoch 10/100\n",
      "161/161 [==============================] - 0s 78us/sample - loss: 0.7977 - accuracy: 0.4969\n",
      "Epoch 11/100\n",
      "161/161 [==============================] - 0s 48us/sample - loss: 0.7679 - accuracy: 0.5093\n",
      "Epoch 12/100\n",
      "161/161 [==============================] - 0s 75us/sample - loss: 0.7776 - accuracy: 0.5342\n",
      "Epoch 13/100\n",
      "161/161 [==============================] - 0s 36us/sample - loss: 0.7935 - accuracy: 0.4720\n",
      "Epoch 14/100\n",
      "161/161 [==============================] - 0s 48us/sample - loss: 0.7455 - accuracy: 0.5280\n",
      "Epoch 15/100\n",
      "161/161 [==============================] - 0s 63us/sample - loss: 0.7464 - accuracy: 0.5280\n",
      "Epoch 16/100\n",
      "161/161 [==============================] - 0s 59us/sample - loss: 0.7662 - accuracy: 0.5280\n",
      "Epoch 17/100\n",
      "161/161 [==============================] - 0s 46us/sample - loss: 0.7874 - accuracy: 0.4596\n",
      "Epoch 18/100\n",
      "161/161 [==============================] - 0s 52us/sample - loss: 0.7272 - accuracy: 0.5280\n",
      "Epoch 19/100\n",
      "161/161 [==============================] - 0s 49us/sample - loss: 0.7369 - accuracy: 0.5342\n",
      "Epoch 20/100\n",
      "161/161 [==============================] - 0s 62us/sample - loss: 0.6918 - accuracy: 0.5839\n",
      "Epoch 21/100\n",
      "161/161 [==============================] - 0s 51us/sample - loss: 0.7228 - accuracy: 0.5217\n",
      "Epoch 22/100\n",
      "161/161 [==============================] - 0s 61us/sample - loss: 0.6982 - accuracy: 0.5217\n",
      "Epoch 23/100\n",
      "161/161 [==============================] - 0s 56us/sample - loss: 0.6809 - accuracy: 0.5652\n",
      "Epoch 24/100\n",
      "161/161 [==============================] - 0s 58us/sample - loss: 0.6854 - accuracy: 0.5528\n",
      "Epoch 25/100\n",
      "161/161 [==============================] - 0s 56us/sample - loss: 0.6593 - accuracy: 0.5839\n",
      "Epoch 26/100\n",
      "161/161 [==============================] - 0s 61us/sample - loss: 0.6966 - accuracy: 0.5280\n",
      "Epoch 27/100\n",
      "161/161 [==============================] - 0s 47us/sample - loss: 0.6678 - accuracy: 0.5652\n",
      "Epoch 28/100\n",
      "161/161 [==============================] - 0s 52us/sample - loss: 0.6945 - accuracy: 0.5714\n",
      "Epoch 29/100\n",
      "161/161 [==============================] - 0s 71us/sample - loss: 0.6570 - accuracy: 0.6025\n",
      "Epoch 30/100\n",
      "161/161 [==============================] - 0s 78us/sample - loss: 0.6458 - accuracy: 0.6149\n",
      "Epoch 31/100\n",
      "161/161 [==============================] - 0s 69us/sample - loss: 0.6460 - accuracy: 0.6149\n",
      "Epoch 32/100\n",
      "161/161 [==============================] - 0s 82us/sample - loss: 0.6572 - accuracy: 0.5714\n",
      "Epoch 33/100\n",
      "161/161 [==============================] - 0s 59us/sample - loss: 0.6318 - accuracy: 0.6087\n",
      "Epoch 34/100\n",
      "161/161 [==============================] - 0s 156us/sample - loss: 0.6195 - accuracy: 0.6273\n",
      "Epoch 35/100\n",
      "161/161 [==============================] - 0s 422us/sample - loss: 0.6249 - accuracy: 0.6522\n",
      "Epoch 36/100\n",
      "161/161 [==============================] - 0s 184us/sample - loss: 0.6237 - accuracy: 0.6335\n",
      "Epoch 37/100\n",
      "161/161 [==============================] - 0s 92us/sample - loss: 0.5832 - accuracy: 0.6708\n",
      "Epoch 38/100\n",
      "161/161 [==============================] - 0s 163us/sample - loss: 0.5891 - accuracy: 0.6273\n",
      "Epoch 39/100\n",
      "161/161 [==============================] - 0s 89us/sample - loss: 0.6029 - accuracy: 0.6398\n",
      "Epoch 40/100\n",
      "161/161 [==============================] - 0s 76us/sample - loss: 0.5925 - accuracy: 0.6832\n",
      "Epoch 41/100\n",
      "161/161 [==============================] - 0s 112us/sample - loss: 0.6361 - accuracy: 0.6025\n",
      "Epoch 42/100\n",
      "161/161 [==============================] - 0s 127us/sample - loss: 0.6097 - accuracy: 0.6646\n",
      "Epoch 43/100\n",
      "161/161 [==============================] - 0s 89us/sample - loss: 0.6067 - accuracy: 0.6646\n",
      "Epoch 44/100\n",
      "161/161 [==============================] - 0s 84us/sample - loss: 0.5841 - accuracy: 0.6770\n",
      "Epoch 45/100\n",
      "161/161 [==============================] - 0s 111us/sample - loss: 0.5673 - accuracy: 0.7143\n",
      "Epoch 46/100\n",
      "161/161 [==============================] - 0s 104us/sample - loss: 0.5636 - accuracy: 0.7081\n",
      "Epoch 47/100\n",
      "161/161 [==============================] - 0s 60us/sample - loss: 0.5714 - accuracy: 0.6894\n",
      "Epoch 48/100\n",
      "161/161 [==============================] - 0s 65us/sample - loss: 0.5539 - accuracy: 0.7143\n",
      "Epoch 49/100\n",
      "161/161 [==============================] - 0s 117us/sample - loss: 0.5509 - accuracy: 0.7081\n",
      "Epoch 50/100\n",
      "161/161 [==============================] - 0s 79us/sample - loss: 0.5755 - accuracy: 0.6894\n",
      "Epoch 51/100\n",
      "161/161 [==============================] - 0s 94us/sample - loss: 0.5709 - accuracy: 0.7019\n",
      "Epoch 52/100\n",
      "161/161 [==============================] - 0s 77us/sample - loss: 0.5622 - accuracy: 0.7267\n",
      "Epoch 53/100\n",
      "161/161 [==============================] - 0s 60us/sample - loss: 0.5506 - accuracy: 0.7391\n",
      "Epoch 54/100\n",
      "161/161 [==============================] - 0s 56us/sample - loss: 0.5343 - accuracy: 0.7267\n",
      "Epoch 55/100\n",
      "161/161 [==============================] - 0s 76us/sample - loss: 0.5345 - accuracy: 0.7516\n",
      "Epoch 56/100\n",
      "161/161 [==============================] - 0s 44us/sample - loss: 0.5344 - accuracy: 0.7391\n",
      "Epoch 57/100\n",
      "161/161 [==============================] - 0s 68us/sample - loss: 0.5379 - accuracy: 0.7143\n",
      "Epoch 58/100\n",
      "161/161 [==============================] - 0s 53us/sample - loss: 0.5315 - accuracy: 0.7640\n",
      "Epoch 59/100\n",
      "161/161 [==============================] - 0s 40us/sample - loss: 0.5406 - accuracy: 0.7143\n",
      "Epoch 60/100\n",
      "161/161 [==============================] - 0s 50us/sample - loss: 0.5233 - accuracy: 0.7143\n",
      "Epoch 61/100\n",
      "161/161 [==============================] - 0s 45us/sample - loss: 0.5283 - accuracy: 0.7516\n",
      "Epoch 62/100\n",
      "161/161 [==============================] - 0s 47us/sample - loss: 0.4986 - accuracy: 0.7578\n",
      "Epoch 63/100\n",
      "161/161 [==============================] - 0s 50us/sample - loss: 0.5115 - accuracy: 0.7391\n",
      "Epoch 64/100\n",
      "161/161 [==============================] - 0s 56us/sample - loss: 0.4888 - accuracy: 0.7640\n",
      "Epoch 65/100\n",
      "161/161 [==============================] - 0s 63us/sample - loss: 0.5317 - accuracy: 0.7640\n",
      "Epoch 66/100\n",
      "161/161 [==============================] - 0s 76us/sample - loss: 0.4904 - accuracy: 0.8012\n",
      "Epoch 67/100\n",
      "161/161 [==============================] - 0s 53us/sample - loss: 0.5066 - accuracy: 0.7391\n",
      "Epoch 68/100\n",
      "161/161 [==============================] - 0s 80us/sample - loss: 0.4878 - accuracy: 0.7888\n",
      "Epoch 69/100\n",
      "161/161 [==============================] - 0s 72us/sample - loss: 0.4927 - accuracy: 0.7826\n",
      "Epoch 70/100\n",
      "161/161 [==============================] - 0s 67us/sample - loss: 0.4821 - accuracy: 0.7267\n",
      "Epoch 71/100\n",
      "161/161 [==============================] - 0s 62us/sample - loss: 0.4776 - accuracy: 0.7702\n",
      "Epoch 72/100\n",
      "161/161 [==============================] - 0s 53us/sample - loss: 0.4857 - accuracy: 0.7826\n",
      "Epoch 73/100\n",
      "161/161 [==============================] - 0s 88us/sample - loss: 0.4809 - accuracy: 0.8075\n",
      "Epoch 74/100\n",
      "161/161 [==============================] - 0s 105us/sample - loss: 0.4840 - accuracy: 0.7702\n",
      "Epoch 75/100\n",
      "161/161 [==============================] - 0s 170us/sample - loss: 0.4745 - accuracy: 0.7702\n",
      "Epoch 76/100\n",
      "161/161 [==============================] - 0s 90us/sample - loss: 0.4690 - accuracy: 0.7950\n",
      "Epoch 77/100\n",
      "161/161 [==============================] - 0s 65us/sample - loss: 0.4923 - accuracy: 0.7764\n",
      "Epoch 78/100\n",
      "161/161 [==============================] - 0s 97us/sample - loss: 0.5030 - accuracy: 0.7267\n",
      "Epoch 79/100\n",
      "161/161 [==============================] - 0s 60us/sample - loss: 0.4739 - accuracy: 0.8012\n",
      "Epoch 80/100\n",
      "161/161 [==============================] - 0s 48us/sample - loss: 0.4979 - accuracy: 0.7826\n",
      "Epoch 81/100\n",
      "161/161 [==============================] - 0s 73us/sample - loss: 0.4610 - accuracy: 0.8199\n",
      "Epoch 82/100\n",
      "161/161 [==============================] - 0s 85us/sample - loss: 0.4513 - accuracy: 0.7950\n",
      "Epoch 83/100\n",
      "161/161 [==============================] - 0s 77us/sample - loss: 0.4803 - accuracy: 0.7888\n",
      "Epoch 84/100\n",
      "161/161 [==============================] - 0s 78us/sample - loss: 0.4850 - accuracy: 0.7764\n",
      "Epoch 85/100\n",
      "161/161 [==============================] - 0s 88us/sample - loss: 0.4597 - accuracy: 0.8075\n",
      "Epoch 86/100\n",
      "161/161 [==============================] - 0s 79us/sample - loss: 0.4535 - accuracy: 0.7950\n",
      "Epoch 87/100\n",
      "161/161 [==============================] - 0s 55us/sample - loss: 0.4646 - accuracy: 0.8075\n",
      "Epoch 88/100\n",
      "161/161 [==============================] - 0s 53us/sample - loss: 0.4543 - accuracy: 0.7640\n",
      "Epoch 89/100\n",
      "161/161 [==============================] - 0s 43us/sample - loss: 0.4683 - accuracy: 0.7888\n",
      "Epoch 90/100\n",
      "161/161 [==============================] - 0s 53us/sample - loss: 0.4533 - accuracy: 0.7764\n",
      "Epoch 91/100\n",
      "161/161 [==============================] - 0s 47us/sample - loss: 0.4790 - accuracy: 0.7764\n",
      "Epoch 92/100\n",
      "161/161 [==============================] - 0s 52us/sample - loss: 0.4819 - accuracy: 0.7516\n",
      "Epoch 93/100\n",
      "161/161 [==============================] - 0s 53us/sample - loss: 0.4238 - accuracy: 0.8137\n",
      "Epoch 94/100\n",
      "161/161 [==============================] - 0s 48us/sample - loss: 0.4188 - accuracy: 0.8199\n",
      "Epoch 95/100\n",
      "161/161 [==============================] - 0s 46us/sample - loss: 0.4378 - accuracy: 0.7888\n",
      "Epoch 96/100\n",
      "161/161 [==============================] - 0s 53us/sample - loss: 0.4524 - accuracy: 0.7950\n",
      "Epoch 97/100\n",
      "161/161 [==============================] - 0s 48us/sample - loss: 0.4317 - accuracy: 0.8571\n",
      "Epoch 98/100\n",
      "161/161 [==============================] - 0s 50us/sample - loss: 0.4280 - accuracy: 0.8137\n",
      "Epoch 99/100\n",
      "161/161 [==============================] - 0s 48us/sample - loss: 0.4017 - accuracy: 0.8137\n",
      "Epoch 100/100\n",
      "161/161 [==============================] - 0s 57us/sample - loss: 0.4333 - accuracy: 0.8323\n",
      "81/81 [==============================] - 0s 2ms/sample - loss: 0.4283 - accuracy: 0.8642\n",
      "Train on 162 samples\n",
      "Epoch 1/100\n",
      "162/162 [==============================] - 1s 4ms/sample - loss: 0.7369 - accuracy: 0.4938\n",
      "Epoch 2/100\n",
      "162/162 [==============================] - 0s 61us/sample - loss: 0.6864 - accuracy: 0.5679\n",
      "Epoch 3/100\n",
      "162/162 [==============================] - 0s 145us/sample - loss: 0.6851 - accuracy: 0.5926\n",
      "Epoch 4/100\n",
      "162/162 [==============================] - 0s 58us/sample - loss: 0.6965 - accuracy: 0.5926\n",
      "Epoch 5/100\n",
      "162/162 [==============================] - 0s 87us/sample - loss: 0.7019 - accuracy: 0.5741\n",
      "Epoch 6/100\n",
      "162/162 [==============================] - 0s 106us/sample - loss: 0.6976 - accuracy: 0.5988\n",
      "Epoch 7/100\n",
      "162/162 [==============================] - 0s 81us/sample - loss: 0.6697 - accuracy: 0.6296\n",
      "Epoch 8/100\n",
      "162/162 [==============================] - 0s 97us/sample - loss: 0.6370 - accuracy: 0.6543\n",
      "Epoch 9/100\n",
      "162/162 [==============================] - 0s 97us/sample - loss: 0.6538 - accuracy: 0.6235\n",
      "Epoch 10/100\n",
      "162/162 [==============================] - 0s 102us/sample - loss: 0.6471 - accuracy: 0.6852\n",
      "Epoch 11/100\n",
      "162/162 [==============================] - 0s 84us/sample - loss: 0.6547 - accuracy: 0.6173\n",
      "Epoch 12/100\n",
      "162/162 [==============================] - 0s 103us/sample - loss: 0.6386 - accuracy: 0.6605\n",
      "Epoch 13/100\n",
      "162/162 [==============================] - 0s 109us/sample - loss: 0.6162 - accuracy: 0.6358\n",
      "Epoch 14/100\n",
      "162/162 [==============================] - 0s 101us/sample - loss: 0.6575 - accuracy: 0.6296\n",
      "Epoch 15/100\n",
      "162/162 [==============================] - 0s 91us/sample - loss: 0.6074 - accuracy: 0.6914\n",
      "Epoch 16/100\n",
      "162/162 [==============================] - 0s 104us/sample - loss: 0.6186 - accuracy: 0.6543\n",
      "Epoch 17/100\n",
      "162/162 [==============================] - 0s 115us/sample - loss: 0.6296 - accuracy: 0.6605\n",
      "Epoch 18/100\n",
      "162/162 [==============================] - 0s 82us/sample - loss: 0.6160 - accuracy: 0.6975\n",
      "Epoch 19/100\n",
      "162/162 [==============================] - 0s 97us/sample - loss: 0.5980 - accuracy: 0.6852\n",
      "Epoch 20/100\n",
      "162/162 [==============================] - 0s 98us/sample - loss: 0.6061 - accuracy: 0.6975\n",
      "Epoch 21/100\n",
      "162/162 [==============================] - 0s 88us/sample - loss: 0.5970 - accuracy: 0.6914\n",
      "Epoch 22/100\n",
      "162/162 [==============================] - 0s 169us/sample - loss: 0.5994 - accuracy: 0.6975\n",
      "Epoch 23/100\n",
      "162/162 [==============================] - 0s 85us/sample - loss: 0.5702 - accuracy: 0.7037\n",
      "Epoch 24/100\n",
      "162/162 [==============================] - 0s 51us/sample - loss: 0.5574 - accuracy: 0.7222\n",
      "Epoch 25/100\n",
      "162/162 [==============================] - 0s 45us/sample - loss: 0.5717 - accuracy: 0.7160\n",
      "Epoch 26/100\n",
      "162/162 [==============================] - 0s 50us/sample - loss: 0.5756 - accuracy: 0.7222\n",
      "Epoch 27/100\n",
      "162/162 [==============================] - 0s 68us/sample - loss: 0.5497 - accuracy: 0.7160\n",
      "Epoch 28/100\n",
      "162/162 [==============================] - 0s 103us/sample - loss: 0.5451 - accuracy: 0.7407\n",
      "Epoch 29/100\n",
      "162/162 [==============================] - 0s 65us/sample - loss: 0.5397 - accuracy: 0.7531\n",
      "Epoch 30/100\n",
      "162/162 [==============================] - 0s 69us/sample - loss: 0.5494 - accuracy: 0.6975\n",
      "Epoch 31/100\n",
      "162/162 [==============================] - 0s 52us/sample - loss: 0.5437 - accuracy: 0.7346\n",
      "Epoch 32/100\n",
      "162/162 [==============================] - 0s 45us/sample - loss: 0.5153 - accuracy: 0.7840\n",
      "Epoch 33/100\n",
      "162/162 [==============================] - 0s 73us/sample - loss: 0.5291 - accuracy: 0.7593\n",
      "Epoch 34/100\n",
      "162/162 [==============================] - 0s 66us/sample - loss: 0.5389 - accuracy: 0.7593\n",
      "Epoch 35/100\n",
      "162/162 [==============================] - 0s 61us/sample - loss: 0.5105 - accuracy: 0.7593\n",
      "Epoch 36/100\n",
      "162/162 [==============================] - 0s 57us/sample - loss: 0.5201 - accuracy: 0.7778\n",
      "Epoch 37/100\n",
      "162/162 [==============================] - 0s 46us/sample - loss: 0.5213 - accuracy: 0.7346\n",
      "Epoch 38/100\n",
      "162/162 [==============================] - 0s 51us/sample - loss: 0.4954 - accuracy: 0.7840\n",
      "Epoch 39/100\n",
      "162/162 [==============================] - 0s 50us/sample - loss: 0.4837 - accuracy: 0.8086\n",
      "Epoch 40/100\n",
      "162/162 [==============================] - 0s 58us/sample - loss: 0.5261 - accuracy: 0.7654\n",
      "Epoch 41/100\n",
      "162/162 [==============================] - 0s 64us/sample - loss: 0.5146 - accuracy: 0.7716\n",
      "Epoch 42/100\n",
      "162/162 [==============================] - 0s 47us/sample - loss: 0.4889 - accuracy: 0.8025\n",
      "Epoch 43/100\n",
      "162/162 [==============================] - 0s 56us/sample - loss: 0.4967 - accuracy: 0.7840\n",
      "Epoch 44/100\n",
      "162/162 [==============================] - 0s 75us/sample - loss: 0.5062 - accuracy: 0.7593\n",
      "Epoch 45/100\n",
      "162/162 [==============================] - 0s 108us/sample - loss: 0.4747 - accuracy: 0.8210\n",
      "Epoch 46/100\n",
      "162/162 [==============================] - 0s 92us/sample - loss: 0.4903 - accuracy: 0.7654\n",
      "Epoch 47/100\n",
      "162/162 [==============================] - 0s 78us/sample - loss: 0.4621 - accuracy: 0.7963\n",
      "Epoch 48/100\n",
      "162/162 [==============================] - 0s 174us/sample - loss: 0.4952 - accuracy: 0.7716\n",
      "Epoch 49/100\n",
      "162/162 [==============================] - 0s 97us/sample - loss: 0.4790 - accuracy: 0.7840\n",
      "Epoch 50/100\n",
      "162/162 [==============================] - 0s 69us/sample - loss: 0.4814 - accuracy: 0.7901\n",
      "Epoch 51/100\n",
      "162/162 [==============================] - 0s 75us/sample - loss: 0.4698 - accuracy: 0.8025\n",
      "Epoch 52/100\n",
      "162/162 [==============================] - 0s 81us/sample - loss: 0.4576 - accuracy: 0.8086\n",
      "Epoch 53/100\n",
      "162/162 [==============================] - 0s 111us/sample - loss: 0.4929 - accuracy: 0.7778\n",
      "Epoch 54/100\n",
      "162/162 [==============================] - 0s 169us/sample - loss: 0.4763 - accuracy: 0.8025\n",
      "Epoch 55/100\n",
      "162/162 [==============================] - 0s 108us/sample - loss: 0.4661 - accuracy: 0.8210\n",
      "Epoch 56/100\n",
      "162/162 [==============================] - 0s 181us/sample - loss: 0.4453 - accuracy: 0.7963\n",
      "Epoch 57/100\n",
      "162/162 [==============================] - 0s 97us/sample - loss: 0.4472 - accuracy: 0.8148\n",
      "Epoch 58/100\n",
      "162/162 [==============================] - 0s 92us/sample - loss: 0.4160 - accuracy: 0.8333\n",
      "Epoch 59/100\n",
      "162/162 [==============================] - 0s 100us/sample - loss: 0.4523 - accuracy: 0.7778\n",
      "Epoch 60/100\n",
      "162/162 [==============================] - 0s 86us/sample - loss: 0.4477 - accuracy: 0.7963\n",
      "Epoch 61/100\n",
      "162/162 [==============================] - 0s 134us/sample - loss: 0.4521 - accuracy: 0.7963\n",
      "Epoch 62/100\n",
      "162/162 [==============================] - 0s 97us/sample - loss: 0.4222 - accuracy: 0.8333\n",
      "Epoch 63/100\n",
      "162/162 [==============================] - 0s 94us/sample - loss: 0.4305 - accuracy: 0.8210\n",
      "Epoch 64/100\n",
      "162/162 [==============================] - 0s 92us/sample - loss: 0.4129 - accuracy: 0.8210\n",
      "Epoch 65/100\n",
      "162/162 [==============================] - 0s 181us/sample - loss: 0.4268 - accuracy: 0.8333\n",
      "Epoch 66/100\n",
      "162/162 [==============================] - 0s 83us/sample - loss: 0.4247 - accuracy: 0.8580\n",
      "Epoch 67/100\n",
      "162/162 [==============================] - 0s 93us/sample - loss: 0.4151 - accuracy: 0.8272\n",
      "Epoch 68/100\n",
      "162/162 [==============================] - 0s 95us/sample - loss: 0.4235 - accuracy: 0.8333\n",
      "Epoch 69/100\n",
      "162/162 [==============================] - 0s 94us/sample - loss: 0.4275 - accuracy: 0.8210\n",
      "Epoch 70/100\n",
      "162/162 [==============================] - 0s 91us/sample - loss: 0.4222 - accuracy: 0.7778\n",
      "Epoch 71/100\n",
      "162/162 [==============================] - 0s 68us/sample - loss: 0.4110 - accuracy: 0.8333\n",
      "Epoch 72/100\n",
      "162/162 [==============================] - 0s 105us/sample - loss: 0.4371 - accuracy: 0.7901\n",
      "Epoch 73/100\n",
      "162/162 [==============================] - 0s 84us/sample - loss: 0.4117 - accuracy: 0.8025\n",
      "Epoch 74/100\n",
      "162/162 [==============================] - 0s 98us/sample - loss: 0.4349 - accuracy: 0.7901\n",
      "Epoch 75/100\n",
      "162/162 [==============================] - 0s 147us/sample - loss: 0.4144 - accuracy: 0.8210\n",
      "Epoch 76/100\n",
      "162/162 [==============================] - 0s 104us/sample - loss: 0.4099 - accuracy: 0.8272\n",
      "Epoch 77/100\n",
      "162/162 [==============================] - 0s 155us/sample - loss: 0.4224 - accuracy: 0.8210\n",
      "Epoch 78/100\n",
      "162/162 [==============================] - 0s 72us/sample - loss: 0.4033 - accuracy: 0.8395\n",
      "Epoch 79/100\n",
      "162/162 [==============================] - 0s 60us/sample - loss: 0.4003 - accuracy: 0.8210\n",
      "Epoch 80/100\n",
      "162/162 [==============================] - 0s 64us/sample - loss: 0.4048 - accuracy: 0.8210\n",
      "Epoch 81/100\n",
      "162/162 [==============================] - 0s 40us/sample - loss: 0.3984 - accuracy: 0.8210\n",
      "Epoch 82/100\n",
      "162/162 [==============================] - 0s 42us/sample - loss: 0.4047 - accuracy: 0.8272\n",
      "Epoch 83/100\n",
      "162/162 [==============================] - 0s 45us/sample - loss: 0.3944 - accuracy: 0.8272\n",
      "Epoch 84/100\n",
      "162/162 [==============================] - 0s 47us/sample - loss: 0.3980 - accuracy: 0.8395\n",
      "Epoch 85/100\n",
      "162/162 [==============================] - 0s 57us/sample - loss: 0.3982 - accuracy: 0.8148\n",
      "Epoch 86/100\n",
      "162/162 [==============================] - 0s 138us/sample - loss: 0.4025 - accuracy: 0.8333\n",
      "Epoch 87/100\n",
      "162/162 [==============================] - ETA: 0s - loss: 0.3945 - accuracy: 0.85 - 0s 54us/sample - loss: 0.3932 - accuracy: 0.8457\n",
      "Epoch 88/100\n",
      "162/162 [==============================] - 0s 85us/sample - loss: 0.4014 - accuracy: 0.8210\n",
      "Epoch 89/100\n",
      "162/162 [==============================] - 0s 63us/sample - loss: 0.3992 - accuracy: 0.8519\n",
      "Epoch 90/100\n",
      "162/162 [==============================] - 0s 53us/sample - loss: 0.3945 - accuracy: 0.8210\n",
      "Epoch 91/100\n",
      "162/162 [==============================] - 0s 47us/sample - loss: 0.3809 - accuracy: 0.8580\n",
      "Epoch 92/100\n",
      "162/162 [==============================] - 0s 50us/sample - loss: 0.3778 - accuracy: 0.8395\n",
      "Epoch 93/100\n",
      "162/162 [==============================] - 0s 72us/sample - loss: 0.4082 - accuracy: 0.8210\n",
      "Epoch 94/100\n",
      "162/162 [==============================] - 0s 55us/sample - loss: 0.3978 - accuracy: 0.8086\n",
      "Epoch 95/100\n",
      "162/162 [==============================] - 0s 63us/sample - loss: 0.3856 - accuracy: 0.8519\n",
      "Epoch 96/100\n",
      "162/162 [==============================] - 0s 80us/sample - loss: 0.3696 - accuracy: 0.8580\n",
      "Epoch 97/100\n",
      "162/162 [==============================] - 0s 42us/sample - loss: 0.3882 - accuracy: 0.8272\n",
      "Epoch 98/100\n",
      "162/162 [==============================] - 0s 52us/sample - loss: 0.3896 - accuracy: 0.8457\n",
      "Epoch 99/100\n",
      "162/162 [==============================] - 0s 52us/sample - loss: 0.3755 - accuracy: 0.8642\n",
      "Epoch 100/100\n",
      "162/162 [==============================] - 0s 67us/sample - loss: 0.3781 - accuracy: 0.8457\n",
      "80/80 [==============================] - 0s 3ms/sample - loss: 0.3734 - accuracy: 0.8250\n",
      "Train on 161 samples\n",
      "Epoch 1/100\n",
      "161/161 [==============================] - 0s 3ms/sample - loss: 0.8413 - accuracy: 0.4720\n",
      "Epoch 2/100\n",
      "161/161 [==============================] - 0s 216us/sample - loss: 0.8080 - accuracy: 0.5093\n",
      "Epoch 3/100\n",
      "161/161 [==============================] - 0s 218us/sample - loss: 0.7443 - accuracy: 0.5714\n",
      "Epoch 4/100\n",
      "161/161 [==============================] - 0s 222us/sample - loss: 0.7242 - accuracy: 0.5466\n",
      "Epoch 5/100\n",
      "161/161 [==============================] - 0s 219us/sample - loss: 0.7263 - accuracy: 0.5404\n",
      "Epoch 6/100\n",
      "161/161 [==============================] - 0s 228us/sample - loss: 0.6909 - accuracy: 0.5652\n",
      "Epoch 7/100\n",
      "161/161 [==============================] - 0s 221us/sample - loss: 0.6396 - accuracy: 0.6770\n",
      "Epoch 8/100\n",
      "161/161 [==============================] - 0s 229us/sample - loss: 0.6483 - accuracy: 0.6149\n",
      "Epoch 9/100\n",
      "161/161 [==============================] - 0s 233us/sample - loss: 0.6378 - accuracy: 0.6398\n",
      "Epoch 10/100\n",
      "161/161 [==============================] - 0s 244us/sample - loss: 0.5988 - accuracy: 0.7143\n",
      "Epoch 11/100\n",
      "161/161 [==============================] - 0s 235us/sample - loss: 0.5964 - accuracy: 0.6646\n",
      "Epoch 12/100\n",
      "161/161 [==============================] - 0s 244us/sample - loss: 0.5747 - accuracy: 0.7267\n",
      "Epoch 13/100\n",
      "161/161 [==============================] - 0s 283us/sample - loss: 0.5695 - accuracy: 0.7019\n",
      "Epoch 14/100\n",
      "161/161 [==============================] - 0s 261us/sample - loss: 0.5644 - accuracy: 0.7578\n",
      "Epoch 15/100\n",
      "161/161 [==============================] - 0s 245us/sample - loss: 0.5605 - accuracy: 0.7267\n",
      "Epoch 16/100\n",
      "161/161 [==============================] - 0s 258us/sample - loss: 0.5677 - accuracy: 0.7019\n",
      "Epoch 17/100\n",
      "161/161 [==============================] - 0s 314us/sample - loss: 0.5340 - accuracy: 0.7764\n",
      "Epoch 18/100\n",
      "161/161 [==============================] - 0s 273us/sample - loss: 0.5154 - accuracy: 0.7640\n",
      "Epoch 19/100\n",
      "161/161 [==============================] - 0s 238us/sample - loss: 0.5332 - accuracy: 0.7640\n",
      "Epoch 20/100\n",
      "161/161 [==============================] - 0s 226us/sample - loss: 0.5422 - accuracy: 0.7578\n",
      "Epoch 21/100\n",
      "161/161 [==============================] - 0s 272us/sample - loss: 0.5250 - accuracy: 0.7826\n",
      "Epoch 22/100\n",
      "161/161 [==============================] - 0s 243us/sample - loss: 0.4965 - accuracy: 0.7826\n",
      "Epoch 23/100\n",
      "161/161 [==============================] - 0s 230us/sample - loss: 0.5002 - accuracy: 0.7826\n",
      "Epoch 24/100\n",
      "161/161 [==============================] - 0s 214us/sample - loss: 0.4683 - accuracy: 0.8012\n",
      "Epoch 25/100\n",
      "161/161 [==============================] - 0s 216us/sample - loss: 0.4742 - accuracy: 0.8075\n",
      "Epoch 26/100\n",
      "161/161 [==============================] - 0s 242us/sample - loss: 0.4755 - accuracy: 0.8323\n",
      "Epoch 27/100\n",
      "161/161 [==============================] - 0s 206us/sample - loss: 0.4764 - accuracy: 0.8075\n",
      "Epoch 28/100\n",
      "161/161 [==============================] - 0s 216us/sample - loss: 0.4698 - accuracy: 0.8075\n",
      "Epoch 29/100\n",
      "161/161 [==============================] - 0s 210us/sample - loss: 0.4627 - accuracy: 0.8323\n",
      "Epoch 30/100\n",
      "161/161 [==============================] - 0s 216us/sample - loss: 0.4532 - accuracy: 0.8075\n",
      "Epoch 31/100\n",
      "161/161 [==============================] - 0s 227us/sample - loss: 0.4519 - accuracy: 0.8137\n",
      "Epoch 32/100\n",
      "161/161 [==============================] - 0s 222us/sample - loss: 0.4571 - accuracy: 0.8012\n",
      "Epoch 33/100\n",
      "161/161 [==============================] - 0s 225us/sample - loss: 0.4605 - accuracy: 0.8075\n",
      "Epoch 34/100\n",
      "161/161 [==============================] - 0s 223us/sample - loss: 0.4410 - accuracy: 0.8012\n",
      "Epoch 35/100\n",
      "161/161 [==============================] - 0s 218us/sample - loss: 0.4577 - accuracy: 0.7888\n",
      "Epoch 36/100\n",
      "161/161 [==============================] - 0s 217us/sample - loss: 0.4378 - accuracy: 0.8261\n",
      "Epoch 37/100\n",
      "161/161 [==============================] - 0s 221us/sample - loss: 0.4578 - accuracy: 0.7950\n",
      "Epoch 38/100\n",
      "161/161 [==============================] - 0s 223us/sample - loss: 0.4453 - accuracy: 0.7888\n",
      "Epoch 39/100\n",
      "161/161 [==============================] - 0s 261us/sample - loss: 0.4237 - accuracy: 0.8012\n",
      "Epoch 40/100\n",
      "161/161 [==============================] - 0s 444us/sample - loss: 0.4130 - accuracy: 0.8261\n",
      "Epoch 41/100\n",
      "161/161 [==============================] - 0s 221us/sample - loss: 0.4293 - accuracy: 0.8509\n",
      "Epoch 42/100\n",
      "161/161 [==============================] - 0s 213us/sample - loss: 0.4107 - accuracy: 0.8385\n",
      "Epoch 43/100\n",
      "161/161 [==============================] - 0s 215us/sample - loss: 0.4207 - accuracy: 0.8261\n",
      "Epoch 44/100\n",
      "161/161 [==============================] - 0s 213us/sample - loss: 0.4111 - accuracy: 0.8447\n",
      "Epoch 45/100\n",
      "161/161 [==============================] - 0s 217us/sample - loss: 0.4124 - accuracy: 0.8261\n",
      "Epoch 46/100\n",
      "161/161 [==============================] - 0s 214us/sample - loss: 0.4496 - accuracy: 0.7702\n",
      "Epoch 47/100\n",
      "161/161 [==============================] - 0s 214us/sample - loss: 0.4151 - accuracy: 0.8261\n",
      "Epoch 48/100\n",
      "161/161 [==============================] - 0s 219us/sample - loss: 0.4080 - accuracy: 0.8323\n",
      "Epoch 49/100\n",
      "161/161 [==============================] - 0s 220us/sample - loss: 0.3984 - accuracy: 0.8571\n",
      "Epoch 50/100\n",
      "161/161 [==============================] - 0s 214us/sample - loss: 0.4158 - accuracy: 0.8199\n",
      "Epoch 51/100\n",
      "161/161 [==============================] - 0s 219us/sample - loss: 0.4239 - accuracy: 0.7888\n",
      "Epoch 52/100\n",
      "161/161 [==============================] - 0s 238us/sample - loss: 0.3863 - accuracy: 0.8323\n",
      "Epoch 53/100\n",
      "161/161 [==============================] - 0s 217us/sample - loss: 0.3709 - accuracy: 0.8634\n",
      "Epoch 54/100\n",
      "161/161 [==============================] - 0s 221us/sample - loss: 0.4021 - accuracy: 0.8385\n",
      "Epoch 55/100\n",
      "161/161 [==============================] - 0s 232us/sample - loss: 0.3962 - accuracy: 0.8571\n",
      "Epoch 56/100\n",
      "161/161 [==============================] - 0s 231us/sample - loss: 0.4005 - accuracy: 0.8571\n",
      "Epoch 57/100\n",
      "161/161 [==============================] - 0s 327us/sample - loss: 0.4058 - accuracy: 0.8447\n",
      "Epoch 58/100\n",
      "161/161 [==============================] - 0s 266us/sample - loss: 0.3867 - accuracy: 0.8509\n",
      "Epoch 59/100\n",
      "161/161 [==============================] - 0s 246us/sample - loss: 0.3923 - accuracy: 0.8323\n",
      "Epoch 60/100\n",
      "161/161 [==============================] - 0s 208us/sample - loss: 0.3859 - accuracy: 0.8137\n",
      "Epoch 61/100\n",
      "161/161 [==============================] - 0s 211us/sample - loss: 0.3893 - accuracy: 0.8199\n",
      "Epoch 62/100\n",
      "161/161 [==============================] - 0s 255us/sample - loss: 0.4269 - accuracy: 0.8385\n",
      "Epoch 63/100\n",
      "161/161 [==============================] - 0s 220us/sample - loss: 0.4140 - accuracy: 0.8137\n",
      "Epoch 64/100\n",
      "161/161 [==============================] - 0s 228us/sample - loss: 0.3742 - accuracy: 0.8385\n",
      "Epoch 65/100\n",
      "161/161 [==============================] - 0s 231us/sample - loss: 0.3700 - accuracy: 0.8385\n",
      "Epoch 66/100\n",
      "161/161 [==============================] - 0s 214us/sample - loss: 0.4100 - accuracy: 0.8261\n",
      "Epoch 67/100\n",
      "161/161 [==============================] - 0s 208us/sample - loss: 0.3844 - accuracy: 0.8385\n",
      "Epoch 68/100\n",
      "161/161 [==============================] - 0s 205us/sample - loss: 0.4097 - accuracy: 0.8323\n",
      "Epoch 69/100\n",
      "161/161 [==============================] - 0s 211us/sample - loss: 0.3841 - accuracy: 0.8385\n",
      "Epoch 70/100\n",
      "161/161 [==============================] - 0s 203us/sample - loss: 0.3689 - accuracy: 0.8634\n",
      "Epoch 71/100\n",
      "161/161 [==============================] - 0s 213us/sample - loss: 0.3723 - accuracy: 0.8447\n",
      "Epoch 72/100\n",
      "161/161 [==============================] - 0s 252us/sample - loss: 0.3810 - accuracy: 0.8509\n",
      "Epoch 73/100\n",
      "161/161 [==============================] - 0s 230us/sample - loss: 0.3909 - accuracy: 0.8199\n",
      "Epoch 74/100\n",
      "161/161 [==============================] - 0s 216us/sample - loss: 0.3619 - accuracy: 0.8509\n",
      "Epoch 75/100\n",
      "161/161 [==============================] - 0s 222us/sample - loss: 0.3794 - accuracy: 0.8447\n",
      "Epoch 76/100\n",
      "161/161 [==============================] - 0s 221us/sample - loss: 0.3735 - accuracy: 0.8634\n",
      "Epoch 77/100\n",
      "161/161 [==============================] - 0s 215us/sample - loss: 0.3592 - accuracy: 0.8385\n",
      "Epoch 78/100\n",
      "161/161 [==============================] - 0s 238us/sample - loss: 0.3772 - accuracy: 0.8323\n",
      "Epoch 79/100\n",
      "161/161 [==============================] - 0s 219us/sample - loss: 0.3773 - accuracy: 0.8571\n",
      "Epoch 80/100\n",
      "161/161 [==============================] - 0s 216us/sample - loss: 0.3800 - accuracy: 0.8509\n",
      "Epoch 81/100\n",
      "161/161 [==============================] - 0s 247us/sample - loss: 0.3740 - accuracy: 0.8385\n",
      "Epoch 82/100\n",
      "161/161 [==============================] - 0s 213us/sample - loss: 0.3892 - accuracy: 0.8571\n",
      "Epoch 83/100\n",
      "161/161 [==============================] - 0s 200us/sample - loss: 0.3769 - accuracy: 0.8509\n",
      "Epoch 84/100\n",
      "161/161 [==============================] - 0s 218us/sample - loss: 0.3762 - accuracy: 0.8323\n",
      "Epoch 85/100\n",
      "161/161 [==============================] - 0s 202us/sample - loss: 0.3708 - accuracy: 0.8447\n",
      "Epoch 86/100\n",
      "161/161 [==============================] - 0s 208us/sample - loss: 0.3709 - accuracy: 0.8509\n",
      "Epoch 87/100\n",
      "161/161 [==============================] - 0s 205us/sample - loss: 0.3537 - accuracy: 0.8634\n",
      "Epoch 88/100\n",
      "161/161 [==============================] - 0s 204us/sample - loss: 0.3543 - accuracy: 0.8385\n",
      "Epoch 89/100\n",
      "161/161 [==============================] - 0s 202us/sample - loss: 0.3419 - accuracy: 0.8634\n",
      "Epoch 90/100\n",
      "161/161 [==============================] - 0s 210us/sample - loss: 0.3661 - accuracy: 0.8696\n",
      "Epoch 91/100\n",
      "161/161 [==============================] - 0s 223us/sample - loss: 0.3666 - accuracy: 0.8447\n",
      "Epoch 92/100\n",
      "161/161 [==============================] - 0s 234us/sample - loss: 0.3695 - accuracy: 0.8385\n",
      "Epoch 93/100\n",
      "161/161 [==============================] - 0s 235us/sample - loss: 0.3500 - accuracy: 0.8385\n",
      "Epoch 94/100\n",
      "161/161 [==============================] - 0s 215us/sample - loss: 0.3723 - accuracy: 0.8323\n",
      "Epoch 95/100\n",
      "161/161 [==============================] - 0s 220us/sample - loss: 0.3423 - accuracy: 0.8758\n",
      "Epoch 96/100\n",
      "161/161 [==============================] - 0s 233us/sample - loss: 0.3873 - accuracy: 0.8696\n",
      "Epoch 97/100\n",
      "161/161 [==============================] - 0s 273us/sample - loss: 0.3764 - accuracy: 0.8758\n",
      "Epoch 98/100\n",
      "161/161 [==============================] - 0s 249us/sample - loss: 0.3592 - accuracy: 0.8323\n",
      "Epoch 99/100\n",
      "161/161 [==============================] - 0s 263us/sample - loss: 0.3608 - accuracy: 0.8634\n",
      "Epoch 100/100\n",
      "161/161 [==============================] - 0s 230us/sample - loss: 0.3639 - accuracy: 0.8509\n",
      "81/81 [==============================] - 0s 1ms/sample - loss: 0.3575 - accuracy: 0.8642\n",
      "Train on 161 samples\n",
      "Epoch 1/100\n",
      "161/161 [==============================] - 1s 3ms/sample - loss: 0.7800 - accuracy: 0.4658\n",
      "Epoch 2/100\n",
      "161/161 [==============================] - 0s 241us/sample - loss: 0.7475 - accuracy: 0.4658\n",
      "Epoch 3/100\n",
      "161/161 [==============================] - 0s 230us/sample - loss: 0.7362 - accuracy: 0.4658\n",
      "Epoch 4/100\n",
      "161/161 [==============================] - 0s 234us/sample - loss: 0.7176 - accuracy: 0.4658\n",
      "Epoch 5/100\n",
      "161/161 [==============================] - 0s 237us/sample - loss: 0.7066 - accuracy: 0.4658\n",
      "Epoch 6/100\n",
      "161/161 [==============================] - 0s 239us/sample - loss: 0.6861 - accuracy: 0.4783\n",
      "Epoch 7/100\n",
      "161/161 [==============================] - 0s 271us/sample - loss: 0.6817 - accuracy: 0.4720\n",
      "Epoch 8/100\n",
      "161/161 [==============================] - 0s 265us/sample - loss: 0.6651 - accuracy: 0.5031\n",
      "Epoch 9/100\n",
      "161/161 [==============================] - 0s 302us/sample - loss: 0.6503 - accuracy: 0.5776\n",
      "Epoch 10/100\n",
      "161/161 [==============================] - 0s 227us/sample - loss: 0.6522 - accuracy: 0.6211\n",
      "Epoch 11/100\n",
      "161/161 [==============================] - 0s 212us/sample - loss: 0.6410 - accuracy: 0.6522\n",
      "Epoch 12/100\n",
      "161/161 [==============================] - 0s 233us/sample - loss: 0.6291 - accuracy: 0.6770\n",
      "Epoch 13/100\n",
      "161/161 [==============================] - 0s 222us/sample - loss: 0.6338 - accuracy: 0.6770\n",
      "Epoch 14/100\n",
      "161/161 [==============================] - 0s 229us/sample - loss: 0.6211 - accuracy: 0.7081\n",
      "Epoch 15/100\n",
      "161/161 [==============================] - 0s 240us/sample - loss: 0.6075 - accuracy: 0.7391\n",
      "Epoch 16/100\n",
      "161/161 [==============================] - 0s 227us/sample - loss: 0.6045 - accuracy: 0.7391\n",
      "Epoch 17/100\n",
      "161/161 [==============================] - 0s 232us/sample - loss: 0.5911 - accuracy: 0.7578\n",
      "Epoch 18/100\n",
      "161/161 [==============================] - 0s 232us/sample - loss: 0.5906 - accuracy: 0.7391\n",
      "Epoch 19/100\n",
      "161/161 [==============================] - 0s 219us/sample - loss: 0.5825 - accuracy: 0.7702\n",
      "Epoch 20/100\n",
      "161/161 [==============================] - 0s 215us/sample - loss: 0.5726 - accuracy: 0.7950\n",
      "Epoch 21/100\n",
      "161/161 [==============================] - 0s 223us/sample - loss: 0.5564 - accuracy: 0.7950\n",
      "Epoch 22/100\n",
      "161/161 [==============================] - 0s 234us/sample - loss: 0.5521 - accuracy: 0.8199\n",
      "Epoch 23/100\n",
      "161/161 [==============================] - 0s 272us/sample - loss: 0.5429 - accuracy: 0.8261\n",
      "Epoch 24/100\n",
      "161/161 [==============================] - 0s 677us/sample - loss: 0.5397 - accuracy: 0.7950\n",
      "Epoch 25/100\n",
      "161/161 [==============================] - 0s 543us/sample - loss: 0.5422 - accuracy: 0.7640\n",
      "Epoch 26/100\n",
      "161/161 [==============================] - 0s 446us/sample - loss: 0.5202 - accuracy: 0.8261\n",
      "Epoch 27/100\n",
      "161/161 [==============================] - 0s 585us/sample - loss: 0.5187 - accuracy: 0.8261\n",
      "Epoch 28/100\n",
      "161/161 [==============================] - 0s 349us/sample - loss: 0.5108 - accuracy: 0.8261\n",
      "Epoch 29/100\n",
      "161/161 [==============================] - 0s 412us/sample - loss: 0.5014 - accuracy: 0.8385\n",
      "Epoch 30/100\n",
      "161/161 [==============================] - 0s 259us/sample - loss: 0.4984 - accuracy: 0.8385\n",
      "Epoch 31/100\n",
      "161/161 [==============================] - 0s 297us/sample - loss: 0.4799 - accuracy: 0.8385\n",
      "Epoch 32/100\n",
      "161/161 [==============================] - 0s 535us/sample - loss: 0.4905 - accuracy: 0.8323\n",
      "Epoch 33/100\n",
      "161/161 [==============================] - 0s 542us/sample - loss: 0.4901 - accuracy: 0.8075\n",
      "Epoch 34/100\n",
      "161/161 [==============================] - 0s 231us/sample - loss: 0.4866 - accuracy: 0.8385\n",
      "Epoch 35/100\n",
      "161/161 [==============================] - 0s 214us/sample - loss: 0.4596 - accuracy: 0.8509\n",
      "Epoch 36/100\n",
      "161/161 [==============================] - 0s 212us/sample - loss: 0.4636 - accuracy: 0.8385\n",
      "Epoch 37/100\n",
      "161/161 [==============================] - 0s 223us/sample - loss: 0.4520 - accuracy: 0.8509\n",
      "Epoch 38/100\n",
      "161/161 [==============================] - 0s 222us/sample - loss: 0.4386 - accuracy: 0.8634\n",
      "Epoch 39/100\n",
      "161/161 [==============================] - 0s 210us/sample - loss: 0.4424 - accuracy: 0.8571\n",
      "Epoch 40/100\n",
      "161/161 [==============================] - 0s 213us/sample - loss: 0.4484 - accuracy: 0.8323\n",
      "Epoch 41/100\n",
      "161/161 [==============================] - 0s 428us/sample - loss: 0.4331 - accuracy: 0.8509\n",
      "Epoch 42/100\n",
      "161/161 [==============================] - 0s 745us/sample - loss: 0.4386 - accuracy: 0.8509\n",
      "Epoch 43/100\n",
      "161/161 [==============================] - 0s 490us/sample - loss: 0.4313 - accuracy: 0.8509\n",
      "Epoch 44/100\n",
      "161/161 [==============================] - 0s 271us/sample - loss: 0.4088 - accuracy: 0.8696\n",
      "Epoch 45/100\n",
      "161/161 [==============================] - 0s 215us/sample - loss: 0.4177 - accuracy: 0.8571\n",
      "Epoch 46/100\n",
      "161/161 [==============================] - 0s 212us/sample - loss: 0.4097 - accuracy: 0.8447\n",
      "Epoch 47/100\n",
      "161/161 [==============================] - 0s 201us/sample - loss: 0.4285 - accuracy: 0.8571\n",
      "Epoch 48/100\n",
      "161/161 [==============================] - 0s 207us/sample - loss: 0.3993 - accuracy: 0.8571\n",
      "Epoch 49/100\n",
      "161/161 [==============================] - 0s 216us/sample - loss: 0.3926 - accuracy: 0.8758\n",
      "Epoch 50/100\n",
      "161/161 [==============================] - 0s 241us/sample - loss: 0.4081 - accuracy: 0.8571\n",
      "Epoch 51/100\n",
      "161/161 [==============================] - 0s 224us/sample - loss: 0.4092 - accuracy: 0.8571\n",
      "Epoch 52/100\n",
      "161/161 [==============================] - 0s 217us/sample - loss: 0.3973 - accuracy: 0.8820\n",
      "Epoch 53/100\n",
      "161/161 [==============================] - 0s 218us/sample - loss: 0.3879 - accuracy: 0.8820\n",
      "Epoch 54/100\n",
      "161/161 [==============================] - 0s 217us/sample - loss: 0.3795 - accuracy: 0.8758\n",
      "Epoch 55/100\n",
      "161/161 [==============================] - 0s 273us/sample - loss: 0.3739 - accuracy: 0.8634\n",
      "Epoch 56/100\n",
      "161/161 [==============================] - 0s 296us/sample - loss: 0.3910 - accuracy: 0.8758\n",
      "Epoch 57/100\n",
      "161/161 [==============================] - 0s 241us/sample - loss: 0.3797 - accuracy: 0.8820\n",
      "Epoch 58/100\n",
      "161/161 [==============================] - 0s 226us/sample - loss: 0.3832 - accuracy: 0.8758\n",
      "Epoch 59/100\n",
      "161/161 [==============================] - 0s 240us/sample - loss: 0.3756 - accuracy: 0.8944\n",
      "Epoch 60/100\n",
      "161/161 [==============================] - 0s 487us/sample - loss: 0.3781 - accuracy: 0.8696\n",
      "Epoch 61/100\n",
      "161/161 [==============================] - 0s 543us/sample - loss: 0.3639 - accuracy: 0.8634\n",
      "Epoch 62/100\n",
      "161/161 [==============================] - 0s 487us/sample - loss: 0.3643 - accuracy: 0.8696\n",
      "Epoch 63/100\n",
      "161/161 [==============================] - 0s 495us/sample - loss: 0.3720 - accuracy: 0.8820\n",
      "Epoch 64/100\n",
      "161/161 [==============================] - 0s 421us/sample - loss: 0.3666 - accuracy: 0.8634\n",
      "Epoch 65/100\n",
      "161/161 [==============================] - 0s 448us/sample - loss: 0.3712 - accuracy: 0.8696\n",
      "Epoch 66/100\n",
      "161/161 [==============================] - 0s 328us/sample - loss: 0.3494 - accuracy: 0.8696\n",
      "Epoch 67/100\n",
      "161/161 [==============================] - 0s 218us/sample - loss: 0.3867 - accuracy: 0.8571\n",
      "Epoch 68/100\n",
      "161/161 [==============================] - 0s 386us/sample - loss: 0.3460 - accuracy: 0.9006\n",
      "Epoch 69/100\n",
      "161/161 [==============================] - 0s 536us/sample - loss: 0.3470 - accuracy: 0.8758\n",
      "Epoch 70/100\n",
      "161/161 [==============================] - 0s 514us/sample - loss: 0.3456 - accuracy: 0.9006\n",
      "Epoch 71/100\n",
      "161/161 [==============================] - 0s 433us/sample - loss: 0.3522 - accuracy: 0.8820\n",
      "Epoch 72/100\n",
      "161/161 [==============================] - 0s 400us/sample - loss: 0.3676 - accuracy: 0.8696\n",
      "Epoch 73/100\n",
      "161/161 [==============================] - 0s 719us/sample - loss: 0.3533 - accuracy: 0.8820\n",
      "Epoch 74/100\n",
      "161/161 [==============================] - 0s 528us/sample - loss: 0.3458 - accuracy: 0.8944\n",
      "Epoch 75/100\n",
      "161/161 [==============================] - 0s 410us/sample - loss: 0.3511 - accuracy: 0.8696\n",
      "Epoch 76/100\n",
      "161/161 [==============================] - 0s 524us/sample - loss: 0.3308 - accuracy: 0.8944\n",
      "Epoch 77/100\n",
      "161/161 [==============================] - 0s 459us/sample - loss: 0.3428 - accuracy: 0.8944\n",
      "Epoch 78/100\n",
      "161/161 [==============================] - 0s 463us/sample - loss: 0.3210 - accuracy: 0.8696\n",
      "Epoch 79/100\n",
      "161/161 [==============================] - 0s 420us/sample - loss: 0.3379 - accuracy: 0.8882\n",
      "Epoch 80/100\n",
      "161/161 [==============================] - 0s 514us/sample - loss: 0.3245 - accuracy: 0.8882\n",
      "Epoch 81/100\n",
      "161/161 [==============================] - 0s 452us/sample - loss: 0.3233 - accuracy: 0.8696\n",
      "Epoch 82/100\n",
      "161/161 [==============================] - 0s 417us/sample - loss: 0.3442 - accuracy: 0.8509\n",
      "Epoch 83/100\n",
      "161/161 [==============================] - 0s 248us/sample - loss: 0.3107 - accuracy: 0.8696\n",
      "Epoch 84/100\n",
      "161/161 [==============================] - 0s 405us/sample - loss: 0.3320 - accuracy: 0.8944\n",
      "Epoch 85/100\n",
      "161/161 [==============================] - 0s 495us/sample - loss: 0.3191 - accuracy: 0.8820\n",
      "Epoch 86/100\n",
      "161/161 [==============================] - 0s 556us/sample - loss: 0.3220 - accuracy: 0.8634\n",
      "Epoch 87/100\n",
      "161/161 [==============================] - 0s 450us/sample - loss: 0.3360 - accuracy: 0.8571\n",
      "Epoch 88/100\n",
      "161/161 [==============================] - 0s 252us/sample - loss: 0.3264 - accuracy: 0.8882\n",
      "Epoch 89/100\n",
      "161/161 [==============================] - 0s 410us/sample - loss: 0.3387 - accuracy: 0.8820\n",
      "Epoch 90/100\n",
      "161/161 [==============================] - 0s 240us/sample - loss: 0.2845 - accuracy: 0.9068\n",
      "Epoch 91/100\n",
      "161/161 [==============================] - 0s 218us/sample - loss: 0.3191 - accuracy: 0.8820\n",
      "Epoch 92/100\n",
      "161/161 [==============================] - 0s 213us/sample - loss: 0.3156 - accuracy: 0.8944\n",
      "Epoch 93/100\n",
      "161/161 [==============================] - 0s 215us/sample - loss: 0.3424 - accuracy: 0.9006\n",
      "Epoch 94/100\n",
      "161/161 [==============================] - 0s 202us/sample - loss: 0.3157 - accuracy: 0.8634\n",
      "Epoch 95/100\n",
      "161/161 [==============================] - 0s 201us/sample - loss: 0.3082 - accuracy: 0.9068\n",
      "Epoch 96/100\n",
      "161/161 [==============================] - 0s 214us/sample - loss: 0.3286 - accuracy: 0.8571\n",
      "Epoch 97/100\n",
      "161/161 [==============================] - 0s 200us/sample - loss: 0.2976 - accuracy: 0.9006\n",
      "Epoch 98/100\n",
      "161/161 [==============================] - 0s 205us/sample - loss: 0.2933 - accuracy: 0.9006\n",
      "Epoch 99/100\n",
      "161/161 [==============================] - 0s 201us/sample - loss: 0.3038 - accuracy: 0.9193\n",
      "Epoch 100/100\n",
      "161/161 [==============================] - 0s 234us/sample - loss: 0.3299 - accuracy: 0.8820\n",
      "81/81 [==============================] - 0s 1ms/sample - loss: 0.4350 - accuracy: 0.8395\n",
      "Train on 162 samples\n",
      "Epoch 1/100\n",
      "162/162 [==============================] - 1s 5ms/sample - loss: 0.6822 - accuracy: 0.6111\n",
      "Epoch 2/100\n",
      "162/162 [==============================] - 0s 276us/sample - loss: 0.6635 - accuracy: 0.6049\n",
      "Epoch 3/100\n",
      "162/162 [==============================] - 0s 256us/sample - loss: 0.6393 - accuracy: 0.6173\n",
      "Epoch 4/100\n",
      "162/162 [==============================] - 0s 430us/sample - loss: 0.6145 - accuracy: 0.7284\n",
      "Epoch 5/100\n",
      "162/162 [==============================] - 0s 292us/sample - loss: 0.6186 - accuracy: 0.6728\n",
      "Epoch 6/100\n",
      "162/162 [==============================] - 0s 214us/sample - loss: 0.5580 - accuracy: 0.7654\n",
      "Epoch 7/100\n",
      "162/162 [==============================] - 0s 220us/sample - loss: 0.5557 - accuracy: 0.7778\n",
      "Epoch 8/100\n",
      "162/162 [==============================] - 0s 222us/sample - loss: 0.5577 - accuracy: 0.7284\n",
      "Epoch 9/100\n",
      "162/162 [==============================] - 0s 221us/sample - loss: 0.5324 - accuracy: 0.7716\n",
      "Epoch 10/100\n",
      "162/162 [==============================] - 0s 225us/sample - loss: 0.5528 - accuracy: 0.7346\n",
      "Epoch 11/100\n",
      "162/162 [==============================] - 0s 216us/sample - loss: 0.5205 - accuracy: 0.7778\n",
      "Epoch 12/100\n",
      "162/162 [==============================] - 0s 240us/sample - loss: 0.5206 - accuracy: 0.7778\n",
      "Epoch 13/100\n",
      "162/162 [==============================] - 0s 208us/sample - loss: 0.5028 - accuracy: 0.7716\n",
      "Epoch 14/100\n",
      "162/162 [==============================] - 0s 204us/sample - loss: 0.5114 - accuracy: 0.8025\n",
      "Epoch 15/100\n",
      "162/162 [==============================] - 0s 207us/sample - loss: 0.4674 - accuracy: 0.8272\n",
      "Epoch 16/100\n",
      "162/162 [==============================] - 0s 205us/sample - loss: 0.4807 - accuracy: 0.8272\n",
      "Epoch 17/100\n",
      "162/162 [==============================] - 0s 309us/sample - loss: 0.4638 - accuracy: 0.8333\n",
      "Epoch 18/100\n",
      "162/162 [==============================] - 0s 306us/sample - loss: 0.4728 - accuracy: 0.7963\n",
      "Epoch 19/100\n",
      "162/162 [==============================] - 0s 314us/sample - loss: 0.4612 - accuracy: 0.8148\n",
      "Epoch 20/100\n",
      "162/162 [==============================] - 0s 311us/sample - loss: 0.4608 - accuracy: 0.7716\n",
      "Epoch 21/100\n",
      "162/162 [==============================] - 0s 313us/sample - loss: 0.4629 - accuracy: 0.8210\n",
      "Epoch 22/100\n",
      "162/162 [==============================] - 0s 203us/sample - loss: 0.4500 - accuracy: 0.7593\n",
      "Epoch 23/100\n",
      "162/162 [==============================] - 0s 198us/sample - loss: 0.4256 - accuracy: 0.8210\n",
      "Epoch 24/100\n",
      "162/162 [==============================] - 0s 247us/sample - loss: 0.4773 - accuracy: 0.7531\n",
      "Epoch 25/100\n",
      "162/162 [==============================] - 0s 199us/sample - loss: 0.4501 - accuracy: 0.7901\n",
      "Epoch 26/100\n",
      "162/162 [==============================] - 0s 209us/sample - loss: 0.4545 - accuracy: 0.8086\n",
      "Epoch 27/100\n",
      "162/162 [==============================] - 0s 198us/sample - loss: 0.4323 - accuracy: 0.8333\n",
      "Epoch 28/100\n",
      "162/162 [==============================] - 0s 207us/sample - loss: 0.4311 - accuracy: 0.8272\n",
      "Epoch 29/100\n",
      "162/162 [==============================] - 0s 218us/sample - loss: 0.4202 - accuracy: 0.8210\n",
      "Epoch 30/100\n",
      "162/162 [==============================] - 0s 198us/sample - loss: 0.4125 - accuracy: 0.8457\n",
      "Epoch 31/100\n",
      "162/162 [==============================] - 0s 198us/sample - loss: 0.4048 - accuracy: 0.8457\n",
      "Epoch 32/100\n",
      "162/162 [==============================] - 0s 203us/sample - loss: 0.3949 - accuracy: 0.8395\n",
      "Epoch 33/100\n",
      "162/162 [==============================] - 0s 201us/sample - loss: 0.4176 - accuracy: 0.8272\n",
      "Epoch 34/100\n",
      "162/162 [==============================] - 0s 208us/sample - loss: 0.3889 - accuracy: 0.8457\n",
      "Epoch 35/100\n",
      "162/162 [==============================] - 0s 204us/sample - loss: 0.4118 - accuracy: 0.8519\n",
      "Epoch 36/100\n",
      "162/162 [==============================] - 0s 213us/sample - loss: 0.3873 - accuracy: 0.8519\n",
      "Epoch 37/100\n",
      "162/162 [==============================] - 0s 196us/sample - loss: 0.3931 - accuracy: 0.8457\n",
      "Epoch 38/100\n",
      "162/162 [==============================] - 0s 244us/sample - loss: 0.4069 - accuracy: 0.8395\n",
      "Epoch 39/100\n",
      "162/162 [==============================] - 0s 206us/sample - loss: 0.3879 - accuracy: 0.8333\n",
      "Epoch 40/100\n",
      "162/162 [==============================] - 0s 220us/sample - loss: 0.3903 - accuracy: 0.8580\n",
      "Epoch 41/100\n",
      "162/162 [==============================] - 0s 202us/sample - loss: 0.3916 - accuracy: 0.8580\n",
      "Epoch 42/100\n",
      "162/162 [==============================] - 0s 200us/sample - loss: 0.3881 - accuracy: 0.8457\n",
      "Epoch 43/100\n",
      "162/162 [==============================] - 0s 217us/sample - loss: 0.4024 - accuracy: 0.8333\n",
      "Epoch 44/100\n",
      "162/162 [==============================] - 0s 220us/sample - loss: 0.3789 - accuracy: 0.8580\n",
      "Epoch 45/100\n",
      "162/162 [==============================] - 0s 232us/sample - loss: 0.3926 - accuracy: 0.8519\n",
      "Epoch 46/100\n",
      "162/162 [==============================] - 0s 231us/sample - loss: 0.3742 - accuracy: 0.8765\n",
      "Epoch 47/100\n",
      "162/162 [==============================] - 0s 233us/sample - loss: 0.3954 - accuracy: 0.8395\n",
      "Epoch 48/100\n",
      "162/162 [==============================] - 0s 247us/sample - loss: 0.3681 - accuracy: 0.8457\n",
      "Epoch 49/100\n",
      "162/162 [==============================] - 0s 394us/sample - loss: 0.3655 - accuracy: 0.8457\n",
      "Epoch 50/100\n",
      "162/162 [==============================] - 0s 436us/sample - loss: 0.3778 - accuracy: 0.8395\n",
      "Epoch 51/100\n",
      "162/162 [==============================] - 0s 600us/sample - loss: 0.3524 - accuracy: 0.8704\n",
      "Epoch 52/100\n",
      "162/162 [==============================] - 0s 500us/sample - loss: 0.3654 - accuracy: 0.8580\n",
      "Epoch 53/100\n",
      "162/162 [==============================] - 0s 431us/sample - loss: 0.3746 - accuracy: 0.8457\n",
      "Epoch 54/100\n",
      "162/162 [==============================] - 0s 339us/sample - loss: 0.3832 - accuracy: 0.8519\n",
      "Epoch 55/100\n",
      "162/162 [==============================] - 0s 391us/sample - loss: 0.3844 - accuracy: 0.8148\n",
      "Epoch 56/100\n",
      "162/162 [==============================] - 0s 435us/sample - loss: 0.3763 - accuracy: 0.8519\n",
      "Epoch 57/100\n",
      "162/162 [==============================] - 0s 688us/sample - loss: 0.3725 - accuracy: 0.8272\n",
      "Epoch 58/100\n",
      "162/162 [==============================] - 0s 934us/sample - loss: 0.4005 - accuracy: 0.8519\n",
      "Epoch 59/100\n",
      "162/162 [==============================] - 0s 668us/sample - loss: 0.3806 - accuracy: 0.8519\n",
      "Epoch 60/100\n",
      "162/162 [==============================] - 0s 424us/sample - loss: 0.3559 - accuracy: 0.8395\n",
      "Epoch 61/100\n",
      "162/162 [==============================] - 0s 327us/sample - loss: 0.3801 - accuracy: 0.8333\n",
      "Epoch 62/100\n",
      "162/162 [==============================] - 0s 278us/sample - loss: 0.3511 - accuracy: 0.8704\n",
      "Epoch 63/100\n",
      "162/162 [==============================] - 0s 242us/sample - loss: 0.3719 - accuracy: 0.8395\n",
      "Epoch 64/100\n",
      "162/162 [==============================] - 0s 224us/sample - loss: 0.3963 - accuracy: 0.8272\n",
      "Epoch 65/100\n",
      "162/162 [==============================] - 0s 213us/sample - loss: 0.3616 - accuracy: 0.8580\n",
      "Epoch 66/100\n",
      "162/162 [==============================] - 0s 236us/sample - loss: 0.3627 - accuracy: 0.8395\n",
      "Epoch 67/100\n",
      "162/162 [==============================] - 0s 218us/sample - loss: 0.3824 - accuracy: 0.8519\n",
      "Epoch 68/100\n",
      "162/162 [==============================] - 0s 201us/sample - loss: 0.3618 - accuracy: 0.8333\n",
      "Epoch 69/100\n",
      "162/162 [==============================] - 0s 211us/sample - loss: 0.3493 - accuracy: 0.8580\n",
      "Epoch 70/100\n",
      "162/162 [==============================] - 0s 211us/sample - loss: 0.3409 - accuracy: 0.8642\n",
      "Epoch 71/100\n",
      "162/162 [==============================] - 0s 212us/sample - loss: 0.3621 - accuracy: 0.8642\n",
      "Epoch 72/100\n",
      "162/162 [==============================] - 0s 221us/sample - loss: 0.3782 - accuracy: 0.8333\n",
      "Epoch 73/100\n",
      "162/162 [==============================] - 0s 349us/sample - loss: 0.3901 - accuracy: 0.8519\n",
      "Epoch 74/100\n",
      "162/162 [==============================] - 0s 228us/sample - loss: 0.3436 - accuracy: 0.8457\n",
      "Epoch 75/100\n",
      "162/162 [==============================] - 0s 221us/sample - loss: 0.3727 - accuracy: 0.8642\n",
      "Epoch 76/100\n",
      "162/162 [==============================] - 0s 208us/sample - loss: 0.3701 - accuracy: 0.8580\n",
      "Epoch 77/100\n",
      "162/162 [==============================] - 0s 209us/sample - loss: 0.3829 - accuracy: 0.8457\n",
      "Epoch 78/100\n",
      "162/162 [==============================] - 0s 212us/sample - loss: 0.3643 - accuracy: 0.8580\n",
      "Epoch 79/100\n",
      "162/162 [==============================] - 0s 209us/sample - loss: 0.3412 - accuracy: 0.8642\n",
      "Epoch 80/100\n",
      "162/162 [==============================] - 0s 230us/sample - loss: 0.3518 - accuracy: 0.8704\n",
      "Epoch 81/100\n",
      "162/162 [==============================] - 0s 222us/sample - loss: 0.3758 - accuracy: 0.8580\n",
      "Epoch 82/100\n",
      "162/162 [==============================] - 0s 211us/sample - loss: 0.3689 - accuracy: 0.8395\n",
      "Epoch 83/100\n",
      "162/162 [==============================] - 0s 226us/sample - loss: 0.3558 - accuracy: 0.8457\n",
      "Epoch 84/100\n",
      "162/162 [==============================] - 0s 220us/sample - loss: 0.3708 - accuracy: 0.8519\n",
      "Epoch 85/100\n",
      "162/162 [==============================] - 0s 386us/sample - loss: 0.3545 - accuracy: 0.8272\n",
      "Epoch 86/100\n",
      "162/162 [==============================] - 0s 662us/sample - loss: 0.3642 - accuracy: 0.8333\n",
      "Epoch 87/100\n",
      "162/162 [==============================] - 0s 378us/sample - loss: 0.3714 - accuracy: 0.8642\n",
      "Epoch 88/100\n",
      "162/162 [==============================] - 0s 339us/sample - loss: 0.3537 - accuracy: 0.8395\n",
      "Epoch 89/100\n",
      "162/162 [==============================] - 0s 267us/sample - loss: 0.3674 - accuracy: 0.8519\n",
      "Epoch 90/100\n",
      "162/162 [==============================] - 0s 248us/sample - loss: 0.3486 - accuracy: 0.8519\n",
      "Epoch 91/100\n",
      "162/162 [==============================] - 0s 224us/sample - loss: 0.3560 - accuracy: 0.8457\n",
      "Epoch 92/100\n",
      "162/162 [==============================] - 0s 249us/sample - loss: 0.3760 - accuracy: 0.8765\n",
      "Epoch 93/100\n",
      "162/162 [==============================] - 0s 227us/sample - loss: 0.3528 - accuracy: 0.8457\n",
      "Epoch 94/100\n",
      "162/162 [==============================] - 0s 226us/sample - loss: 0.3599 - accuracy: 0.8333\n",
      "Epoch 95/100\n",
      "162/162 [==============================] - 0s 237us/sample - loss: 0.3420 - accuracy: 0.8457\n",
      "Epoch 96/100\n",
      "162/162 [==============================] - 0s 226us/sample - loss: 0.3552 - accuracy: 0.8580\n",
      "Epoch 97/100\n",
      "162/162 [==============================] - 0s 262us/sample - loss: 0.3542 - accuracy: 0.8457\n",
      "Epoch 98/100\n",
      "162/162 [==============================] - 0s 249us/sample - loss: 0.3645 - accuracy: 0.8457\n",
      "Epoch 99/100\n",
      "162/162 [==============================] - 0s 242us/sample - loss: 0.3310 - accuracy: 0.8827\n",
      "Epoch 100/100\n",
      "162/162 [==============================] - 0s 287us/sample - loss: 0.3665 - accuracy: 0.8519\n",
      "80/80 [==============================] - 0s 1ms/sample - loss: 0.3826 - accuracy: 0.8000\n",
      "Train on 161 samples\n",
      "Epoch 1/100\n",
      "161/161 [==============================] - 0s 3ms/sample - loss: 0.7188 - accuracy: 0.5528\n",
      "Epoch 2/100\n",
      "161/161 [==============================] - 0s 68us/sample - loss: 0.7294 - accuracy: 0.5466\n",
      "Epoch 3/100\n",
      "161/161 [==============================] - 0s 70us/sample - loss: 0.6951 - accuracy: 0.5839\n",
      "Epoch 4/100\n",
      "161/161 [==============================] - 0s 73us/sample - loss: 0.6972 - accuracy: 0.5404\n",
      "Epoch 5/100\n",
      "161/161 [==============================] - 0s 74us/sample - loss: 0.6962 - accuracy: 0.5528\n",
      "Epoch 6/100\n",
      "161/161 [==============================] - 0s 71us/sample - loss: 0.6731 - accuracy: 0.5652\n",
      "Epoch 7/100\n",
      "161/161 [==============================] - 0s 78us/sample - loss: 0.6420 - accuracy: 0.6025\n",
      "Epoch 8/100\n",
      "161/161 [==============================] - 0s 80us/sample - loss: 0.6267 - accuracy: 0.6335\n",
      "Epoch 9/100\n",
      "161/161 [==============================] - 0s 91us/sample - loss: 0.6386 - accuracy: 0.6832\n",
      "Epoch 10/100\n",
      "161/161 [==============================] - 0s 74us/sample - loss: 0.6454 - accuracy: 0.6211\n",
      "Epoch 11/100\n",
      "161/161 [==============================] - 0s 78us/sample - loss: 0.6309 - accuracy: 0.6087\n",
      "Epoch 12/100\n",
      "161/161 [==============================] - 0s 79us/sample - loss: 0.6222 - accuracy: 0.6522\n",
      "Epoch 13/100\n",
      "161/161 [==============================] - 0s 74us/sample - loss: 0.6275 - accuracy: 0.6522\n",
      "Epoch 14/100\n",
      "161/161 [==============================] - 0s 77us/sample - loss: 0.6247 - accuracy: 0.6584\n",
      "Epoch 15/100\n",
      "161/161 [==============================] - 0s 78us/sample - loss: 0.6161 - accuracy: 0.6957\n",
      "Epoch 16/100\n",
      "161/161 [==============================] - 0s 91us/sample - loss: 0.5897 - accuracy: 0.7391\n",
      "Epoch 17/100\n",
      "161/161 [==============================] - 0s 91us/sample - loss: 0.6158 - accuracy: 0.6770\n",
      "Epoch 18/100\n",
      "161/161 [==============================] - 0s 195us/sample - loss: 0.6062 - accuracy: 0.6646\n",
      "Epoch 19/100\n",
      "161/161 [==============================] - 0s 135us/sample - loss: 0.5628 - accuracy: 0.7516\n",
      "Epoch 20/100\n",
      "161/161 [==============================] - 0s 96us/sample - loss: 0.5907 - accuracy: 0.6832\n",
      "Epoch 21/100\n",
      "161/161 [==============================] - 0s 91us/sample - loss: 0.5842 - accuracy: 0.7143\n",
      "Epoch 22/100\n",
      "161/161 [==============================] - 0s 87us/sample - loss: 0.5822 - accuracy: 0.6894\n",
      "Epoch 23/100\n",
      "161/161 [==============================] - 0s 79us/sample - loss: 0.5731 - accuracy: 0.7453\n",
      "Epoch 24/100\n",
      "161/161 [==============================] - 0s 81us/sample - loss: 0.5706 - accuracy: 0.7205\n",
      "Epoch 25/100\n",
      "161/161 [==============================] - 0s 111us/sample - loss: 0.5715 - accuracy: 0.7578\n",
      "Epoch 26/100\n",
      "161/161 [==============================] - 0s 77us/sample - loss: 0.5495 - accuracy: 0.7826\n",
      "Epoch 27/100\n",
      "161/161 [==============================] - 0s 80us/sample - loss: 0.5702 - accuracy: 0.7081\n",
      "Epoch 28/100\n",
      "161/161 [==============================] - 0s 80us/sample - loss: 0.5609 - accuracy: 0.7453\n",
      "Epoch 29/100\n",
      "161/161 [==============================] - 0s 80us/sample - loss: 0.5393 - accuracy: 0.7516\n",
      "Epoch 30/100\n",
      "161/161 [==============================] - 0s 76us/sample - loss: 0.5647 - accuracy: 0.7267\n",
      "Epoch 31/100\n",
      "161/161 [==============================] - 0s 71us/sample - loss: 0.5454 - accuracy: 0.7578\n",
      "Epoch 32/100\n",
      "161/161 [==============================] - 0s 93us/sample - loss: 0.5506 - accuracy: 0.7329\n",
      "Epoch 33/100\n",
      "161/161 [==============================] - 0s 90us/sample - loss: 0.5566 - accuracy: 0.7329\n",
      "Epoch 34/100\n",
      "161/161 [==============================] - 0s 77us/sample - loss: 0.5669 - accuracy: 0.7081\n",
      "Epoch 35/100\n",
      "161/161 [==============================] - 0s 70us/sample - loss: 0.5355 - accuracy: 0.7640\n",
      "Epoch 36/100\n",
      "161/161 [==============================] - 0s 67us/sample - loss: 0.5617 - accuracy: 0.6832\n",
      "Epoch 37/100\n",
      "161/161 [==============================] - 0s 96us/sample - loss: 0.5196 - accuracy: 0.7764\n",
      "Epoch 38/100\n",
      "161/161 [==============================] - 0s 78us/sample - loss: 0.5199 - accuracy: 0.7702\n",
      "Epoch 39/100\n",
      "161/161 [==============================] - 0s 75us/sample - loss: 0.5287 - accuracy: 0.7702\n",
      "Epoch 40/100\n",
      "161/161 [==============================] - 0s 126us/sample - loss: 0.5378 - accuracy: 0.7391\n",
      "Epoch 41/100\n",
      "161/161 [==============================] - 0s 77us/sample - loss: 0.4905 - accuracy: 0.8447\n",
      "Epoch 42/100\n",
      "161/161 [==============================] - 0s 81us/sample - loss: 0.5220 - accuracy: 0.7888\n",
      "Epoch 43/100\n",
      "161/161 [==============================] - 0s 89us/sample - loss: 0.5265 - accuracy: 0.7826\n",
      "Epoch 44/100\n",
      "161/161 [==============================] - 0s 96us/sample - loss: 0.5007 - accuracy: 0.7888\n",
      "Epoch 45/100\n",
      "161/161 [==============================] - 0s 66us/sample - loss: 0.5044 - accuracy: 0.8137\n",
      "Epoch 46/100\n",
      "161/161 [==============================] - 0s 69us/sample - loss: 0.5029 - accuracy: 0.7888\n",
      "Epoch 47/100\n",
      "161/161 [==============================] - 0s 73us/sample - loss: 0.5200 - accuracy: 0.7888\n",
      "Epoch 48/100\n",
      "161/161 [==============================] - 0s 84us/sample - loss: 0.5241 - accuracy: 0.7950\n",
      "Epoch 49/100\n",
      "161/161 [==============================] - 0s 84us/sample - loss: 0.4953 - accuracy: 0.7888\n",
      "Epoch 50/100\n",
      "161/161 [==============================] - 0s 90us/sample - loss: 0.4805 - accuracy: 0.8199\n",
      "Epoch 51/100\n",
      "161/161 [==============================] - ETA: 0s - loss: 0.5037 - accuracy: 0.82 - 0s 83us/sample - loss: 0.4903 - accuracy: 0.8199\n",
      "Epoch 52/100\n",
      "161/161 [==============================] - 0s 89us/sample - loss: 0.4956 - accuracy: 0.7640\n",
      "Epoch 53/100\n",
      "161/161 [==============================] - 0s 91us/sample - loss: 0.4766 - accuracy: 0.7950\n",
      "Epoch 54/100\n",
      "161/161 [==============================] - 0s 69us/sample - loss: 0.4894 - accuracy: 0.8137\n",
      "Epoch 55/100\n",
      "161/161 [==============================] - 0s 92us/sample - loss: 0.4887 - accuracy: 0.7826\n",
      "Epoch 56/100\n",
      "161/161 [==============================] - 0s 92us/sample - loss: 0.4575 - accuracy: 0.8261\n",
      "Epoch 57/100\n",
      "161/161 [==============================] - 0s 98us/sample - loss: 0.5025 - accuracy: 0.8012\n",
      "Epoch 58/100\n",
      "161/161 [==============================] - 0s 74us/sample - loss: 0.4844 - accuracy: 0.7888\n",
      "Epoch 59/100\n",
      "161/161 [==============================] - 0s 71us/sample - loss: 0.4713 - accuracy: 0.8137\n",
      "Epoch 60/100\n",
      "161/161 [==============================] - 0s 72us/sample - loss: 0.4874 - accuracy: 0.7888\n",
      "Epoch 61/100\n",
      "161/161 [==============================] - 0s 92us/sample - loss: 0.4589 - accuracy: 0.8261\n",
      "Epoch 62/100\n",
      "161/161 [==============================] - 0s 87us/sample - loss: 0.4799 - accuracy: 0.7950\n",
      "Epoch 63/100\n",
      "161/161 [==============================] - 0s 97us/sample - loss: 0.4729 - accuracy: 0.8261\n",
      "Epoch 64/100\n",
      "161/161 [==============================] - 0s 100us/sample - loss: 0.4823 - accuracy: 0.7640\n",
      "Epoch 65/100\n",
      "161/161 [==============================] - 0s 82us/sample - loss: 0.4557 - accuracy: 0.8137\n",
      "Epoch 66/100\n",
      "161/161 [==============================] - 0s 73us/sample - loss: 0.4433 - accuracy: 0.8261\n",
      "Epoch 67/100\n",
      "161/161 [==============================] - 0s 67us/sample - loss: 0.4655 - accuracy: 0.8012\n",
      "Epoch 68/100\n",
      "161/161 [==============================] - 0s 87us/sample - loss: 0.4612 - accuracy: 0.8199\n",
      "Epoch 69/100\n",
      "161/161 [==============================] - 0s 83us/sample - loss: 0.4611 - accuracy: 0.8075\n",
      "Epoch 70/100\n",
      "161/161 [==============================] - 0s 80us/sample - loss: 0.4635 - accuracy: 0.7826\n",
      "Epoch 71/100\n",
      "161/161 [==============================] - 0s 80us/sample - loss: 0.4791 - accuracy: 0.7950\n",
      "Epoch 72/100\n",
      "161/161 [==============================] - 0s 102us/sample - loss: 0.4414 - accuracy: 0.8323\n",
      "Epoch 73/100\n",
      "161/161 [==============================] - 0s 69us/sample - loss: 0.4518 - accuracy: 0.8385\n",
      "Epoch 74/100\n",
      "161/161 [==============================] - 0s 114us/sample - loss: 0.4466 - accuracy: 0.8199\n",
      "Epoch 75/100\n",
      "161/161 [==============================] - 0s 71us/sample - loss: 0.4894 - accuracy: 0.8137\n",
      "Epoch 76/100\n",
      "161/161 [==============================] - 0s 75us/sample - loss: 0.4532 - accuracy: 0.8137\n",
      "Epoch 77/100\n",
      "161/161 [==============================] - 0s 67us/sample - loss: 0.4655 - accuracy: 0.7888\n",
      "Epoch 78/100\n",
      "161/161 [==============================] - 0s 74us/sample - loss: 0.4362 - accuracy: 0.8137\n",
      "Epoch 79/100\n",
      "161/161 [==============================] - 0s 89us/sample - loss: 0.4794 - accuracy: 0.8075\n",
      "Epoch 80/100\n",
      "161/161 [==============================] - 0s 74us/sample - loss: 0.4480 - accuracy: 0.8385\n",
      "Epoch 81/100\n",
      "161/161 [==============================] - 0s 68us/sample - loss: 0.4309 - accuracy: 0.8323\n",
      "Epoch 82/100\n",
      "161/161 [==============================] - 0s 69us/sample - loss: 0.4437 - accuracy: 0.8137\n",
      "Epoch 83/100\n",
      "161/161 [==============================] - 0s 73us/sample - loss: 0.4300 - accuracy: 0.8261\n",
      "Epoch 84/100\n",
      "161/161 [==============================] - 0s 76us/sample - loss: 0.4370 - accuracy: 0.8261\n",
      "Epoch 85/100\n",
      "161/161 [==============================] - 0s 72us/sample - loss: 0.4547 - accuracy: 0.7950\n",
      "Epoch 86/100\n",
      "161/161 [==============================] - 0s 69us/sample - loss: 0.4237 - accuracy: 0.8447\n",
      "Epoch 87/100\n",
      "161/161 [==============================] - 0s 74us/sample - loss: 0.4180 - accuracy: 0.8385\n",
      "Epoch 88/100\n",
      "161/161 [==============================] - 0s 75us/sample - loss: 0.4502 - accuracy: 0.8012\n",
      "Epoch 89/100\n",
      "161/161 [==============================] - 0s 73us/sample - loss: 0.4409 - accuracy: 0.8075\n",
      "Epoch 90/100\n",
      "161/161 [==============================] - 0s 70us/sample - loss: 0.4466 - accuracy: 0.7950\n",
      "Epoch 91/100\n",
      "161/161 [==============================] - 0s 69us/sample - loss: 0.4374 - accuracy: 0.8137\n",
      "Epoch 92/100\n",
      "161/161 [==============================] - 0s 78us/sample - loss: 0.3962 - accuracy: 0.8447\n",
      "Epoch 93/100\n",
      "161/161 [==============================] - 0s 75us/sample - loss: 0.4269 - accuracy: 0.8137\n",
      "Epoch 94/100\n",
      "161/161 [==============================] - 0s 69us/sample - loss: 0.4216 - accuracy: 0.8385\n",
      "Epoch 95/100\n",
      "161/161 [==============================] - 0s 68us/sample - loss: 0.4020 - accuracy: 0.8261\n",
      "Epoch 96/100\n",
      "161/161 [==============================] - 0s 79us/sample - loss: 0.4220 - accuracy: 0.8323\n",
      "Epoch 97/100\n",
      "161/161 [==============================] - 0s 73us/sample - loss: 0.4364 - accuracy: 0.7764\n",
      "Epoch 98/100\n",
      "161/161 [==============================] - 0s 73us/sample - loss: 0.4019 - accuracy: 0.8199\n",
      "Epoch 99/100\n",
      "161/161 [==============================] - 0s 79us/sample - loss: 0.4160 - accuracy: 0.8012\n",
      "Epoch 100/100\n",
      "161/161 [==============================] - 0s 75us/sample - loss: 0.4239 - accuracy: 0.8261\n",
      "81/81 [==============================] - 0s 1ms/sample - loss: 0.4069 - accuracy: 0.8519\n",
      "Train on 161 samples\n",
      "Epoch 1/100\n",
      "161/161 [==============================] - 0s 3ms/sample - loss: 1.1044 - accuracy: 0.4658\n",
      "Epoch 2/100\n",
      "161/161 [==============================] - 0s 75us/sample - loss: 1.0570 - accuracy: 0.4658\n",
      "Epoch 3/100\n",
      "161/161 [==============================] - 0s 82us/sample - loss: 1.0349 - accuracy: 0.4658\n",
      "Epoch 4/100\n",
      "161/161 [==============================] - 0s 79us/sample - loss: 1.0356 - accuracy: 0.4658\n",
      "Epoch 5/100\n",
      "161/161 [==============================] - 0s 73us/sample - loss: 0.9785 - accuracy: 0.4658\n",
      "Epoch 6/100\n",
      "161/161 [==============================] - 0s 79us/sample - loss: 0.9676 - accuracy: 0.4658\n",
      "Epoch 7/100\n",
      "161/161 [==============================] - 0s 85us/sample - loss: 0.9531 - accuracy: 0.4658\n",
      "Epoch 8/100\n",
      "161/161 [==============================] - 0s 75us/sample - loss: 0.9671 - accuracy: 0.4658\n",
      "Epoch 9/100\n",
      "161/161 [==============================] - 0s 80us/sample - loss: 0.9406 - accuracy: 0.4658\n",
      "Epoch 10/100\n",
      "161/161 [==============================] - 0s 80us/sample - loss: 0.9352 - accuracy: 0.4658\n",
      "Epoch 11/100\n",
      "161/161 [==============================] - 0s 71us/sample - loss: 0.9026 - accuracy: 0.4596\n",
      "Epoch 12/100\n",
      "161/161 [==============================] - 0s 78us/sample - loss: 0.8996 - accuracy: 0.4596\n",
      "Epoch 13/100\n",
      "161/161 [==============================] - 0s 76us/sample - loss: 0.8773 - accuracy: 0.4658\n",
      "Epoch 14/100\n",
      "161/161 [==============================] - 0s 76us/sample - loss: 0.8818 - accuracy: 0.4658\n",
      "Epoch 15/100\n",
      "161/161 [==============================] - 0s 79us/sample - loss: 0.8327 - accuracy: 0.4658\n",
      "Epoch 16/100\n",
      "161/161 [==============================] - 0s 83us/sample - loss: 0.8461 - accuracy: 0.4658\n",
      "Epoch 17/100\n",
      "161/161 [==============================] - 0s 83us/sample - loss: 0.8205 - accuracy: 0.4658\n",
      "Epoch 18/100\n",
      "161/161 [==============================] - 0s 88us/sample - loss: 0.8022 - accuracy: 0.4907\n",
      "Epoch 19/100\n",
      "161/161 [==============================] - 0s 79us/sample - loss: 0.7886 - accuracy: 0.4907\n",
      "Epoch 20/100\n",
      "161/161 [==============================] - 0s 75us/sample - loss: 0.7842 - accuracy: 0.4658\n",
      "Epoch 21/100\n",
      "161/161 [==============================] - 0s 77us/sample - loss: 0.7976 - accuracy: 0.4783\n",
      "Epoch 22/100\n",
      "161/161 [==============================] - 0s 78us/sample - loss: 0.7750 - accuracy: 0.4783\n",
      "Epoch 23/100\n",
      "161/161 [==============================] - 0s 76us/sample - loss: 0.7651 - accuracy: 0.4969\n",
      "Epoch 24/100\n",
      "161/161 [==============================] - 0s 82us/sample - loss: 0.7478 - accuracy: 0.5093\n",
      "Epoch 25/100\n",
      "161/161 [==============================] - 0s 74us/sample - loss: 0.7444 - accuracy: 0.5031\n",
      "Epoch 26/100\n",
      "161/161 [==============================] - 0s 85us/sample - loss: 0.7425 - accuracy: 0.5093\n",
      "Epoch 27/100\n",
      "161/161 [==============================] - 0s 72us/sample - loss: 0.7178 - accuracy: 0.5093\n",
      "Epoch 28/100\n",
      "161/161 [==============================] - 0s 79us/sample - loss: 0.6706 - accuracy: 0.5342\n",
      "Epoch 29/100\n",
      "161/161 [==============================] - 0s 77us/sample - loss: 0.7250 - accuracy: 0.4907\n",
      "Epoch 30/100\n",
      "161/161 [==============================] - 0s 77us/sample - loss: 0.7140 - accuracy: 0.4907\n",
      "Epoch 31/100\n",
      "161/161 [==============================] - 0s 91us/sample - loss: 0.6692 - accuracy: 0.5155\n",
      "Epoch 32/100\n",
      "161/161 [==============================] - 0s 99us/sample - loss: 0.6610 - accuracy: 0.5342\n",
      "Epoch 33/100\n",
      "161/161 [==============================] - 0s 80us/sample - loss: 0.6687 - accuracy: 0.5404\n",
      "Epoch 34/100\n",
      "161/161 [==============================] - 0s 75us/sample - loss: 0.6676 - accuracy: 0.5590\n",
      "Epoch 35/100\n",
      "161/161 [==============================] - 0s 77us/sample - loss: 0.6260 - accuracy: 0.6025\n",
      "Epoch 36/100\n",
      "161/161 [==============================] - 0s 76us/sample - loss: 0.6384 - accuracy: 0.5652\n",
      "Epoch 37/100\n",
      "161/161 [==============================] - 0s 80us/sample - loss: 0.6387 - accuracy: 0.6025\n",
      "Epoch 38/100\n",
      "161/161 [==============================] - 0s 76us/sample - loss: 0.6336 - accuracy: 0.5901\n",
      "Epoch 39/100\n",
      "161/161 [==============================] - 0s 70us/sample - loss: 0.6364 - accuracy: 0.6025\n",
      "Epoch 40/100\n",
      "161/161 [==============================] - 0s 75us/sample - loss: 0.6571 - accuracy: 0.5714\n",
      "Epoch 41/100\n",
      "161/161 [==============================] - 0s 79us/sample - loss: 0.6222 - accuracy: 0.6149\n",
      "Epoch 42/100\n",
      "161/161 [==============================] - 0s 74us/sample - loss: 0.6346 - accuracy: 0.5901\n",
      "Epoch 43/100\n",
      "161/161 [==============================] - 0s 82us/sample - loss: 0.6249 - accuracy: 0.6522\n",
      "Epoch 44/100\n",
      "161/161 [==============================] - 0s 81us/sample - loss: 0.5987 - accuracy: 0.6646\n",
      "Epoch 45/100\n",
      "161/161 [==============================] - 0s 76us/sample - loss: 0.6056 - accuracy: 0.6335\n",
      "Epoch 46/100\n",
      "161/161 [==============================] - 0s 76us/sample - loss: 0.6058 - accuracy: 0.6460\n",
      "Epoch 47/100\n",
      "161/161 [==============================] - 0s 78us/sample - loss: 0.5950 - accuracy: 0.6957\n",
      "Epoch 48/100\n",
      "161/161 [==============================] - 0s 78us/sample - loss: 0.5990 - accuracy: 0.6460\n",
      "Epoch 49/100\n",
      "161/161 [==============================] - 0s 75us/sample - loss: 0.6099 - accuracy: 0.6398\n",
      "Epoch 50/100\n",
      "161/161 [==============================] - 0s 79us/sample - loss: 0.6005 - accuracy: 0.7081\n",
      "Epoch 51/100\n",
      "161/161 [==============================] - 0s 77us/sample - loss: 0.5690 - accuracy: 0.7205\n",
      "Epoch 52/100\n",
      "161/161 [==============================] - 0s 73us/sample - loss: 0.5640 - accuracy: 0.7453\n",
      "Epoch 53/100\n",
      "161/161 [==============================] - 0s 83us/sample - loss: 0.5680 - accuracy: 0.7205\n",
      "Epoch 54/100\n",
      "161/161 [==============================] - 0s 76us/sample - loss: 0.5690 - accuracy: 0.7453\n",
      "Epoch 55/100\n",
      "161/161 [==============================] - 0s 82us/sample - loss: 0.5780 - accuracy: 0.7081\n",
      "Epoch 56/100\n",
      "161/161 [==============================] - 0s 74us/sample - loss: 0.5836 - accuracy: 0.7205\n",
      "Epoch 57/100\n",
      "161/161 [==============================] - 0s 82us/sample - loss: 0.5655 - accuracy: 0.6770\n",
      "Epoch 58/100\n",
      "161/161 [==============================] - 0s 78us/sample - loss: 0.5427 - accuracy: 0.7640\n",
      "Epoch 59/100\n",
      "161/161 [==============================] - 0s 79us/sample - loss: 0.5538 - accuracy: 0.7329\n",
      "Epoch 60/100\n",
      "161/161 [==============================] - 0s 75us/sample - loss: 0.5745 - accuracy: 0.7205\n",
      "Epoch 61/100\n",
      "161/161 [==============================] - 0s 76us/sample - loss: 0.5413 - accuracy: 0.7888\n",
      "Epoch 62/100\n",
      "161/161 [==============================] - 0s 75us/sample - loss: 0.5477 - accuracy: 0.7081\n",
      "Epoch 63/100\n",
      "161/161 [==============================] - 0s 75us/sample - loss: 0.5358 - accuracy: 0.7764\n",
      "Epoch 64/100\n",
      "161/161 [==============================] - 0s 72us/sample - loss: 0.5345 - accuracy: 0.7453\n",
      "Epoch 65/100\n",
      "161/161 [==============================] - 0s 81us/sample - loss: 0.5337 - accuracy: 0.7764\n",
      "Epoch 66/100\n",
      "161/161 [==============================] - 0s 65us/sample - loss: 0.5507 - accuracy: 0.7516\n",
      "Epoch 67/100\n",
      "161/161 [==============================] - 0s 82us/sample - loss: 0.5188 - accuracy: 0.7826\n",
      "Epoch 68/100\n",
      "161/161 [==============================] - 0s 112us/sample - loss: 0.5123 - accuracy: 0.8075\n",
      "Epoch 69/100\n",
      "161/161 [==============================] - 0s 74us/sample - loss: 0.5257 - accuracy: 0.7516\n",
      "Epoch 70/100\n",
      "161/161 [==============================] - 0s 92us/sample - loss: 0.5524 - accuracy: 0.7143\n",
      "Epoch 71/100\n",
      "161/161 [==============================] - 0s 83us/sample - loss: 0.5413 - accuracy: 0.7578\n",
      "Epoch 72/100\n",
      "161/161 [==============================] - 0s 76us/sample - loss: 0.5156 - accuracy: 0.7888\n",
      "Epoch 73/100\n",
      "161/161 [==============================] - 0s 70us/sample - loss: 0.5112 - accuracy: 0.7764\n",
      "Epoch 74/100\n",
      "161/161 [==============================] - 0s 76us/sample - loss: 0.5204 - accuracy: 0.7640\n",
      "Epoch 75/100\n",
      "161/161 [==============================] - 0s 67us/sample - loss: 0.5310 - accuracy: 0.7516\n",
      "Epoch 76/100\n",
      "161/161 [==============================] - 0s 73us/sample - loss: 0.5131 - accuracy: 0.7826\n",
      "Epoch 77/100\n",
      "161/161 [==============================] - 0s 78us/sample - loss: 0.5031 - accuracy: 0.8012\n",
      "Epoch 78/100\n",
      "161/161 [==============================] - 0s 77us/sample - loss: 0.5000 - accuracy: 0.8137\n",
      "Epoch 79/100\n",
      "161/161 [==============================] - 0s 99us/sample - loss: 0.4965 - accuracy: 0.8012\n",
      "Epoch 80/100\n",
      "161/161 [==============================] - 0s 82us/sample - loss: 0.5123 - accuracy: 0.7888\n",
      "Epoch 81/100\n",
      "161/161 [==============================] - 0s 77us/sample - loss: 0.4852 - accuracy: 0.8137\n",
      "Epoch 82/100\n",
      "161/161 [==============================] - 0s 90us/sample - loss: 0.4888 - accuracy: 0.7888\n",
      "Epoch 83/100\n",
      "161/161 [==============================] - 0s 90us/sample - loss: 0.5132 - accuracy: 0.7640\n",
      "Epoch 84/100\n",
      "161/161 [==============================] - 0s 79us/sample - loss: 0.5004 - accuracy: 0.7578\n",
      "Epoch 85/100\n",
      "161/161 [==============================] - 0s 73us/sample - loss: 0.4768 - accuracy: 0.8261\n",
      "Epoch 86/100\n",
      "161/161 [==============================] - 0s 75us/sample - loss: 0.5082 - accuracy: 0.7826\n",
      "Epoch 87/100\n",
      "161/161 [==============================] - 0s 69us/sample - loss: 0.4935 - accuracy: 0.8199\n",
      "Epoch 88/100\n",
      "161/161 [==============================] - 0s 70us/sample - loss: 0.4923 - accuracy: 0.7950\n",
      "Epoch 89/100\n",
      "161/161 [==============================] - 0s 72us/sample - loss: 0.4894 - accuracy: 0.8137\n",
      "Epoch 90/100\n",
      "161/161 [==============================] - 0s 67us/sample - loss: 0.4760 - accuracy: 0.8012\n",
      "Epoch 91/100\n",
      "161/161 [==============================] - 0s 79us/sample - loss: 0.4851 - accuracy: 0.8261\n",
      "Epoch 92/100\n",
      "161/161 [==============================] - 0s 79us/sample - loss: 0.4906 - accuracy: 0.7888\n",
      "Epoch 93/100\n",
      "161/161 [==============================] - 0s 77us/sample - loss: 0.4959 - accuracy: 0.8012\n",
      "Epoch 94/100\n",
      "161/161 [==============================] - 0s 68us/sample - loss: 0.4796 - accuracy: 0.8137\n",
      "Epoch 95/100\n",
      "161/161 [==============================] - 0s 70us/sample - loss: 0.4943 - accuracy: 0.8137\n",
      "Epoch 96/100\n",
      "161/161 [==============================] - 0s 81us/sample - loss: 0.4690 - accuracy: 0.7950\n",
      "Epoch 97/100\n",
      "161/161 [==============================] - 0s 97us/sample - loss: 0.4760 - accuracy: 0.8075\n",
      "Epoch 98/100\n",
      "161/161 [==============================] - 0s 95us/sample - loss: 0.4696 - accuracy: 0.8075\n",
      "Epoch 99/100\n",
      "161/161 [==============================] - 0s 94us/sample - loss: 0.4610 - accuracy: 0.8323\n",
      "Epoch 100/100\n",
      "161/161 [==============================] - 0s 93us/sample - loss: 0.4608 - accuracy: 0.8323\n",
      "81/81 [==============================] - 0s 1ms/sample - loss: 0.4639 - accuracy: 0.8642\n",
      "Train on 162 samples\n",
      "Epoch 1/100\n",
      "162/162 [==============================] - 0s 3ms/sample - loss: 0.7912 - accuracy: 0.4753\n",
      "Epoch 2/100\n",
      "162/162 [==============================] - 0s 74us/sample - loss: 0.7643 - accuracy: 0.4506\n",
      "Epoch 3/100\n",
      "162/162 [==============================] - 0s 75us/sample - loss: 0.7274 - accuracy: 0.4568\n",
      "Epoch 4/100\n",
      "162/162 [==============================] - 0s 70us/sample - loss: 0.7443 - accuracy: 0.4444\n",
      "Epoch 5/100\n",
      "162/162 [==============================] - 0s 73us/sample - loss: 0.7316 - accuracy: 0.4630\n",
      "Epoch 6/100\n",
      "162/162 [==============================] - 0s 66us/sample - loss: 0.7076 - accuracy: 0.5370\n",
      "Epoch 7/100\n",
      "162/162 [==============================] - 0s 71us/sample - loss: 0.7110 - accuracy: 0.5123\n",
      "Epoch 8/100\n",
      "162/162 [==============================] - 0s 71us/sample - loss: 0.6823 - accuracy: 0.5309\n",
      "Epoch 9/100\n",
      "162/162 [==============================] - 0s 73us/sample - loss: 0.6961 - accuracy: 0.5494\n",
      "Epoch 10/100\n",
      "162/162 [==============================] - 0s 71us/sample - loss: 0.6534 - accuracy: 0.5988\n",
      "Epoch 11/100\n",
      "162/162 [==============================] - 0s 74us/sample - loss: 0.6420 - accuracy: 0.5926\n",
      "Epoch 12/100\n",
      "162/162 [==============================] - 0s 69us/sample - loss: 0.6409 - accuracy: 0.6358\n",
      "Epoch 13/100\n",
      "162/162 [==============================] - 0s 71us/sample - loss: 0.6468 - accuracy: 0.5864\n",
      "Epoch 14/100\n",
      "162/162 [==============================] - 0s 66us/sample - loss: 0.6617 - accuracy: 0.5988\n",
      "Epoch 15/100\n",
      "162/162 [==============================] - 0s 74us/sample - loss: 0.6085 - accuracy: 0.6914\n",
      "Epoch 16/100\n",
      "162/162 [==============================] - 0s 73us/sample - loss: 0.6547 - accuracy: 0.6235\n",
      "Epoch 17/100\n",
      "162/162 [==============================] - 0s 75us/sample - loss: 0.6148 - accuracy: 0.7099\n",
      "Epoch 18/100\n",
      "162/162 [==============================] - 0s 98us/sample - loss: 0.5988 - accuracy: 0.6790\n",
      "Epoch 19/100\n",
      "162/162 [==============================] - 0s 88us/sample - loss: 0.6082 - accuracy: 0.7160\n",
      "Epoch 20/100\n",
      "162/162 [==============================] - 0s 89us/sample - loss: 0.6047 - accuracy: 0.6790\n",
      "Epoch 21/100\n",
      "162/162 [==============================] - 0s 85us/sample - loss: 0.5852 - accuracy: 0.7099\n",
      "Epoch 22/100\n",
      "162/162 [==============================] - 0s 94us/sample - loss: 0.5985 - accuracy: 0.6852\n",
      "Epoch 23/100\n",
      "162/162 [==============================] - 0s 98us/sample - loss: 0.6141 - accuracy: 0.6914\n",
      "Epoch 24/100\n",
      "162/162 [==============================] - 0s 62us/sample - loss: 0.5735 - accuracy: 0.7654\n",
      "Epoch 25/100\n",
      "162/162 [==============================] - 0s 80us/sample - loss: 0.5904 - accuracy: 0.7160\n",
      "Epoch 26/100\n",
      "162/162 [==============================] - 0s 133us/sample - loss: 0.5713 - accuracy: 0.7346\n",
      "Epoch 27/100\n",
      "162/162 [==============================] - 0s 91us/sample - loss: 0.5558 - accuracy: 0.7469\n",
      "Epoch 28/100\n",
      "162/162 [==============================] - 0s 75us/sample - loss: 0.5508 - accuracy: 0.7346\n",
      "Epoch 29/100\n",
      "162/162 [==============================] - 0s 105us/sample - loss: 0.5416 - accuracy: 0.7716\n",
      "Epoch 30/100\n",
      "162/162 [==============================] - 0s 83us/sample - loss: 0.5542 - accuracy: 0.7901\n",
      "Epoch 31/100\n",
      "162/162 [==============================] - 0s 83us/sample - loss: 0.5542 - accuracy: 0.7716\n",
      "Epoch 32/100\n",
      "162/162 [==============================] - 0s 94us/sample - loss: 0.5542 - accuracy: 0.7778\n",
      "Epoch 33/100\n",
      "162/162 [==============================] - 0s 83us/sample - loss: 0.5458 - accuracy: 0.7778\n",
      "Epoch 34/100\n",
      "162/162 [==============================] - 0s 85us/sample - loss: 0.5393 - accuracy: 0.7593\n",
      "Epoch 35/100\n",
      "162/162 [==============================] - 0s 97us/sample - loss: 0.5385 - accuracy: 0.7654\n",
      "Epoch 36/100\n",
      "162/162 [==============================] - 0s 111us/sample - loss: 0.5406 - accuracy: 0.7469\n",
      "Epoch 37/100\n",
      "162/162 [==============================] - 0s 114us/sample - loss: 0.5156 - accuracy: 0.8210\n",
      "Epoch 38/100\n",
      "162/162 [==============================] - 0s 101us/sample - loss: 0.5125 - accuracy: 0.7840\n",
      "Epoch 39/100\n",
      "162/162 [==============================] - 0s 89us/sample - loss: 0.5330 - accuracy: 0.7469\n",
      "Epoch 40/100\n",
      "162/162 [==============================] - 0s 85us/sample - loss: 0.5273 - accuracy: 0.7778\n",
      "Epoch 41/100\n",
      "162/162 [==============================] - 0s 70us/sample - loss: 0.5381 - accuracy: 0.7531\n",
      "Epoch 42/100\n",
      "162/162 [==============================] - 0s 74us/sample - loss: 0.5129 - accuracy: 0.7901\n",
      "Epoch 43/100\n",
      "162/162 [==============================] - 0s 72us/sample - loss: 0.4931 - accuracy: 0.7840\n",
      "Epoch 44/100\n",
      "162/162 [==============================] - 0s 77us/sample - loss: 0.4913 - accuracy: 0.8148\n",
      "Epoch 45/100\n",
      "162/162 [==============================] - 0s 79us/sample - loss: 0.5128 - accuracy: 0.7963\n",
      "Epoch 46/100\n",
      "162/162 [==============================] - 0s 87us/sample - loss: 0.4933 - accuracy: 0.7778\n",
      "Epoch 47/100\n",
      "162/162 [==============================] - 0s 82us/sample - loss: 0.5151 - accuracy: 0.8025\n",
      "Epoch 48/100\n",
      "162/162 [==============================] - 0s 100us/sample - loss: 0.4757 - accuracy: 0.8148\n",
      "Epoch 49/100\n",
      "162/162 [==============================] - 0s 84us/sample - loss: 0.4626 - accuracy: 0.8333\n",
      "Epoch 50/100\n",
      "162/162 [==============================] - 0s 69us/sample - loss: 0.4781 - accuracy: 0.8025\n",
      "Epoch 51/100\n",
      "162/162 [==============================] - 0s 79us/sample - loss: 0.4686 - accuracy: 0.8457\n",
      "Epoch 52/100\n",
      "162/162 [==============================] - 0s 91us/sample - loss: 0.4750 - accuracy: 0.8148\n",
      "Epoch 53/100\n",
      "162/162 [==============================] - 0s 79us/sample - loss: 0.4739 - accuracy: 0.8086\n",
      "Epoch 54/100\n",
      "162/162 [==============================] - 0s 66us/sample - loss: 0.4849 - accuracy: 0.8025\n",
      "Epoch 55/100\n",
      "162/162 [==============================] - 0s 72us/sample - loss: 0.4662 - accuracy: 0.7963\n",
      "Epoch 56/100\n",
      "162/162 [==============================] - 0s 88us/sample - loss: 0.4902 - accuracy: 0.8025\n",
      "Epoch 57/100\n",
      "162/162 [==============================] - 0s 112us/sample - loss: 0.4675 - accuracy: 0.8333\n",
      "Epoch 58/100\n",
      "162/162 [==============================] - 0s 81us/sample - loss: 0.4772 - accuracy: 0.7901\n",
      "Epoch 59/100\n",
      "162/162 [==============================] - 0s 98us/sample - loss: 0.4483 - accuracy: 0.8333\n",
      "Epoch 60/100\n",
      "162/162 [==============================] - 0s 85us/sample - loss: 0.4472 - accuracy: 0.8333\n",
      "Epoch 61/100\n",
      "162/162 [==============================] - 0s 102us/sample - loss: 0.4809 - accuracy: 0.7901\n",
      "Epoch 62/100\n",
      "162/162 [==============================] - 0s 81us/sample - loss: 0.4399 - accuracy: 0.8272\n",
      "Epoch 63/100\n",
      "162/162 [==============================] - 0s 74us/sample - loss: 0.4607 - accuracy: 0.8333\n",
      "Epoch 64/100\n",
      "162/162 [==============================] - 0s 104us/sample - loss: 0.4533 - accuracy: 0.8272\n",
      "Epoch 65/100\n",
      "162/162 [==============================] - 0s 100us/sample - loss: 0.4823 - accuracy: 0.8086\n",
      "Epoch 66/100\n",
      "162/162 [==============================] - 0s 81us/sample - loss: 0.4849 - accuracy: 0.7963\n",
      "Epoch 67/100\n",
      "162/162 [==============================] - 0s 75us/sample - loss: 0.4515 - accuracy: 0.8210\n",
      "Epoch 68/100\n",
      "162/162 [==============================] - 0s 79us/sample - loss: 0.4465 - accuracy: 0.8210\n",
      "Epoch 69/100\n",
      "162/162 [==============================] - 0s 66us/sample - loss: 0.4536 - accuracy: 0.8333\n",
      "Epoch 70/100\n",
      "162/162 [==============================] - 0s 95us/sample - loss: 0.4594 - accuracy: 0.8210\n",
      "Epoch 71/100\n",
      "162/162 [==============================] - 0s 83us/sample - loss: 0.4481 - accuracy: 0.8086\n",
      "Epoch 72/100\n",
      "162/162 [==============================] - 0s 66us/sample - loss: 0.4358 - accuracy: 0.8395\n",
      "Epoch 73/100\n",
      "162/162 [==============================] - 0s 65us/sample - loss: 0.4471 - accuracy: 0.8272\n",
      "Epoch 74/100\n",
      "162/162 [==============================] - 0s 83us/sample - loss: 0.4223 - accuracy: 0.8395\n",
      "Epoch 75/100\n",
      "162/162 [==============================] - 0s 88us/sample - loss: 0.4543 - accuracy: 0.7901\n",
      "Epoch 76/100\n",
      "162/162 [==============================] - 0s 68us/sample - loss: 0.4066 - accuracy: 0.8519\n",
      "Epoch 77/100\n",
      "162/162 [==============================] - 0s 90us/sample - loss: 0.4359 - accuracy: 0.8210\n",
      "Epoch 78/100\n",
      "162/162 [==============================] - 0s 89us/sample - loss: 0.4261 - accuracy: 0.8457\n",
      "Epoch 79/100\n",
      "162/162 [==============================] - 0s 99us/sample - loss: 0.4386 - accuracy: 0.8272\n",
      "Epoch 80/100\n",
      "162/162 [==============================] - 0s 91us/sample - loss: 0.4220 - accuracy: 0.8210\n",
      "Epoch 81/100\n",
      "162/162 [==============================] - 0s 83us/sample - loss: 0.4405 - accuracy: 0.8395\n",
      "Epoch 82/100\n",
      "162/162 [==============================] - 0s 94us/sample - loss: 0.4377 - accuracy: 0.8395\n",
      "Epoch 83/100\n",
      "162/162 [==============================] - 0s 69us/sample - loss: 0.4085 - accuracy: 0.8395\n",
      "Epoch 84/100\n",
      "162/162 [==============================] - 0s 73us/sample - loss: 0.4326 - accuracy: 0.8148\n",
      "Epoch 85/100\n",
      "162/162 [==============================] - 0s 83us/sample - loss: 0.4278 - accuracy: 0.8457\n",
      "Epoch 86/100\n",
      "162/162 [==============================] - 0s 92us/sample - loss: 0.4199 - accuracy: 0.8580\n",
      "Epoch 87/100\n",
      "162/162 [==============================] - 0s 66us/sample - loss: 0.4356 - accuracy: 0.8457\n",
      "Epoch 88/100\n",
      "162/162 [==============================] - 0s 62us/sample - loss: 0.4236 - accuracy: 0.8457\n",
      "Epoch 89/100\n",
      "162/162 [==============================] - 0s 132us/sample - loss: 0.4392 - accuracy: 0.8086\n",
      "Epoch 90/100\n",
      "162/162 [==============================] - 0s 76us/sample - loss: 0.4389 - accuracy: 0.8580\n",
      "Epoch 91/100\n",
      "162/162 [==============================] - 0s 76us/sample - loss: 0.4270 - accuracy: 0.8333\n",
      "Epoch 92/100\n",
      "162/162 [==============================] - 0s 71us/sample - loss: 0.4371 - accuracy: 0.8272\n",
      "Epoch 93/100\n",
      "162/162 [==============================] - 0s 76us/sample - loss: 0.4048 - accuracy: 0.8272\n",
      "Epoch 94/100\n",
      "162/162 [==============================] - 0s 77us/sample - loss: 0.4065 - accuracy: 0.8395\n",
      "Epoch 95/100\n",
      "162/162 [==============================] - 0s 76us/sample - loss: 0.4330 - accuracy: 0.8272\n",
      "Epoch 96/100\n",
      "162/162 [==============================] - 0s 89us/sample - loss: 0.4233 - accuracy: 0.8395\n",
      "Epoch 97/100\n",
      "162/162 [==============================] - 0s 74us/sample - loss: 0.4222 - accuracy: 0.8519\n",
      "Epoch 98/100\n",
      "162/162 [==============================] - 0s 86us/sample - loss: 0.4256 - accuracy: 0.8333\n",
      "Epoch 99/100\n",
      "162/162 [==============================] - 0s 75us/sample - loss: 0.4295 - accuracy: 0.8395\n",
      "Epoch 100/100\n",
      "162/162 [==============================] - 0s 115us/sample - loss: 0.4160 - accuracy: 0.8395\n",
      "80/80 [==============================] - 0s 1ms/sample - loss: 0.4255 - accuracy: 0.8125\n",
      "Train on 161 samples\n",
      "Epoch 1/100\n",
      "161/161 [==============================] - 0s 3ms/sample - loss: 0.8452 - accuracy: 0.4845\n",
      "Epoch 2/100\n",
      "161/161 [==============================] - 0s 49us/sample - loss: 0.8531 - accuracy: 0.4534\n",
      "Epoch 3/100\n",
      "161/161 [==============================] - 0s 54us/sample - loss: 0.8394 - accuracy: 0.4658\n",
      "Epoch 4/100\n",
      "161/161 [==============================] - 0s 46us/sample - loss: 0.8391 - accuracy: 0.4410\n",
      "Epoch 5/100\n",
      "161/161 [==============================] - 0s 50us/sample - loss: 0.8221 - accuracy: 0.4658\n",
      "Epoch 6/100\n",
      "161/161 [==============================] - 0s 46us/sample - loss: 0.8288 - accuracy: 0.4658\n",
      "Epoch 7/100\n",
      "161/161 [==============================] - 0s 51us/sample - loss: 0.8395 - accuracy: 0.4472\n",
      "Epoch 8/100\n",
      "161/161 [==============================] - 0s 49us/sample - loss: 0.8150 - accuracy: 0.4783\n",
      "Epoch 9/100\n",
      "161/161 [==============================] - 0s 50us/sample - loss: 0.7989 - accuracy: 0.4472\n",
      "Epoch 10/100\n",
      "161/161 [==============================] - 0s 52us/sample - loss: 0.8228 - accuracy: 0.4658\n",
      "Epoch 11/100\n",
      "161/161 [==============================] - 0s 46us/sample - loss: 0.8064 - accuracy: 0.4348\n",
      "Epoch 12/100\n",
      "161/161 [==============================] - 0s 50us/sample - loss: 0.7764 - accuracy: 0.5155\n",
      "Epoch 13/100\n",
      "161/161 [==============================] - 0s 48us/sample - loss: 0.7785 - accuracy: 0.5031\n",
      "Epoch 14/100\n",
      "161/161 [==============================] - 0s 45us/sample - loss: 0.7764 - accuracy: 0.4658\n",
      "Epoch 15/100\n",
      "161/161 [==============================] - 0s 50us/sample - loss: 0.7543 - accuracy: 0.4845\n",
      "Epoch 16/100\n",
      "161/161 [==============================] - 0s 51us/sample - loss: 0.7536 - accuracy: 0.4720\n",
      "Epoch 17/100\n",
      "161/161 [==============================] - 0s 81us/sample - loss: 0.7761 - accuracy: 0.4720\n",
      "Epoch 18/100\n",
      "161/161 [==============================] - 0s 51us/sample - loss: 0.7474 - accuracy: 0.5031\n",
      "Epoch 19/100\n",
      "161/161 [==============================] - 0s 57us/sample - loss: 0.7695 - accuracy: 0.4969\n",
      "Epoch 20/100\n",
      "161/161 [==============================] - 0s 48us/sample - loss: 0.7317 - accuracy: 0.5031\n",
      "Epoch 21/100\n",
      "161/161 [==============================] - 0s 48us/sample - loss: 0.7160 - accuracy: 0.5342\n",
      "Epoch 22/100\n",
      "161/161 [==============================] - 0s 46us/sample - loss: 0.7501 - accuracy: 0.4845\n",
      "Epoch 23/100\n",
      "161/161 [==============================] - 0s 57us/sample - loss: 0.7254 - accuracy: 0.5155\n",
      "Epoch 24/100\n",
      "161/161 [==============================] - 0s 55us/sample - loss: 0.7254 - accuracy: 0.5155\n",
      "Epoch 25/100\n",
      "161/161 [==============================] - 0s 56us/sample - loss: 0.7293 - accuracy: 0.5528\n",
      "Epoch 26/100\n",
      "161/161 [==============================] - 0s 55us/sample - loss: 0.7276 - accuracy: 0.5155\n",
      "Epoch 27/100\n",
      "161/161 [==============================] - 0s 70us/sample - loss: 0.7067 - accuracy: 0.5714\n",
      "Epoch 28/100\n",
      "161/161 [==============================] - 0s 59us/sample - loss: 0.7077 - accuracy: 0.5280\n",
      "Epoch 29/100\n",
      "161/161 [==============================] - 0s 63us/sample - loss: 0.7048 - accuracy: 0.5590\n",
      "Epoch 30/100\n",
      "161/161 [==============================] - 0s 68us/sample - loss: 0.6716 - accuracy: 0.5901\n",
      "Epoch 31/100\n",
      "161/161 [==============================] - 0s 107us/sample - loss: 0.6831 - accuracy: 0.5590\n",
      "Epoch 32/100\n",
      "161/161 [==============================] - 0s 70us/sample - loss: 0.7021 - accuracy: 0.5901\n",
      "Epoch 33/100\n",
      "161/161 [==============================] - 0s 59us/sample - loss: 0.6867 - accuracy: 0.5590\n",
      "Epoch 34/100\n",
      "161/161 [==============================] - 0s 48us/sample - loss: 0.6890 - accuracy: 0.5901\n",
      "Epoch 35/100\n",
      "161/161 [==============================] - 0s 44us/sample - loss: 0.7015 - accuracy: 0.5466\n",
      "Epoch 36/100\n",
      "161/161 [==============================] - 0s 50us/sample - loss: 0.6941 - accuracy: 0.6211\n",
      "Epoch 37/100\n",
      "161/161 [==============================] - 0s 63us/sample - loss: 0.6641 - accuracy: 0.5963\n",
      "Epoch 38/100\n",
      "161/161 [==============================] - 0s 68us/sample - loss: 0.7032 - accuracy: 0.5590\n",
      "Epoch 39/100\n",
      "161/161 [==============================] - 0s 63us/sample - loss: 0.6585 - accuracy: 0.6522\n",
      "Epoch 40/100\n",
      "161/161 [==============================] - 0s 63us/sample - loss: 0.6783 - accuracy: 0.5466\n",
      "Epoch 41/100\n",
      "161/161 [==============================] - 0s 54us/sample - loss: 0.6618 - accuracy: 0.6211\n",
      "Epoch 42/100\n",
      "161/161 [==============================] - 0s 65us/sample - loss: 0.6591 - accuracy: 0.5963\n",
      "Epoch 43/100\n",
      "161/161 [==============================] - 0s 44us/sample - loss: 0.6725 - accuracy: 0.6460\n",
      "Epoch 44/100\n",
      "161/161 [==============================] - 0s 53us/sample - loss: 0.6412 - accuracy: 0.6460\n",
      "Epoch 45/100\n",
      "161/161 [==============================] - 0s 56us/sample - loss: 0.6875 - accuracy: 0.6025\n",
      "Epoch 46/100\n",
      "161/161 [==============================] - 0s 38us/sample - loss: 0.6687 - accuracy: 0.5901\n",
      "Epoch 47/100\n",
      "161/161 [==============================] - 0s 40us/sample - loss: 0.6313 - accuracy: 0.6335\n",
      "Epoch 48/100\n",
      "161/161 [==============================] - 0s 47us/sample - loss: 0.6529 - accuracy: 0.5901\n",
      "Epoch 49/100\n",
      "161/161 [==============================] - 0s 49us/sample - loss: 0.6252 - accuracy: 0.6957\n",
      "Epoch 50/100\n",
      "161/161 [==============================] - 0s 42us/sample - loss: 0.6307 - accuracy: 0.6335\n",
      "Epoch 51/100\n",
      "161/161 [==============================] - 0s 47us/sample - loss: 0.6420 - accuracy: 0.6335\n",
      "Epoch 52/100\n",
      "161/161 [==============================] - 0s 46us/sample - loss: 0.6281 - accuracy: 0.6522\n",
      "Epoch 53/100\n",
      "161/161 [==============================] - 0s 52us/sample - loss: 0.6173 - accuracy: 0.6957\n",
      "Epoch 54/100\n",
      "161/161 [==============================] - 0s 58us/sample - loss: 0.6236 - accuracy: 0.6584\n",
      "Epoch 55/100\n",
      "161/161 [==============================] - 0s 58us/sample - loss: 0.6320 - accuracy: 0.6211\n",
      "Epoch 56/100\n",
      "161/161 [==============================] - 0s 44us/sample - loss: 0.6337 - accuracy: 0.6273\n",
      "Epoch 57/100\n",
      "161/161 [==============================] - 0s 50us/sample - loss: 0.6184 - accuracy: 0.6770\n",
      "Epoch 58/100\n",
      "161/161 [==============================] - 0s 60us/sample - loss: 0.6082 - accuracy: 0.7081\n",
      "Epoch 59/100\n",
      "161/161 [==============================] - 0s 40us/sample - loss: 0.6213 - accuracy: 0.6646\n",
      "Epoch 60/100\n",
      "161/161 [==============================] - 0s 60us/sample - loss: 0.6140 - accuracy: 0.7081\n",
      "Epoch 61/100\n",
      "161/161 [==============================] - 0s 52us/sample - loss: 0.6245 - accuracy: 0.6584\n",
      "Epoch 62/100\n",
      "161/161 [==============================] - 0s 93us/sample - loss: 0.5991 - accuracy: 0.6770\n",
      "Epoch 63/100\n",
      "161/161 [==============================] - 0s 57us/sample - loss: 0.6074 - accuracy: 0.6894\n",
      "Epoch 64/100\n",
      "161/161 [==============================] - 0s 50us/sample - loss: 0.6040 - accuracy: 0.6832\n",
      "Epoch 65/100\n",
      "161/161 [==============================] - 0s 42us/sample - loss: 0.6055 - accuracy: 0.6894\n",
      "Epoch 66/100\n",
      "161/161 [==============================] - 0s 52us/sample - loss: 0.5874 - accuracy: 0.7391\n",
      "Epoch 67/100\n",
      "161/161 [==============================] - 0s 45us/sample - loss: 0.6012 - accuracy: 0.7329\n",
      "Epoch 68/100\n",
      "161/161 [==============================] - 0s 57us/sample - loss: 0.6234 - accuracy: 0.6335\n",
      "Epoch 69/100\n",
      "161/161 [==============================] - 0s 52us/sample - loss: 0.5748 - accuracy: 0.7267\n",
      "Epoch 70/100\n",
      "161/161 [==============================] - 0s 60us/sample - loss: 0.6044 - accuracy: 0.6646\n",
      "Epoch 71/100\n",
      "161/161 [==============================] - 0s 55us/sample - loss: 0.5956 - accuracy: 0.6894\n",
      "Epoch 72/100\n",
      "161/161 [==============================] - 0s 62us/sample - loss: 0.5894 - accuracy: 0.6957\n",
      "Epoch 73/100\n",
      "161/161 [==============================] - 0s 55us/sample - loss: 0.5611 - accuracy: 0.7702\n",
      "Epoch 74/100\n",
      "161/161 [==============================] - 0s 53us/sample - loss: 0.5659 - accuracy: 0.7081\n",
      "Epoch 75/100\n",
      "161/161 [==============================] - 0s 58us/sample - loss: 0.6044 - accuracy: 0.6832\n",
      "Epoch 76/100\n",
      "161/161 [==============================] - 0s 47us/sample - loss: 0.5771 - accuracy: 0.7391\n",
      "Epoch 77/100\n",
      "161/161 [==============================] - 0s 52us/sample - loss: 0.5630 - accuracy: 0.7453\n",
      "Epoch 78/100\n",
      "161/161 [==============================] - 0s 45us/sample - loss: 0.5780 - accuracy: 0.7019\n",
      "Epoch 79/100\n",
      "161/161 [==============================] - 0s 58us/sample - loss: 0.5688 - accuracy: 0.7516\n",
      "Epoch 80/100\n",
      "161/161 [==============================] - 0s 42us/sample - loss: 0.5957 - accuracy: 0.6522\n",
      "Epoch 81/100\n",
      "161/161 [==============================] - 0s 45us/sample - loss: 0.5410 - accuracy: 0.7640\n",
      "Epoch 82/100\n",
      "161/161 [==============================] - 0s 49us/sample - loss: 0.5632 - accuracy: 0.7950\n",
      "Epoch 83/100\n",
      "161/161 [==============================] - 0s 52us/sample - loss: 0.5549 - accuracy: 0.7143\n",
      "Epoch 84/100\n",
      "161/161 [==============================] - 0s 48us/sample - loss: 0.5666 - accuracy: 0.6957\n",
      "Epoch 85/100\n",
      "161/161 [==============================] - 0s 51us/sample - loss: 0.5606 - accuracy: 0.7267\n",
      "Epoch 86/100\n",
      "161/161 [==============================] - 0s 48us/sample - loss: 0.5668 - accuracy: 0.7267\n",
      "Epoch 87/100\n",
      "161/161 [==============================] - 0s 40us/sample - loss: 0.5599 - accuracy: 0.7453\n",
      "Epoch 88/100\n",
      "161/161 [==============================] - 0s 62us/sample - loss: 0.5465 - accuracy: 0.7453\n",
      "Epoch 89/100\n",
      "161/161 [==============================] - 0s 53us/sample - loss: 0.5352 - accuracy: 0.7702\n",
      "Epoch 90/100\n",
      "161/161 [==============================] - 0s 43us/sample - loss: 0.5651 - accuracy: 0.7516\n",
      "Epoch 91/100\n",
      "161/161 [==============================] - 0s 52us/sample - loss: 0.5423 - accuracy: 0.7391\n",
      "Epoch 92/100\n",
      "161/161 [==============================] - 0s 47us/sample - loss: 0.5369 - accuracy: 0.7516\n",
      "Epoch 93/100\n",
      "161/161 [==============================] - 0s 43us/sample - loss: 0.5359 - accuracy: 0.7702\n",
      "Epoch 94/100\n",
      "161/161 [==============================] - 0s 44us/sample - loss: 0.5432 - accuracy: 0.7640\n",
      "Epoch 95/100\n",
      "161/161 [==============================] - 0s 43us/sample - loss: 0.5313 - accuracy: 0.7453\n",
      "Epoch 96/100\n",
      "161/161 [==============================] - 0s 54us/sample - loss: 0.5387 - accuracy: 0.7950\n",
      "Epoch 97/100\n",
      "161/161 [==============================] - 0s 45us/sample - loss: 0.5287 - accuracy: 0.7640\n",
      "Epoch 98/100\n",
      "161/161 [==============================] - 0s 51us/sample - loss: 0.5451 - accuracy: 0.7391\n",
      "Epoch 99/100\n",
      "161/161 [==============================] - 0s 46us/sample - loss: 0.5498 - accuracy: 0.7640\n",
      "Epoch 100/100\n",
      "161/161 [==============================] - 0s 49us/sample - loss: 0.5329 - accuracy: 0.7391\n",
      "81/81 [==============================] - 0s 1ms/sample - loss: 0.5193 - accuracy: 0.8025\n",
      "Train on 161 samples\n",
      "Epoch 1/100\n",
      "161/161 [==============================] - 1s 3ms/sample - loss: 0.9517 - accuracy: 0.4658\n",
      "Epoch 2/100\n",
      "161/161 [==============================] - 0s 45us/sample - loss: 0.9742 - accuracy: 0.4658\n",
      "Epoch 3/100\n",
      "161/161 [==============================] - 0s 49us/sample - loss: 0.9479 - accuracy: 0.4658\n",
      "Epoch 4/100\n",
      "161/161 [==============================] - 0s 45us/sample - loss: 0.9520 - accuracy: 0.4658\n",
      "Epoch 5/100\n",
      "161/161 [==============================] - 0s 43us/sample - loss: 0.9118 - accuracy: 0.4658\n",
      "Epoch 6/100\n",
      "161/161 [==============================] - 0s 50us/sample - loss: 0.9345 - accuracy: 0.4596\n",
      "Epoch 7/100\n",
      "161/161 [==============================] - 0s 44us/sample - loss: 0.9033 - accuracy: 0.4658\n",
      "Epoch 8/100\n",
      "161/161 [==============================] - 0s 46us/sample - loss: 0.9087 - accuracy: 0.4658\n",
      "Epoch 9/100\n",
      "161/161 [==============================] - 0s 51us/sample - loss: 0.8706 - accuracy: 0.4658\n",
      "Epoch 10/100\n",
      "161/161 [==============================] - 0s 52us/sample - loss: 0.8825 - accuracy: 0.4658\n",
      "Epoch 11/100\n",
      "161/161 [==============================] - 0s 45us/sample - loss: 0.8607 - accuracy: 0.4596\n",
      "Epoch 12/100\n",
      "161/161 [==============================] - 0s 48us/sample - loss: 0.8771 - accuracy: 0.4658\n",
      "Epoch 13/100\n",
      "161/161 [==============================] - 0s 49us/sample - loss: 0.8345 - accuracy: 0.4658\n",
      "Epoch 14/100\n",
      "161/161 [==============================] - 0s 46us/sample - loss: 0.8416 - accuracy: 0.4534\n",
      "Epoch 15/100\n",
      "161/161 [==============================] - 0s 48us/sample - loss: 0.8252 - accuracy: 0.4596\n",
      "Epoch 16/100\n",
      "161/161 [==============================] - 0s 48us/sample - loss: 0.8285 - accuracy: 0.4658\n",
      "Epoch 17/100\n",
      "161/161 [==============================] - 0s 46us/sample - loss: 0.8348 - accuracy: 0.4783\n",
      "Epoch 18/100\n",
      "161/161 [==============================] - 0s 44us/sample - loss: 0.8083 - accuracy: 0.4783\n",
      "Epoch 19/100\n",
      "161/161 [==============================] - 0s 48us/sample - loss: 0.8146 - accuracy: 0.4783\n",
      "Epoch 20/100\n",
      "161/161 [==============================] - 0s 46us/sample - loss: 0.8033 - accuracy: 0.4658\n",
      "Epoch 21/100\n",
      "161/161 [==============================] - 0s 48us/sample - loss: 0.7900 - accuracy: 0.4720\n",
      "Epoch 22/100\n",
      "161/161 [==============================] - 0s 45us/sample - loss: 0.8063 - accuracy: 0.4658\n",
      "Epoch 23/100\n",
      "161/161 [==============================] - 0s 49us/sample - loss: 0.7749 - accuracy: 0.4720\n",
      "Epoch 24/100\n",
      "161/161 [==============================] - 0s 54us/sample - loss: 0.8077 - accuracy: 0.4783\n",
      "Epoch 25/100\n",
      "161/161 [==============================] - 0s 52us/sample - loss: 0.7622 - accuracy: 0.4907\n",
      "Epoch 26/100\n",
      "161/161 [==============================] - 0s 60us/sample - loss: 0.7764 - accuracy: 0.4720\n",
      "Epoch 27/100\n",
      "161/161 [==============================] - 0s 53us/sample - loss: 0.7579 - accuracy: 0.4845\n",
      "Epoch 28/100\n",
      "161/161 [==============================] - 0s 50us/sample - loss: 0.7541 - accuracy: 0.4907\n",
      "Epoch 29/100\n",
      "161/161 [==============================] - 0s 55us/sample - loss: 0.7514 - accuracy: 0.4907\n",
      "Epoch 30/100\n",
      "161/161 [==============================] - 0s 77us/sample - loss: 0.7711 - accuracy: 0.4845\n",
      "Epoch 31/100\n",
      "161/161 [==============================] - 0s 45us/sample - loss: 0.7699 - accuracy: 0.4845\n",
      "Epoch 32/100\n",
      "161/161 [==============================] - 0s 77us/sample - loss: 0.7335 - accuracy: 0.5093\n",
      "Epoch 33/100\n",
      "161/161 [==============================] - 0s 65us/sample - loss: 0.7115 - accuracy: 0.5155\n",
      "Epoch 34/100\n",
      "161/161 [==============================] - 0s 49us/sample - loss: 0.7263 - accuracy: 0.4907\n",
      "Epoch 35/100\n",
      "161/161 [==============================] - 0s 55us/sample - loss: 0.6919 - accuracy: 0.5155\n",
      "Epoch 36/100\n",
      "161/161 [==============================] - 0s 55us/sample - loss: 0.7059 - accuracy: 0.5031\n",
      "Epoch 37/100\n",
      "161/161 [==============================] - 0s 43us/sample - loss: 0.7103 - accuracy: 0.4907\n",
      "Epoch 38/100\n",
      "161/161 [==============================] - 0s 50us/sample - loss: 0.6905 - accuracy: 0.5280\n",
      "Epoch 39/100\n",
      "161/161 [==============================] - 0s 47us/sample - loss: 0.7182 - accuracy: 0.5093\n",
      "Epoch 40/100\n",
      "161/161 [==============================] - 0s 49us/sample - loss: 0.6955 - accuracy: 0.5280\n",
      "Epoch 41/100\n",
      "161/161 [==============================] - 0s 47us/sample - loss: 0.6978 - accuracy: 0.5280\n",
      "Epoch 42/100\n",
      "161/161 [==============================] - 0s 48us/sample - loss: 0.6701 - accuracy: 0.5652\n",
      "Epoch 43/100\n",
      "161/161 [==============================] - 0s 51us/sample - loss: 0.6807 - accuracy: 0.5342\n",
      "Epoch 44/100\n",
      "161/161 [==============================] - 0s 54us/sample - loss: 0.6794 - accuracy: 0.5155\n",
      "Epoch 45/100\n",
      "161/161 [==============================] - 0s 61us/sample - loss: 0.6810 - accuracy: 0.5404\n",
      "Epoch 46/100\n",
      "161/161 [==============================] - 0s 62us/sample - loss: 0.6938 - accuracy: 0.5217\n",
      "Epoch 47/100\n",
      "161/161 [==============================] - 0s 64us/sample - loss: 0.6538 - accuracy: 0.5590\n",
      "Epoch 48/100\n",
      "161/161 [==============================] - 0s 81us/sample - loss: 0.6614 - accuracy: 0.5466\n",
      "Epoch 49/100\n",
      "161/161 [==============================] - 0s 39us/sample - loss: 0.6853 - accuracy: 0.5652\n",
      "Epoch 50/100\n",
      "161/161 [==============================] - ETA: 0s - loss: 0.6346 - accuracy: 0.63 - 0s 53us/sample - loss: 0.6621 - accuracy: 0.5776\n",
      "Epoch 51/100\n",
      "161/161 [==============================] - 0s 60us/sample - loss: 0.6497 - accuracy: 0.5714\n",
      "Epoch 52/100\n",
      "161/161 [==============================] - 0s 48us/sample - loss: 0.6512 - accuracy: 0.6335\n",
      "Epoch 53/100\n",
      "161/161 [==============================] - 0s 58us/sample - loss: 0.6425 - accuracy: 0.6273\n",
      "Epoch 54/100\n",
      "161/161 [==============================] - 0s 58us/sample - loss: 0.6546 - accuracy: 0.6087\n",
      "Epoch 55/100\n",
      "161/161 [==============================] - 0s 55us/sample - loss: 0.6361 - accuracy: 0.6335\n",
      "Epoch 56/100\n",
      "161/161 [==============================] - 0s 54us/sample - loss: 0.6404 - accuracy: 0.5901\n",
      "Epoch 57/100\n",
      "161/161 [==============================] - 0s 56us/sample - loss: 0.6456 - accuracy: 0.6211\n",
      "Epoch 58/100\n",
      "161/161 [==============================] - 0s 50us/sample - loss: 0.6287 - accuracy: 0.6273\n",
      "Epoch 59/100\n",
      "161/161 [==============================] - 0s 51us/sample - loss: 0.6462 - accuracy: 0.5652\n",
      "Epoch 60/100\n",
      "161/161 [==============================] - 0s 56us/sample - loss: 0.6249 - accuracy: 0.6398\n",
      "Epoch 61/100\n",
      "161/161 [==============================] - 0s 56us/sample - loss: 0.6345 - accuracy: 0.6460\n",
      "Epoch 62/100\n",
      "161/161 [==============================] - 0s 43us/sample - loss: 0.6253 - accuracy: 0.6211\n",
      "Epoch 63/100\n",
      "161/161 [==============================] - 0s 55us/sample - loss: 0.6285 - accuracy: 0.6335\n",
      "Epoch 64/100\n",
      "161/161 [==============================] - 0s 48us/sample - loss: 0.6174 - accuracy: 0.6460\n",
      "Epoch 65/100\n",
      "161/161 [==============================] - 0s 52us/sample - loss: 0.6102 - accuracy: 0.6460\n",
      "Epoch 66/100\n",
      "161/161 [==============================] - 0s 47us/sample - loss: 0.6076 - accuracy: 0.6522\n",
      "Epoch 67/100\n",
      "161/161 [==============================] - 0s 44us/sample - loss: 0.6111 - accuracy: 0.6584\n",
      "Epoch 68/100\n",
      "161/161 [==============================] - 0s 57us/sample - loss: 0.6011 - accuracy: 0.6708\n",
      "Epoch 69/100\n",
      "161/161 [==============================] - 0s 47us/sample - loss: 0.6242 - accuracy: 0.6584\n",
      "Epoch 70/100\n",
      "161/161 [==============================] - 0s 46us/sample - loss: 0.5977 - accuracy: 0.6770\n",
      "Epoch 71/100\n",
      "161/161 [==============================] - 0s 61us/sample - loss: 0.6058 - accuracy: 0.6522\n",
      "Epoch 72/100\n",
      "161/161 [==============================] - 0s 59us/sample - loss: 0.5857 - accuracy: 0.7019\n",
      "Epoch 73/100\n",
      "161/161 [==============================] - 0s 43us/sample - loss: 0.5991 - accuracy: 0.7019\n",
      "Epoch 74/100\n",
      "161/161 [==============================] - 0s 53us/sample - loss: 0.5772 - accuracy: 0.7081\n",
      "Epoch 75/100\n",
      "161/161 [==============================] - 0s 47us/sample - loss: 0.5753 - accuracy: 0.6584\n",
      "Epoch 76/100\n",
      "161/161 [==============================] - 0s 53us/sample - loss: 0.5813 - accuracy: 0.7205\n",
      "Epoch 77/100\n",
      "161/161 [==============================] - 0s 66us/sample - loss: 0.5884 - accuracy: 0.6646\n",
      "Epoch 78/100\n",
      "161/161 [==============================] - 0s 69us/sample - loss: 0.5581 - accuracy: 0.7329\n",
      "Epoch 79/100\n",
      "161/161 [==============================] - 0s 55us/sample - loss: 0.5754 - accuracy: 0.7081\n",
      "Epoch 80/100\n",
      "161/161 [==============================] - 0s 53us/sample - loss: 0.5758 - accuracy: 0.7391\n",
      "Epoch 81/100\n",
      "161/161 [==============================] - 0s 49us/sample - loss: 0.5516 - accuracy: 0.7391\n",
      "Epoch 82/100\n",
      "161/161 [==============================] - 0s 51us/sample - loss: 0.5500 - accuracy: 0.7578\n",
      "Epoch 83/100\n",
      "161/161 [==============================] - 0s 57us/sample - loss: 0.5778 - accuracy: 0.7267\n",
      "Epoch 84/100\n",
      "161/161 [==============================] - 0s 50us/sample - loss: 0.5649 - accuracy: 0.7329\n",
      "Epoch 85/100\n",
      "161/161 [==============================] - 0s 55us/sample - loss: 0.5678 - accuracy: 0.7143\n",
      "Epoch 86/100\n",
      "161/161 [==============================] - 0s 42us/sample - loss: 0.5650 - accuracy: 0.7267\n",
      "Epoch 87/100\n",
      "161/161 [==============================] - 0s 55us/sample - loss: 0.5584 - accuracy: 0.7267\n",
      "Epoch 88/100\n",
      "161/161 [==============================] - 0s 50us/sample - loss: 0.5623 - accuracy: 0.7453\n",
      "Epoch 89/100\n",
      "161/161 [==============================] - 0s 50us/sample - loss: 0.5401 - accuracy: 0.7578\n",
      "Epoch 90/100\n",
      "161/161 [==============================] - 0s 52us/sample - loss: 0.5474 - accuracy: 0.7453\n",
      "Epoch 91/100\n",
      "161/161 [==============================] - 0s 61us/sample - loss: 0.5315 - accuracy: 0.7764\n",
      "Epoch 92/100\n",
      "161/161 [==============================] - 0s 56us/sample - loss: 0.5519 - accuracy: 0.7453\n",
      "Epoch 93/100\n",
      "161/161 [==============================] - 0s 46us/sample - loss: 0.5640 - accuracy: 0.7391\n",
      "Epoch 94/100\n",
      "161/161 [==============================] - 0s 40us/sample - loss: 0.5552 - accuracy: 0.7516\n",
      "Epoch 95/100\n",
      "161/161 [==============================] - 0s 61us/sample - loss: 0.5276 - accuracy: 0.7764\n",
      "Epoch 96/100\n",
      "161/161 [==============================] - 0s 59us/sample - loss: 0.5507 - accuracy: 0.7329\n",
      "Epoch 97/100\n",
      "161/161 [==============================] - 0s 66us/sample - loss: 0.5353 - accuracy: 0.7826\n",
      "Epoch 98/100\n",
      "161/161 [==============================] - 0s 54us/sample - loss: 0.5567 - accuracy: 0.7391\n",
      "Epoch 99/100\n",
      "161/161 [==============================] - 0s 62us/sample - loss: 0.5509 - accuracy: 0.7516\n",
      "Epoch 100/100\n",
      "161/161 [==============================] - 0s 62us/sample - loss: 0.5425 - accuracy: 0.7640\n",
      "81/81 [==============================] - 0s 1ms/sample - loss: 0.5219 - accuracy: 0.8025\n",
      "Train on 162 samples\n",
      "Epoch 1/100\n",
      "162/162 [==============================] - 1s 3ms/sample - loss: 0.8467 - accuracy: 0.4012\n",
      "Epoch 2/100\n",
      "162/162 [==============================] - 0s 45us/sample - loss: 0.8078 - accuracy: 0.4198\n",
      "Epoch 3/100\n",
      "162/162 [==============================] - 0s 48us/sample - loss: 0.8153 - accuracy: 0.4198\n",
      "Epoch 4/100\n",
      "162/162 [==============================] - 0s 49us/sample - loss: 0.8170 - accuracy: 0.3457\n",
      "Epoch 5/100\n",
      "162/162 [==============================] - 0s 43us/sample - loss: 0.8034 - accuracy: 0.4136\n",
      "Epoch 6/100\n",
      "162/162 [==============================] - 0s 45us/sample - loss: 0.7958 - accuracy: 0.4012\n",
      "Epoch 7/100\n",
      "162/162 [==============================] - 0s 45us/sample - loss: 0.8065 - accuracy: 0.3827\n",
      "Epoch 8/100\n",
      "162/162 [==============================] - 0s 46us/sample - loss: 0.8087 - accuracy: 0.4012\n",
      "Epoch 9/100\n",
      "162/162 [==============================] - 0s 45us/sample - loss: 0.7663 - accuracy: 0.4691\n",
      "Epoch 10/100\n",
      "162/162 [==============================] - 0s 43us/sample - loss: 0.7606 - accuracy: 0.4630\n",
      "Epoch 11/100\n",
      "162/162 [==============================] - 0s 43us/sample - loss: 0.7647 - accuracy: 0.3951\n",
      "Epoch 12/100\n",
      "162/162 [==============================] - 0s 43us/sample - loss: 0.7617 - accuracy: 0.4568\n",
      "Epoch 13/100\n",
      "162/162 [==============================] - 0s 43us/sample - loss: 0.7428 - accuracy: 0.4753\n",
      "Epoch 14/100\n",
      "162/162 [==============================] - 0s 46us/sample - loss: 0.7307 - accuracy: 0.4506\n",
      "Epoch 15/100\n",
      "162/162 [==============================] - 0s 40us/sample - loss: 0.7535 - accuracy: 0.4753\n",
      "Epoch 16/100\n",
      "162/162 [==============================] - 0s 46us/sample - loss: 0.7405 - accuracy: 0.4815\n",
      "Epoch 17/100\n",
      "162/162 [==============================] - 0s 50us/sample - loss: 0.7355 - accuracy: 0.4753\n",
      "Epoch 18/100\n",
      "162/162 [==============================] - 0s 45us/sample - loss: 0.7505 - accuracy: 0.4074\n",
      "Epoch 19/100\n",
      "162/162 [==============================] - 0s 46us/sample - loss: 0.7450 - accuracy: 0.4568\n",
      "Epoch 20/100\n",
      "162/162 [==============================] - 0s 41us/sample - loss: 0.7332 - accuracy: 0.5062\n",
      "Epoch 21/100\n",
      "162/162 [==============================] - 0s 42us/sample - loss: 0.7297 - accuracy: 0.4815\n",
      "Epoch 22/100\n",
      "162/162 [==============================] - 0s 47us/sample - loss: 0.7229 - accuracy: 0.4753\n",
      "Epoch 23/100\n",
      "162/162 [==============================] - 0s 42us/sample - loss: 0.6982 - accuracy: 0.5432\n",
      "Epoch 24/100\n",
      "162/162 [==============================] - 0s 46us/sample - loss: 0.6867 - accuracy: 0.5494\n",
      "Epoch 25/100\n",
      "162/162 [==============================] - 0s 54us/sample - loss: 0.6794 - accuracy: 0.5247\n",
      "Epoch 26/100\n",
      "162/162 [==============================] - 0s 55us/sample - loss: 0.7225 - accuracy: 0.4815\n",
      "Epoch 27/100\n",
      "162/162 [==============================] - 0s 62us/sample - loss: 0.6955 - accuracy: 0.5062\n",
      "Epoch 28/100\n",
      "162/162 [==============================] - 0s 46us/sample - loss: 0.6933 - accuracy: 0.5062\n",
      "Epoch 29/100\n",
      "162/162 [==============================] - 0s 62us/sample - loss: 0.6808 - accuracy: 0.5556\n",
      "Epoch 30/100\n",
      "162/162 [==============================] - 0s 62us/sample - loss: 0.6962 - accuracy: 0.5432\n",
      "Epoch 31/100\n",
      "162/162 [==============================] - 0s 56us/sample - loss: 0.6894 - accuracy: 0.5185\n",
      "Epoch 32/100\n",
      "162/162 [==============================] - 0s 77us/sample - loss: 0.6682 - accuracy: 0.5926\n",
      "Epoch 33/100\n",
      "162/162 [==============================] - 0s 76us/sample - loss: 0.6622 - accuracy: 0.6049\n",
      "Epoch 34/100\n",
      "162/162 [==============================] - 0s 55us/sample - loss: 0.6800 - accuracy: 0.5617\n",
      "Epoch 35/100\n",
      "162/162 [==============================] - 0s 58us/sample - loss: 0.6876 - accuracy: 0.5370\n",
      "Epoch 36/100\n",
      "162/162 [==============================] - 0s 53us/sample - loss: 0.6825 - accuracy: 0.5741\n",
      "Epoch 37/100\n",
      "162/162 [==============================] - 0s 58us/sample - loss: 0.6475 - accuracy: 0.6235\n",
      "Epoch 38/100\n",
      "162/162 [==============================] - 0s 59us/sample - loss: 0.6474 - accuracy: 0.6235\n",
      "Epoch 39/100\n",
      "162/162 [==============================] - 0s 62us/sample - loss: 0.6414 - accuracy: 0.6111\n",
      "Epoch 40/100\n",
      "162/162 [==============================] - 0s 51us/sample - loss: 0.6589 - accuracy: 0.5988\n",
      "Epoch 41/100\n",
      "162/162 [==============================] - 0s 49us/sample - loss: 0.6430 - accuracy: 0.6358\n",
      "Epoch 42/100\n",
      "162/162 [==============================] - 0s 58us/sample - loss: 0.6318 - accuracy: 0.6235\n",
      "Epoch 43/100\n",
      "162/162 [==============================] - 0s 63us/sample - loss: 0.6417 - accuracy: 0.6235\n",
      "Epoch 44/100\n",
      "162/162 [==============================] - 0s 51us/sample - loss: 0.6372 - accuracy: 0.6111\n",
      "Epoch 45/100\n",
      "162/162 [==============================] - 0s 54us/sample - loss: 0.6451 - accuracy: 0.6111\n",
      "Epoch 46/100\n",
      "162/162 [==============================] - 0s 58us/sample - loss: 0.6520 - accuracy: 0.6235\n",
      "Epoch 47/100\n",
      "162/162 [==============================] - 0s 59us/sample - loss: 0.6125 - accuracy: 0.6975\n",
      "Epoch 48/100\n",
      "162/162 [==============================] - 0s 50us/sample - loss: 0.6283 - accuracy: 0.6481\n",
      "Epoch 49/100\n",
      "162/162 [==============================] - 0s 59us/sample - loss: 0.6028 - accuracy: 0.6914\n",
      "Epoch 50/100\n",
      "162/162 [==============================] - 0s 55us/sample - loss: 0.6284 - accuracy: 0.6975\n",
      "Epoch 51/100\n",
      "162/162 [==============================] - 0s 69us/sample - loss: 0.6350 - accuracy: 0.6605\n",
      "Epoch 52/100\n",
      "162/162 [==============================] - 0s 51us/sample - loss: 0.6343 - accuracy: 0.6358\n",
      "Epoch 53/100\n",
      "162/162 [==============================] - 0s 56us/sample - loss: 0.6024 - accuracy: 0.6852\n",
      "Epoch 54/100\n",
      "162/162 [==============================] - 0s 57us/sample - loss: 0.6078 - accuracy: 0.6914\n",
      "Epoch 55/100\n",
      "162/162 [==============================] - 0s 62us/sample - loss: 0.5928 - accuracy: 0.7531\n",
      "Epoch 56/100\n",
      "162/162 [==============================] - 0s 63us/sample - loss: 0.5886 - accuracy: 0.7099\n",
      "Epoch 57/100\n",
      "162/162 [==============================] - 0s 60us/sample - loss: 0.6105 - accuracy: 0.6852\n",
      "Epoch 58/100\n",
      "162/162 [==============================] - 0s 75us/sample - loss: 0.5984 - accuracy: 0.6914\n",
      "Epoch 59/100\n",
      "162/162 [==============================] - 0s 59us/sample - loss: 0.5842 - accuracy: 0.7469\n",
      "Epoch 60/100\n",
      "162/162 [==============================] - 0s 68us/sample - loss: 0.5722 - accuracy: 0.7284\n",
      "Epoch 61/100\n",
      "162/162 [==============================] - 0s 53us/sample - loss: 0.5866 - accuracy: 0.6914\n",
      "Epoch 62/100\n",
      "162/162 [==============================] - 0s 74us/sample - loss: 0.5840 - accuracy: 0.7222\n",
      "Epoch 63/100\n",
      "162/162 [==============================] - 0s 57us/sample - loss: 0.5865 - accuracy: 0.7469\n",
      "Epoch 64/100\n",
      "162/162 [==============================] - 0s 75us/sample - loss: 0.5755 - accuracy: 0.7284\n",
      "Epoch 65/100\n",
      "162/162 [==============================] - 0s 46us/sample - loss: 0.5696 - accuracy: 0.7407\n",
      "Epoch 66/100\n",
      "162/162 [==============================] - 0s 59us/sample - loss: 0.5760 - accuracy: 0.7469\n",
      "Epoch 67/100\n",
      "162/162 [==============================] - 0s 60us/sample - loss: 0.5794 - accuracy: 0.7284\n",
      "Epoch 68/100\n",
      "162/162 [==============================] - 0s 44us/sample - loss: 0.5700 - accuracy: 0.7407\n",
      "Epoch 69/100\n",
      "162/162 [==============================] - 0s 47us/sample - loss: 0.5668 - accuracy: 0.7407\n",
      "Epoch 70/100\n",
      "162/162 [==============================] - 0s 49us/sample - loss: 0.5847 - accuracy: 0.6790\n",
      "Epoch 71/100\n",
      "162/162 [==============================] - 0s 49us/sample - loss: 0.5431 - accuracy: 0.7840\n",
      "Epoch 72/100\n",
      "162/162 [==============================] - 0s 46us/sample - loss: 0.5766 - accuracy: 0.7407\n",
      "Epoch 73/100\n",
      "162/162 [==============================] - 0s 51us/sample - loss: 0.5352 - accuracy: 0.8086\n",
      "Epoch 74/100\n",
      "162/162 [==============================] - 0s 45us/sample - loss: 0.5645 - accuracy: 0.7284\n",
      "Epoch 75/100\n",
      "162/162 [==============================] - 0s 46us/sample - loss: 0.5447 - accuracy: 0.7654\n",
      "Epoch 76/100\n",
      "162/162 [==============================] - 0s 48us/sample - loss: 0.5462 - accuracy: 0.7901\n",
      "Epoch 77/100\n",
      "162/162 [==============================] - 0s 47us/sample - loss: 0.5291 - accuracy: 0.7654\n",
      "Epoch 78/100\n",
      "162/162 [==============================] - 0s 46us/sample - loss: 0.5353 - accuracy: 0.7716\n",
      "Epoch 79/100\n",
      "162/162 [==============================] - 0s 42us/sample - loss: 0.5385 - accuracy: 0.7716\n",
      "Epoch 80/100\n",
      "162/162 [==============================] - 0s 42us/sample - loss: 0.5536 - accuracy: 0.7037\n",
      "Epoch 81/100\n",
      "162/162 [==============================] - 0s 50us/sample - loss: 0.5392 - accuracy: 0.7407\n",
      "Epoch 82/100\n",
      "162/162 [==============================] - 0s 46us/sample - loss: 0.5339 - accuracy: 0.7593\n",
      "Epoch 83/100\n",
      "162/162 [==============================] - 0s 50us/sample - loss: 0.5323 - accuracy: 0.7593\n",
      "Epoch 84/100\n",
      "162/162 [==============================] - 0s 61us/sample - loss: 0.5318 - accuracy: 0.8025\n",
      "Epoch 85/100\n",
      "162/162 [==============================] - 0s 38us/sample - loss: 0.5402 - accuracy: 0.7963\n",
      "Epoch 86/100\n",
      "162/162 [==============================] - 0s 41us/sample - loss: 0.5281 - accuracy: 0.7654\n",
      "Epoch 87/100\n",
      "162/162 [==============================] - 0s 40us/sample - loss: 0.5393 - accuracy: 0.7222\n",
      "Epoch 88/100\n",
      "162/162 [==============================] - 0s 48us/sample - loss: 0.5476 - accuracy: 0.7840\n",
      "Epoch 89/100\n",
      "162/162 [==============================] - 0s 50us/sample - loss: 0.5362 - accuracy: 0.7778\n",
      "Epoch 90/100\n",
      "162/162 [==============================] - 0s 48us/sample - loss: 0.5104 - accuracy: 0.7716\n",
      "Epoch 91/100\n",
      "162/162 [==============================] - 0s 58us/sample - loss: 0.5108 - accuracy: 0.7778\n",
      "Epoch 92/100\n",
      "162/162 [==============================] - 0s 49us/sample - loss: 0.5127 - accuracy: 0.8272\n",
      "Epoch 93/100\n",
      "162/162 [==============================] - 0s 41us/sample - loss: 0.4968 - accuracy: 0.7963\n",
      "Epoch 94/100\n",
      "162/162 [==============================] - 0s 47us/sample - loss: 0.5158 - accuracy: 0.7963\n",
      "Epoch 95/100\n",
      "162/162 [==============================] - 0s 50us/sample - loss: 0.5115 - accuracy: 0.7963\n",
      "Epoch 96/100\n",
      "162/162 [==============================] - 0s 58us/sample - loss: 0.4900 - accuracy: 0.7963\n",
      "Epoch 97/100\n",
      "162/162 [==============================] - 0s 54us/sample - loss: 0.5290 - accuracy: 0.7716\n",
      "Epoch 98/100\n",
      "162/162 [==============================] - 0s 48us/sample - loss: 0.5094 - accuracy: 0.7716\n",
      "Epoch 99/100\n",
      "162/162 [==============================] - 0s 56us/sample - loss: 0.5153 - accuracy: 0.8148\n",
      "Epoch 100/100\n",
      "162/162 [==============================] - 0s 62us/sample - loss: 0.5087 - accuracy: 0.7840\n",
      "80/80 [==============================] - 0s 1ms/sample - loss: 0.4994 - accuracy: 0.7750\n",
      "Train on 242 samples\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/sample - loss: 0.8848 - accuracy: 0.4752\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 41us/sample - loss: 0.8655 - accuracy: 0.4421\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 44us/sample - loss: 0.8870 - accuracy: 0.4463\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 44us/sample - loss: 0.8251 - accuracy: 0.5041\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 39us/sample - loss: 0.8273 - accuracy: 0.4752\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 41us/sample - loss: 0.8115 - accuracy: 0.4793\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 44us/sample - loss: 0.8379 - accuracy: 0.4876\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 43us/sample - loss: 0.7985 - accuracy: 0.5207\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 40us/sample - loss: 0.7576 - accuracy: 0.5455\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 42us/sample - loss: 0.7561 - accuracy: 0.5207\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 40us/sample - loss: 0.7438 - accuracy: 0.5579\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 51us/sample - loss: 0.7769 - accuracy: 0.5537\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 50us/sample - loss: 0.7373 - accuracy: 0.5413\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 53us/sample - loss: 0.7133 - accuracy: 0.5992\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 42us/sample - loss: 0.7053 - accuracy: 0.5702\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 40us/sample - loss: 0.6969 - accuracy: 0.5744\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 43us/sample - loss: 0.6878 - accuracy: 0.5868\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 43us/sample - loss: 0.6697 - accuracy: 0.6240\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 51us/sample - loss: 0.6796 - accuracy: 0.5868\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 56us/sample - loss: 0.6639 - accuracy: 0.6157\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 44us/sample - loss: 0.6381 - accuracy: 0.6570\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 52us/sample - loss: 0.6318 - accuracy: 0.6322\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 47us/sample - loss: 0.6217 - accuracy: 0.6446\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 48us/sample - loss: 0.5963 - accuracy: 0.6736\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 55us/sample - loss: 0.6288 - accuracy: 0.6198\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 60us/sample - loss: 0.5885 - accuracy: 0.6736\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 51us/sample - loss: 0.5869 - accuracy: 0.6860\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 47us/sample - loss: 0.6018 - accuracy: 0.6446\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 40us/sample - loss: 0.6125 - accuracy: 0.6488\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 42us/sample - loss: 0.6104 - accuracy: 0.6612\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 43us/sample - loss: 0.6137 - accuracy: 0.6901\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 43us/sample - loss: 0.6042 - accuracy: 0.6612\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 44us/sample - loss: 0.5805 - accuracy: 0.6983\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 44us/sample - loss: 0.5738 - accuracy: 0.7025\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 47us/sample - loss: 0.5669 - accuracy: 0.7025\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 45us/sample - loss: 0.5395 - accuracy: 0.7107\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 49us/sample - loss: 0.5532 - accuracy: 0.7149\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 41us/sample - loss: 0.5477 - accuracy: 0.7603\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 53us/sample - loss: 0.5565 - accuracy: 0.7231\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 42us/sample - loss: 0.5399 - accuracy: 0.7190\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 42us/sample - loss: 0.5060 - accuracy: 0.7562\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 43us/sample - loss: 0.5197 - accuracy: 0.7355\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 37us/sample - loss: 0.4903 - accuracy: 0.7727\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 42us/sample - loss: 0.5173 - accuracy: 0.7355\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 46us/sample - loss: 0.4987 - accuracy: 0.7810\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 40us/sample - loss: 0.4979 - accuracy: 0.7727\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 47us/sample - loss: 0.5173 - accuracy: 0.7355\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 47us/sample - loss: 0.5064 - accuracy: 0.7562\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 49us/sample - loss: 0.5104 - accuracy: 0.7231\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 42us/sample - loss: 0.4995 - accuracy: 0.7521\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 49us/sample - loss: 0.4915 - accuracy: 0.7273\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 34us/sample - loss: 0.4930 - accuracy: 0.7603\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 43us/sample - loss: 0.4676 - accuracy: 0.7975\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 61us/sample - loss: 0.4828 - accuracy: 0.7562\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 55us/sample - loss: 0.4836 - accuracy: 0.7479\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 63us/sample - loss: 0.4819 - accuracy: 0.7521\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 47us/sample - loss: 0.4700 - accuracy: 0.7603\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 42us/sample - loss: 0.4683 - accuracy: 0.7397\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 51us/sample - loss: 0.4855 - accuracy: 0.7645\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 50us/sample - loss: 0.4435 - accuracy: 0.8017\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 66us/sample - loss: 0.4483 - accuracy: 0.7810\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 37us/sample - loss: 0.4633 - accuracy: 0.7851\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 62us/sample - loss: 0.4522 - accuracy: 0.7769\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 49us/sample - loss: 0.4713 - accuracy: 0.7934\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 46us/sample - loss: 0.4618 - accuracy: 0.7686\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 50us/sample - loss: 0.4519 - accuracy: 0.7727\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 47us/sample - loss: 0.4277 - accuracy: 0.8140\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 50us/sample - loss: 0.4316 - accuracy: 0.7934\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 52us/sample - loss: 0.4488 - accuracy: 0.8017\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 52us/sample - loss: 0.4436 - accuracy: 0.7851\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 43us/sample - loss: 0.4097 - accuracy: 0.8099\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 63us/sample - loss: 0.4352 - accuracy: 0.8306\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 53us/sample - loss: 0.4535 - accuracy: 0.7893\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 47us/sample - loss: 0.4332 - accuracy: 0.7851\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 40us/sample - loss: 0.4191 - accuracy: 0.8017\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 40us/sample - loss: 0.4468 - accuracy: 0.7769\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 35us/sample - loss: 0.4068 - accuracy: 0.8264\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 44us/sample - loss: 0.4042 - accuracy: 0.8017\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 41us/sample - loss: 0.4160 - accuracy: 0.7975\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 40us/sample - loss: 0.4172 - accuracy: 0.7810\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 42us/sample - loss: 0.4372 - accuracy: 0.7851\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 42us/sample - loss: 0.3932 - accuracy: 0.7934\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 42us/sample - loss: 0.4151 - accuracy: 0.8099\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 42us/sample - loss: 0.4190 - accuracy: 0.8017\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 55us/sample - loss: 0.4321 - accuracy: 0.8017\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 48us/sample - loss: 0.4094 - accuracy: 0.8099\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 38us/sample - loss: 0.3845 - accuracy: 0.8306\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 46us/sample - loss: 0.4232 - accuracy: 0.8099\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 57us/sample - loss: 0.4062 - accuracy: 0.8140\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - ETA: 0s - loss: 0.3382 - accuracy: 0.86 - 0s 48us/sample - loss: 0.3972 - accuracy: 0.8347\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 47us/sample - loss: 0.3864 - accuracy: 0.8306\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 38us/sample - loss: 0.3826 - accuracy: 0.8264\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 39us/sample - loss: 0.3955 - accuracy: 0.8264\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 42us/sample - loss: 0.3988 - accuracy: 0.8058\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 48us/sample - loss: 0.3977 - accuracy: 0.8182\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 44us/sample - loss: 0.3907 - accuracy: 0.8430\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 45us/sample - loss: 0.4170 - accuracy: 0.8058\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 40us/sample - loss: 0.4096 - accuracy: 0.8099\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 41us/sample - loss: 0.3895 - accuracy: 0.8264\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 45us/sample - loss: 0.3853 - accuracy: 0.8058\n",
      "Best: 0.8470164736111959 using {'activation': 'relu', 'batch_size': 100, 'epochs': 100}\n",
      "Means: 0.8346707820892334, Stdev: 0.01218084663266205 with: {'activation': 'relu', 'batch_size': 10, 'epochs': 100}\n",
      "Means: 0.822376549243927, Stdev: 0.04171483518665689 with: {'activation': 'relu', 'batch_size': 50, 'epochs': 100}\n",
      "Means: 0.8470164736111959, Stdev: 0.01636353931835924 with: {'activation': 'relu', 'batch_size': 100, 'epochs': 100}\n",
      "Means: 0.8345679044723511, Stdev: 0.026440130370674626 with: {'activation': 'sigmoid', 'batch_size': 10, 'epochs': 100}\n",
      "Means: 0.8428498109181722, Stdev: 0.022044457643924015 with: {'activation': 'sigmoid', 'batch_size': 50, 'epochs': 100}\n",
      "Means: 0.7933127482732137, Stdev: 0.012949085344891052 with: {'activation': 'sigmoid', 'batch_size': 100, 'epochs': 100}\n",
      "61/61 [==============================] - 0s 2ms/sample - loss: 0.4456 - accuracy: 0.8525\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.852459"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(activation):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim=13, activation=activation))\n",
    "    model.add(Dropout(.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'activation': ['relu', 'sigmoid'],\n",
    "              'batch_size': [10, 50, 100],\n",
    "              'epochs': [100]}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") \n",
    "\n",
    "grid_result.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COOl ! Increase in accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U4-S2-NeuralNetworks",
   "language": "python",
   "name": "u4-s2-neuralnetworks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
